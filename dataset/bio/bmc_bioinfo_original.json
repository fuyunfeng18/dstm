{"s12859-019-2641-8": "Harmonization techniques make different gene expression profiles and their sets compatible and ready for comparisons. Here we present a new bioinformatic tool termed Shambhala for harmonization of multiple human gene expression datasets obtained using different experimental methods and platforms of microarray hybridization and RNA sequencing.Unlike previously published methods enabling good quality data harmonization for only two datasets, Shambhala allows conversion of multiple datasets into the universal form suitable for further comparisons. Shambhala harmonization is based on the calibration of gene expression profiles using the auxiliary standardization dataset. Each profile is transformed to make it similar to the output of microarray hybridization platform Affymetrix Human Gene. This platform was chosen because it has the biggest number of human gene expression profiles deposited in public databases. We evaluated Shambhala ability to retain biologically important features after harmonization. The same four biological samples taken in multiple replicates were profiled independently using three and four different experimental platforms, respectively, then Shambhala-harmonized and investigated by hierarchical clustering.Our results showed that unlike other frequently used methods: quantile normalization and DESeq/DESeq2 normalization, Shambhala harmonization was the only method supporting sample-specific and platform-independent biologically meaningful clustering for the data obtained from multiple experimental platforms.Public repositories of gene expression data cover a rich spectrum of normal and pathological conditions, including all known human diseases and developmental features [1\u20134]. The most popular repositories such as Gene Expression Omnibus (GEO) [3] and Array-Express [4] accumulate data for more than 2 million of individual expression profiles in more than 70,000 series of experiments. These transcriptional profiles were generally obtained using different experimental modifications of microarray hybridization and RNA sequencing. However, the expression data is poorly comparable among the different experimental datasets [5\u20139]. This problem is due to both (i) technical features linked with the experimental platforms, and (ii) so-called batch effect [10]. The latter term means that even the experimental results obtained using the same reagents and on the same equipment can be significantly biased over time.This non-comparability of gene expression data hampers further levels of data analysis for the different datasets, e.g. finding differentially expressed genes and assessing activation of molecular pathways [11, 12].To solve this problem, the data must be either normalized (when datasets under comparison were obtained using one experimental platform) or harmonized (when different platforms were used) [12]. For the normalization, more attention is paid to mere equilibration of the scaling factors. Contrarily, for most cases of the harmonization, there is a need to reshape distributions for the entire gene expression profiles.The normalization methods include quantile normalization (QN) [13], frozen robust multi-array analysis for microarray hybridization data (FRMA) [14], Empirical Bayes (EB) method also known as ComBat [15], or Differential Expression analysis for Sequence count data, DESeq [16]/DESeq2 [17]. The methods for harmonization include distance-weighted discrimination (DWD) [18, 19], cross-platform normalization (XPN) [20, 21], Quantile Discretization (QD) [22], Normalized Discretization (NorDi) [22], DisTran (Distribution Transformation) [23], Gene Quantiles (GQ) [24], and platform-independent latent Dirichlet allocation (PLIDA) [25]. In a fundamental survey of different harmonization techniques [20] the XPN method showed the best performance. The harmonization acts by deeply restructuring distributions of gene expression levels for the samples under comparison. As a rule, harmonization algorithms use data clustering to identify similarities between the gene expression profiles obtained using different experimental platforms, and then increase these similarity regions during subsequent reshaping of the expression profiles.However, to our knowledge all previously published harmonization methods have a substantial limitation that they are capable of performing harmonization for only two expression datasets [20]. Thus, only the data from two experimental platforms can be simultaneously harmonized. Moreover, the resulting hybrid data are not further compatible with any of the existing formats for the experimental platforms. Moreover, the published methods show good performance only for the datasets of a comparable sample size, therefore complicating harmonization of the existing data.Here, we present a new method for cross-platform data harmonization termed Shambhala that may be considered a more universal tool compared to the existing approaches. Unlike previous harmonizers, Shambhala is independent on (i) number of harmonized datasets and/or experimental platforms, and (ii) number of samples in every dataset. The Shambhala harmonization protocol includes several specific features such as the auxiliary calibration dataset that helps to initially transform the data, and the reference definitive dataset that defines the universal shape of the output harmonized gene expression profile. Next, we investigated the performance of Shambhala to harmonize the gene expression data from multiple experimental platforms obtained from the Microarray Quality Control (MAQC) [26] and Sequencing Quality Control (SEQC) datasets [27]. Our data evidence that being currently a unique tool for harmonization of multiple datasets, Shambhala provides outputs reflecting biological origin of a biosample rather than the experimental platform used. In contrast, other harmonization tools are not applicable to this type of tasks in principle, and the normalization tools such as QN and DESeq/DESeq2, return low-quality platform-biased outputs.We developed Shambhala method for cross-platform comparisons of multiple datasets. In its present form, the method was tailored for the comparison of human gene expression data, and its application for other organism data requires further specific data search. Let us look at the problem of cross-platform harmonization in more detail. Imagine an arbitrary set of experimental platforms that has produced a set gene expression profiles. Our goal is to make them all comparable. To do so, we may make them similar to a pre-defined reference. This reference may be taken from a set of profiles that has been obtained at a widely used experimental platform; we can term this set the reference definitive dataset (Q). The process of profile transformation involves multiple iteration steps, when the dataset P, which contains profiles under harmonization, is altered, whereas the dataset Q remains unchanged. Consequently, the output of such transform has gene expression profiles like those obtained using the same experimental platform, as for the dataset Q.Some features of this pipeline, which are used for transformation of dataset P into the shape of the dataset Q, were inspired by the XPN method [21] that showed the best performance among the pairwise cross-platform harmonization techniques [20]. Such features include stochastic clustering for gene and samples using genetic algorithms, and partially-linear iterative harmonization of two datasets. However, the major distinctions here are that (i) in the Shambhala algorithm, the dataset P changes, while Q remains constant during the iteration steps, whereas in the XPN both are transformed iteratively; (ii) to increase stability of the results, Shambhala uses spherical (cosine-based) [28, 29] rather than barycentric (as in the XPN) clustering of samples in P and Q datasets.Importantly, the Shambhala pipeline depends on two datasets, P0 and Q, the latter acting as the reference for gene expression profiles after harmonization, and the former serving for preliminary calibration of expression level ranges. As the dataset Q for this application, we used the mRNA expression profiles taken from the Genotype Tissue Expression (GTEx) project [30], namely one hundred samples corresponding to ten normal human tissue types (brain, nerve, skin, adipose, muscle, heart, lung, thyroid, blood vessels and blood). Among the others, the GTEx comprised profiling using microarray platform Affymetrix Human Gene 1.1 ST (GPL16977; deposited under accession number GSE45878) and NGS platform Illumina HiSeq 2000 (accession number E-MTAB-5214). We selected the microarray GTEx results as the Q dataset because it is frequently considered the golden standard for microarray hybridization of human tissues [31, 32], while Affymetrix microarray-profiled expression data are the most abundant kind of data in public databases, e.g. in the Gene Expression Omnibus (GEO) database as for 2018-11-06. To investigate the influence of the definitive dataset on the performance of Shambhala harmonization, we also analyzed an alternative Q-set obtained using the Illumina HiSeq 2000 platform.When selecting the optimal auxiliary calibration dataset (P0) for Shambhala implementation, we found that our previous experimental dataset including 39 human gene expression profiles obtained using CustomArray microchip platform (CustomArray, USA) showed the best performance in clustering tests compared to more than twenty other datasets of the comparable size (data not shown). Interestingly, our attempts to use the GTEx dataset for both P0 and Q, have failed to produce good sample clustering.To investigate the robustness and quality of Shambhala approach, we took a model of gene expression profiles obtained for the same biosamples using different experimental platforms.The MAQC project investigated the expression profiles for 14\u201315 technical replicates of all sample types, A to D, for the most popular microarray platforms, including Agilent-012391 Whole Human Genome Oligo Microarray G4112A (GPL1708), Affymetrix Human Genome U133 Plus 2.0 Array (GPL570) and Illumina Sentrix Human-6 Expression Beadchip (GPL2507). In the SEQC project, the microarray expression profiles for the same biosamples were compared with the RNA sequencing data obtained using Illumina HiSeq 2000 platform (GPL11154), see Table 1.To assess quality of data harmonization, we tested whether hierarchical clustering of the harmonized genes expression profiles will be biologically meaningful or rather dependent on the experimental platforms used. For the clustering, Euclidean distance was used as a metric of proximity. An ideal method for data harmonization would allow grouping of output expression profiles according to the type of biosamples (A to D), but not according to a platform used. Similar types of biosamples (type A and C, and type B and D) were expected to show more tight clustering. In contrast, the platform-based clustering independent on the biological similarities of biosamples could be considered bad result.Quantile normalization, QN [13].Differential expression analysis for sequence count data, DESeq [16]/DESeq2 [17] using the estimateSizeFactors module. To make the microarray data formally suitable for DESeq/DESeq2 normalization, we took an integer part of all microarray-measured expression level values for each gene and each sample. The intensity values for microarray-measured signal were taken as they were deposited in GEO repository, i.e. after device-dependent primary background correction or equilibration but before any cross-platform transformation or harmonization. Although the DESeq/DESeq2 method was designed for normalization of NGS data and assumes that the count data follow a negative binomial distribution, there were several examples when DESeq/DESeq2 was formally applied to rounded microarray data, both in model investigations based on microarray profiles [34] and for processing human patient\u2019s data [35, 36]. Moreover, having applied the negative binomial regression followed by the Pearson chi-squared test, we found that although the MAQC microarray gene expression values were not distributed according to negative binomial law (particularly for the Illumina GPL2507 and Agilent GPL1708 platforms; Fig. 2a), the SEQC microarray profiles (platforms Illumina GPL10558, as well as Affymetrix GPL17930 and GPL16043) matched the negative binomial distribution (Fig. 2b).Shambhala harmonization with two different GTEx definitive datasets (obtained using either microarray Affymetrix or NGS Illumina HiSeq 2000 platforms). Shambhala method was compared with other above normalization techniques (QN, DESeq/DESeq2) because they are popular tools used for merging data from multiple datasets. The standard harmonization methods such as XPN [20, 21] are not applicable because they enable comparisons of only up to two datasets.We tested Shambhala, QN and DESeq/DESeq2 methods for their abilities to simultaneously harmonize data from three experimental microarray platforms (Affymetrix GPL570, Agilent GPL1708 and Illumina GPL2507) from the MAQC project.We next compared the abilities of Shambhala, QN and DESeq/DESeq2 methods to harmonize the data obtained using four experimental platforms. To this end we took the gene expression profiles from the Sequencing Quality Control (SEQC) project [33], Table 1. In this case, we harmonized data obtained for three microarray platforms and one RNA sequencing platforms, Illumina HumanHT-12\u2009V4.0 (GPL10558), Affymetrix Human Gene 2.0 ST (GPL17930), Affymetrix GeneChip PrimeView (GPL16043), and Illumina HiSeq 2000 (GPL11154), respectively. For RNA sequencing data, we applied filtering to remove profiles with low, and, therefore, unreliably measured, numbers of mapped reads (Additional file 1). Following filtering, we identified for further comparisons 5486 reliable genes out of the initial set of 17,567 genes.However, the Shambhala algorithm outputs with microarray Affymetrix Q-dataset (Fig. 4c) again supported biological type-specific clustering for most of the samples, irrespective of their experimental microarray or sequencing platform. Again, the performance of Shambhala with Illumina HiSeq 2000 Q-dataset (Fig. 4d) was better than QN and DESeq/DESeq2 but worse than for the Affymetrix Q-dataset. To our knowledge, this was the first study when the microarray and RNA sequencing data were successfully harmonized. However, as before, the biologically similar A\u2009+\u2009C and B\u2009+\u2009D sample types were merged on the dendrogram, which most probably stresses natural limitations of the Shambhala harmonization tool (Fig. 4c).Although attempts to develop universal cross-platform transcriptome harmonization technique are known for more than a decade, the acceptable performance was shown before only for harmonization of up to two expression datasets [20, 21]. In this study, we developed a new method termed Shambhala suitable for the universal, platform-agnostic harmonization. Unlike previous techniques, Shambhala enables simultaneous harmonization of multiple gene expression datasets, with the standardized uniformly shaped gene expression output. We used the rationale of transforming the experimental expression profiles into the shape of a pre-selected known gene expression platform. Transformation of different sample profiles into the standard definitive form is done for all profiles independently upon other profiles under harmonization. Another distinguishing feature of Shambhala protocol is that any single profile cannot be transformed alone into the definitive shape. Instead, it should be reshaped into to the Q-form within an auxiliary calibration dataset (P0-dataset).In this study, we tried two sets of expression profiles (obtained using microarray Affymetrix and Illumina HiSeq 2000 platforms) from the GTEx project [30] as the reference definitive dataset, and the MAQC [26] and SEQC [27] datasets for validation of Shambhala algorithm. The latter two datasets were selected because they contain gene expression data for the same four types of biosamples profiled using different experimental platforms.The criteria for selecting the auxiliary calibration dataset (P0-dataset) were to provide the best merging of biologically relevant profiles after harmonization. During the training stage, we selected the P0-dataset, which could ensure the good-quality harmonization of the MAQC dataset, namely for the profiles obtained using the Affymetrix and Agilent microarray platforms. Importantly, we did not observe good clustering quality when trying the same GTEx dataset as both P0 and Q, so we had to select another dataset (originated from the CustomArray platfrom) as P0.We validated Shambhala performance for three experimental platforms from the MAQC and four \u2013 from the SEQC dataset. In the latter case, three microarray platforms were merged with one RNA sequencing platform. Shambhala could effectively convert the transcriptomes from multiple platforms, into a standard uniformly shaped form (Fig. 5). In both cases, we showed that Shambhala method significantly outperformed the existing agnostic multi-platform normalization tools, QN [13] and DESeq/DESeq2 [16, 17]. Unlike the other methods, Shambhala could allocate biological sample type-specific clustering of the expression profiles, even for the comparison of microarray versus RNA sequencing data. The highly similar biosamples A and C could be efficiently distinguished from biosamples B and D, also highly similar. Type C and D samples were the mixtures of A and B. Type A, therefore, was 100% A, type B \u2013 100% B, type C \u2013 75% A and 25% B, type D \u2013 25% A and 75% B. However, in neither trial could the algorithm distinguish between the A vs C, or B vs D biosamples. Nevertheless, the method may afford simultaneous harmonization of any number of transcriptomes obtained using any number of experimental platforms; the method\u2019s quantitative performance is only limited by the capacity of a hardware used and/or calculation facilities.The Shambhala performance with the NGS reference definitive dataset appeared better than for QN or DESeq2 normalization, but somewhat worse than for Shambhala with microarray Affymetrix reference dataset.In the present form Shambhala data harmonizer tool was implemented only for the human gene expression data, with the species-specificity being dependent on the reference definitive and auxiliary calibration datasets. Its further adaptation to other organisms is a technical task that would require a representative sampling of gene expression data to complete good quality P0 and Q datasets.Finally, we suggest that the Shambhala approach, or its further modifications, can be a perspective candidate for a massive platform-agnostic harmonization technique enabling direct comparisons of the data accumulated in different laboratories using different equipment and reagents.We presented here a new approach, termed Shambhala, to universal harmonization of gene expression profiles obtained using multiple experimental platforms, for both microarray hybridization and RNA sequencing methods. In this application, Shambhala algorithm was tuned and applied for the comparisons of human gene expression profiles. During harmonization, every single gene expression profile is transformed into the definitive shape using the reference gene expression dataset. We showed that unlike any previous methods, Shambhala may enable biologically meaningful harmonization of gene expression data obtained using three or four experimental platforms.The code for Shambhala was written as further modification and upgrade of the R package CONOR [20]. The whole code was arranged as the R package HARMONY. This package, as well as a code example for Shambhala application are deposited at Github, https://github.com/oncobox-admin/harmony.The cluster dendrograms were built using R package dendextend. The reliability of hierarchical clustering was assessed with the bootstrap procedure using the R package pvclust.Differential expression of RNA-seq dataDistance-weighted discriminationEmpirical BayesFrozen robust multi-array analysisGene expression omnibusGene quantilesGenotype-tissue expressionMicroarray quality controlPlatform-independent latent Dirichlet allocationQuantile discretizationQuantile normalizationSequencing quality controlCross-planform normalizationFinancial support was provided by the Russian Science Foundation grant no. 17-75-30066.All the data, including the definitive and auxiliary calibration datasets, as well as datasets before and after harmonization, are provided as Additional file 2 (definitive and auxiliary calibration datasets), Additional file 3 (harmonized MAQC gene expression profiles), and Additional file 4 (harmonized SEQC gene expression profiles). The whole code was arranged as the R package HARMONY. This package, as well as a code example for Shambhala application, are deposited at Github, https://github.com/oncobox-admin/harmonyThe datasets analyzed during the current study are available in the GEO repository, https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE45878\nhttps://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5350\n\nhttps://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE47792\n\nhttps://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE56457\nCurrent research did not involve any new human material. All the gene expression data that were used for research, including the datasets from MAQC, SEQC and GTEx projects, were taken from publicly available repository Gene Expression Omnibus (GEO), and had been previously anonymized by the teams, who had worked with them.Current research did not involve any new human material.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.To ensure comparable harmonization results for the datasets of different size, we developed the following procedure. The profiles from different platforms are sometimes completely different, and to make the gene expression distribution comparable for each profile before transformation into the Q-shape, we should equalize it using another pre-defined dataset called auxiliary calibration dataset (P0). In other words, it means that each individual gene expression profile under harmonization, say i, is transformed into the Q-shape not within the original dataset of unharmonized profiles from certain experimental platform, but rather being taken alone, and then merged with P0. Namely, we quantile-normalize [13] profile i with the dataset P0, which produces the dataset P for further transformation. This dataset P is then transformed into the shape of the dataset Q, thus producing the dataset P1. From this dataset P1, only the transformed single profile i is taken for further analysis. This procedure is then applied to all other gene expression profiles which need to be harmonized (Fig. 1).\n\nFig. 1\nSchematic representation of Shambhala pipeline for harmonization of gene expression data. Various profiles from samples (1\u2026 N) obtained at different platforms are taken one-by-one, merged with an auxiliary calibration dataset P0 and then quantile-normalized with it. This produces the dataset P, which is then transformed into the shape of the definitive dataset Q; during transformation, only the dataset P changes, while Q remains constant. The result of such a conversion, dataset P1, contains the transformed profile for sample i, which is considered harmonized. Profiles from all other samples (1,\u2026,N) are harmonized one-by-one using the same algorithm\nWe used published gene expression data from the Microarray Quality Control [26]; GEO accession number GSE5350) and Sequencing Quality Control, SEQC [27]; GSE47792 and GSE56457) projects (Table\u00a01). Both MAQC and SEQC projects investigated compatibilities of gene expression profiles obtained using various microarray and sequencing platforms for the same set of four sample types (named A, B, C, D), each done in multiple replicates. Type A samples were the commercially available Stratagene Universal Human Reference RNA specimens for all but brain human tissues; type B samples \u2013 also commercially available the Ambion Human Brain Reference RNA. Type C and D samples were the mixtures of A and B with the A:B ratios of 3:1 and 1:3, respectively. Type C sample, therefore, was biologically closer to the sample A, and type D \u2013 to the sample B.Table 1\nMAQC and SEQC project data used for Shambhala validation\nTo test Shambhala, we took data from three experimental platforms for MAQC dataset and from four platforms for SEQC. All gene expression profiles were harmonized using three alternative methods:1)\nQuantile normalization, QN [13].\n\u00a02)\nDifferential expression analysis for sequence count data, DESeq [16]/DESeq2 [17] using the estimateSizeFactors module. To make the microarray data formally suitable for DESeq/DESeq2 normalization, we took an integer part of all microarray-measured expression level values for each gene and each sample. The intensity values for microarray-measured signal were taken as they were deposited in GEO repository, i.e. after device-dependent primary background correction or equilibration but before any cross-platform transformation or harmonization. Although the DESeq/DESeq2 method was designed for normalization of NGS data and assumes that the count data follow a negative binomial distribution, there were several examples when DESeq/DESeq2 was formally applied to rounded microarray data, both in model investigations based on microarray profiles [34] and for processing human patient\u2019s data [35, 36]. Moreover, having applied the negative binomial regression followed by the Pearson chi-squared test, we found that although the MAQC microarray gene expression values were not distributed according to negative binomial law (particularly for the Illumina GPL2507 and Agilent GPL1708 platforms; Fig. 2a), the SEQC microarray profiles (platforms Illumina GPL10558, as well as Affymetrix GPL17930 and GPL16043) matched the negative binomial distribution (Fig. 2b).\n\u00a03)\nShambhala harmonization with two different GTEx definitive datasets (obtained using either microarray Affymetrix or NGS Illumina HiSeq 2000 platforms). Shambhala method was compared with other above normalization techniques (QN, DESeq/DESeq2) because they are popular tools used for merging data from multiple datasets. The standard harmonization methods such as XPN [20, 21] are not applicable because they enable comparisons of only up to two datasets.\n\u00a0\n\nFig. 2\nPearson chi squared test p-value for gene expression levels. The null hypothesis was that gene expression level do not match the negative binomial law. The optimal parameters for negative binomial distribution for every gene were first assessed using the glm.nb R function, and then the applicability of negative binomial law was checked using the chisq.test function. Panel a: MAQC data (platforms Agilent GPL1708, Affymetrix GPL570, Illumina GPL3507). Panel b: SEQC data (platforms Illumina HiSeq 2000 GPL11154, microarray platforms Illumina GPL10558, Affymetrix GPL17930 and GPL16043)\nThe results (Fig. 3) suggest that the clustering following QN (Fig. 3a) and DESeq/DESeq2 (Fig. 3b) both occur on a platform-specific basis that ignores the biological nature of biosamples under comparison. All the expression profiles are clustered into the three major groups specific only to the microarray platforms used (shown by cyan, yellow and black markers on the figure). In contrast, following Shambhala harmonization with Affymetrix definitive dataset (Fig. 3c) we observed sample type-specific clustering where the biologically similar samples A\u2009+\u2009C and B\u2009+\u2009D formed clear-cut separate clusters. Shambhala harmonization with HiSeq 2000 definitive dataset produced results of an intermediate quality between Shambhala with Afftmetrix Q-set and QN/DESeq2 normalization (Fig. 3d). However, neither algorithm could correctly distinguish between the samples A and C or B and D, which is the obvious limitation of our approach.\n\nFig. 3\nHierarchical clustering at the level of individual gene expression for MAQC project data. Panel a \u2013 results following quantile normalization (QN); b \u2013 DESeq/DESeq2; c \u2013 Shambhala with Affymetrix microarray Q-dataset; d \u2013 Shambhala with Illumina HiSeq 2000 Q-dataset. Panel e \u2013 legend explaining origin of biosamples A, B, C, D and experimental platform in the project. More detailed view of the dendrograms is given in Additional file 5\nThe results obtained (Fig. 4) suggest that as in the previous case, the QN and DESeq/DESeq2 methods provide purely platform-specific outputs ignoring the biological composition of biosamples tested (Fig. 4a and b, respectively; platforms indicated by the lower marker), thus giving four major clusters specific to the above experimental platforms.\n\nFig. 4\nHierarchical clustering at the level of individual gene expression for SEQC project data. Panel a \u2013 results following quantile normalization (QN); b \u2013 DESeq/DESeq2; c \u2013 Shambhala with Affymetrix microarray Q-dataset; d \u2013 Shambhala with Illumina HiSeq 2000 Q-dataset. Panel e \u2013 legend explaining origin of biosamples A, B, C, D and experimental platform in the project. To facilitate the visual analysis of the hierarchical clustering dendrogram, we selected randomly only 20 profiles out of 1324 that were obtained using the Illumina HiSeq 2000 (GPL11154) platform. More detailed view of the dendrograms is given in Additional file 6\nIn should be mentioned that for all the platforms investigated, Shambhala tool produced uniformly shaped and similarly distributed gene expression density profiles (Fig. 5), thus confirming its ability to standardize various types of experimental outputs; note the initial distribution profiles were highly different among the experimental platforms.\n\nFig. 5\nAveraged expression profile for samples of type A before (upper row, panels a to d) and after (lower row, panels e to h) the Shambhala harmonization. The profiles were obtained using the platforms Illumina HiSeq 2000, GPL11154 (panels a and e), Illumina HumanHT-12\u2009V4.0 expression beadchip, GPL10558 (b and f), Affymetrix Human Gene 2.0 ST Array, GPL17930 (c and g), and Affymetrix GeneChip PrimeView Human Gene Expression Array, GPL16043 (d and h)\n\n\nAdditional file 1:\nDescription and validation of the reliability filter for the results of NGS gene expression profiling (DOCX 204 kb)\n\n\n\nAdditional file 2:\nDefinitive (Q) and auxiliary calibration (P0) datasets for the Shambhala method. (XLSX 42198 kb)\n\n\n\nAdditional file 3:\nHarmonized MAQC gene expression profiles. (XLSX 45943 kb)\n\n\n\nAdditional file 4:\nHarmonized SEQC gene expression profiles. (XLSX 219245 kb)\n\n\n\nAdditional file 5:\nA detailed view of hierarchical clustering for gene expression levels for MAQC project data after application different harmonization methods. (PPTX 857 kb)\n\n\n\nAdditional file 6:\nA\u00a0detailed view of hierarchical clustering for gene expression levels for SEQC project data after application different harmonization methods. (PPTX 647 kb)", "s12859-019-2677-9": "Protein ubiquitination occurs when the ubiquitin protein binds to a target protein residue of lysine (K), and it is an important regulator of many cellular functions, such as signal transduction, cell division, and immune reactions, in eukaryotes. Experimental and clinical studies have shown that ubiquitination plays a key role in several human diseases, and recent advances in proteomic technology have spurred interest in identifying ubiquitination sites. However, most current computing tools for predicting target sites are based on small-scale data and shallow machine learning algorithms.As more experimentally validated ubiquitination sites emerge, we need to design a predictor that can identify lysine ubiquitination sites in large-scale proteome data. In this work, we propose a deep learning predictor, DeepUbi, based on convolutional neural networks. Four different features are adopted from the sequences and physicochemical properties. In a 10-fold cross validation, DeepUbi obtains an AUC (area under the Receiver Operating Characteristic curve) of 0.9, and the accuracy, sensitivity and specificity exceeded 85%. The more comprehensive indicator, MCC, reaches 0.78. We also develop a software package that can be freely downloaded from https://github.com/Sunmile/DeepUbi.Our results show that DeepUbi has excellent performance in predicting ubiquitination based on large data.Ubiquitin was first discovered by Goldstein et al. in 1975 [1]. Ubiquitination, covalent attachment of ubiquitin to a variety of cellular proteins, is a common post-translational modification (PTM) in eukaryotic cells [2]. In the process of ubiquitination, ubiquitin is attached to substrates on lysine (K) residues by a three-stage enzymatic reaction. There are three enzymes involved-ubiquitin activating enzyme (E1s), ubiquitin conjugating enzyme (E2s) and ubiquitin ligating enzyme (E3s), which work one after another [3\u20135]. The ubiquitination system is responsible for many aspects of cellular molecular function, such as protein localization, metabolism, regulation and degradation [4\u20137]. It also participates in the regulation of various biological processes such as cell division and apoptosis, signal transduction, gene transcription, DNA repair and replication, intracellular transport and virus budding [4, 5]. Evidence has shown that ubiquitination has a close relationship with cell transformation, immune response and inflammatory response [8]. Abnormal ubiquitination status is also involved in many diseases. For example, the ubiquitination of metastasis suppressor 1, mediated by the skp1-cullin1-F- box beta-transducin repeat-containing protein, is essential for regulating cell proliferation and migration in breast and prostate cancers [9].Due to the roles of ubiquitination, the precise prediction of ubiquitination sites is particularly important. Conventional experimental methods are time-consuming and labour-intensive, and thus computational methods are necessary as a supplementary approach [10, 11]. In recent years, a variety of machine learning methods have been applied to predict protein ubiquitination sites. Tung and Ho [12] developed a ubiquitination site predictor UbiPred, using support vector machine (SVM) with 31 informative physicochemical features selected from the published amino acid indices [13]. Radivojac [14] used a random forest algorithm to develop a predictor, UbPred, in which 586 sequence attributes were employed as the input feature vector. Zhao [15] adopted an ensemble approach to the voting mechanism. Lee [16] designed UbSite, which uses an efficient radial basis function (RBF) kernel to identify ubiquitination sites. Chen [17] proposed a predictor, CKSAAP_UbSite, using the composition of k-spaced amino acid pairs (CKSAAP). Cai [18] proposed a predictor utilizing the nearest neighbour algorithm. Chen [19] proposed a new tool, UbiProber, which was designed for general and specific species. Chen [20] developed hCKSAAP_UbSite by integrating four different types of predictive variables. Qiu [21] developed iubiq-lys using support vector machine. Cai and Jiang [22] used multiple machine learning algorithms to predict ubiquitination sites. Wang [23] designed a tool, ESA-UbiSite, using an evolutionary algorithm (ESA). In addition, there are many other predictors such as UbiSite [24], UbiBrowser [25], RUBI [26], the WPAAN classifier [27], MDDLogoclustered SVM models [28] and the non-canonical pathway network [29]. Although various ubiquitination site predictors have been developed, there are still limitations. As noted above, the existing computational methods for predicting ubiquitination sites are shallow machine learning methods and their datasets are small. However, a large amount of biomedical data has been accumulated and shallow machine learning algorithms do not handle big data well. In this study, we propose a lysine ubiquitination predictor, DeepUbi, using a deep learning framework on a large dataset.We use the biggest data repository designed for protein lysine modification to learn the DeepUbi predictor. A convolutional neural network, a deep learning framework, is adopted to predict ubiquitination. It is composed of a convolutional layer, a nonlinear layer and a pooling layer. Convolutional neural networks can learn a large number of mapping relations between input and output without any precise mathematical expression between the input and output. We construct six steps, including inputting the fragment, constructing an embedding layer, building multi-convolution-pooling layers, adding features, constructing fully connected layers, and the output layer. The deep learning framework is first used to predict ubiquitination.Four better encoding schemes are adopted in the feature construction, One-Hot encoding, the physicochemical properties, the composition of k-spaced amino acid pairs (CKSAAP) and the pseudo amino acid composition. One-Hot plus CKSAAP have the best performance with and AUC of 0.9066 in the cross-validation.In the data, the sequence motif analysis shows that there are differences between positive and negative fragments. Thus, it is feasible to obtain classification information from the peptide itself. Different features are adopted to train the model. The hybrid of One-Hot and CKSAAP is selected as the best, with an AUC of 0.9066.DeepUbi has better performance than the existing tools. Researchers could use the predictor to select potential candidates and conduct experiments to verify them. This will reduce the range of candidate proteins and save time and labour. The sequence analysis of the ubiquitination will provide suggestions for future work.In the future, we will investigate other feature constructions that may better extract the properties of samples. Second, we aim to improve performance by increasing the depth and model parameters through system learning. The current method may also be used to identify other PTM sites in proteins.In this work, we propose a new ubiquitination predictor, DeepUbi, which uses a deep learning framework and achieves satisfactory success with the biggest data set. DeepUbi extracts features from the original protein fragments with an AUC of 0.9066 and an MCC of 0.78. We construct six steps including inputting fragment, constructing an embedding layer, building multi-convolution-pooling layers, adding features, constructing fully connected layers, and output layer. The deep learning framework is first used in prediction of ubiquitination. However, DeepUbi is not too deep, as we only use two convolution-pooling structures. We also develop a software package for DeepUbi that can be freely downloaded from https://github.com/Sunmile/DeepUbi. The deep learning model is an effective prediction method and will improve accuracy by increasing the depth in the future.A good feature can extract the correlation of instinct ubiquitination characters and the targets from peptide sequences [34]. Four better feature encoding schemes are adopted, One-Hot encoding, the physicochemical properties, the composition of k-spaced amino acid pairs and the pseudo amino acid composition.One-Hot Encoding.The conventional feature representation of amino acid composition uses 20 binary bits to represent an amino acid. To deal with the problem of sliding windows spanning out of the N-terminal or C-terminal, one additional bit is appended to indicate this situation. Then, a vector of size (20\u2009+\u20091) bits is used to represent a sample. For example, the amino acid A is represented by \u2018100000000000000000000\u2019 and R is represented by \u2018010000000000000000000\u2019.In PTM site prediction, physicochemical properties are essential to extract information for a fragment or protein. Tung [12] proposed an informative physicochemical property mining algorithm that could quantify the effectiveness of individual physicochemical properties in prediction. They used the value of the main effect difference (MED) [35] to estimate the individual effects of physicochemical properties. The property with the largest MED is the most effective in predicting ubiquitination sites. In the study, 31 informative physicochemical properties are selected as the features for calculation, and are listed in Additional\u00a0file\u00a01: Table S1.Pseudo Amino Acid Composition (PseAAC).Chou\u2019s pseudo amino acid composition is a set of discrete serial correlation factors combined with traditional 20 amino acid components [37]. In the study, we select 20 correlation factors and the weight of these factors is 0.05, and a 40-dimension vector is acquired.where N represents the batch size of the training set and xn and yn represent the n-th protein fragment and its label, respectively. Using the Adam optimizers, DeepUbi is trained based on a variety of super-parameters such as the batch size, maximum epoch, learning rate, dropout rate and convolution blocks.where Sn represents the sensitivity, Sp is the specificity, Acc is the accuracy, and MCC is the Matthew\u2019s correlation coefficient. The ROC (Receiver Operating Characteristic) curves and the area under the ROC curve (AUC) are usually used to evaluate the classifier\u2019s resolving power.AccuracyArea under the ROC curveComposition of k-spaced amino acid pairsConvolutional neural networkInformative physicochemical propertiesMathew\u2019s correlation coefficientMain effect differencePseudo amino acid compositionPost-translational modificationRadial basis functionRectified linear unitSensitivitySpecificitySupport vector machineThis work is supported by grants from the Natural Science Foundation of China (11671032) and the 2015 National Traditional Medicine Clinical Research Base Business Construction Special Topics (JDZX2015299). The funders have no role in the design of the study, collection, analysis, and interpretation of the data or writing the manuscript.A total of 121,742 ubiquitination sites were collected from PLMD database (http://plmd.biocuckoo.org/) and the proteins were retrieved from UniProt (https://www.uniprot.org/). The data is provided on website https://github.com/Sunmile/DeepUbi and the file name is \u201cRaw Data\u201d.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.For the series of hyperparameter choices, we obtain a set of better performing hyper-parameters, which are shown in Table\u00a01. Using a set of clear and effective metrics defined in Eq. 4 to measure the quality of predictors, we considered how to objectively derive the values. Three different verification methods are generally used to evaluate the predictive performance: the independent dataset test, sub-sampling test and jackknife test [30]. The jackknife test can exclude the \u201cmemory\u201d effect and the arbitrariness problem because the outcome obtained by the jackknife cross-validation is always unique for a given benchmark dataset [21]. However, it is time-consuming, especially for big datasets. In this study, k-fold cross validation was utilized to evaluate the performance of the proposed predictors because of the large dataset.Table 1\nThe values of super-parameter tuning\nFirst, the 4-fold, 6-fold, 8-fold and 10-fold cross validations are executed 10 times on the simple One-Hot encoding scheme. The results are shown in Table\u00a02. All of the accuracies are greater than 85% and the highest accuracy reaches 88.74%, illustrating the robustness of the CNNUbi. The ROC curves and AUC values are shown in Fig.\u00a01 and are more intuitive, and the largest AUC value was 0.89. These results show that the deep learning framework learns some instinct information and has good performance. To obtain more information, we add three other features into the One-Hot encoding scheme (see Table\u00a03 and Fig.\u00a02). In the 10-fold cross-validation, all the ROC curves are very close to each other. The One-Hot plus CKSAAP encoding scheme clearly performs the best in all of these features. We call it DeepUbi with an AUC of 0.9066 and MCC of 0.78.Table 2\nThe results of 4-, 6-,8-,10-fold cross-validations with the One-Hot feature\n\n\nFig. 1\nROC curves of different cross-validations. ROC curves and their AUC values of 4-, 6-, 8-, and 10-fold cross validations with the One-Hot encoding scheme\nTable 3\nThe results of four different encoding schemes in the 10-fold cross-validation\n\n\nFig. 2\nROC curves of different feature constructions. ROC curves and their AUC values of four features in the 10-fold cross validation. These curves are very close to each other which illustrate the robustness of the model\nOur DeepUbi predictor was obtained using balanced data. In the experimentally verified ubiquitination and non-ubiquitination data, the ratio of positive and negative peptides was 1:8. We also tested the performance on naturally distributed data when the algorithm was trained with balanced data. The results in Table\u00a04 illustrate that the performance is slightly worse than with balanced data.Table 4\nThe results for naturally distributed DeepUbi data\nA comprehensive comparison of our models with the available sequence-based predictors was performed and the corresponding data and results are shown in Table\u00a05. In the last decade, many researchers have contributed to the prediction and research of ubiquitination sites in proteins. The comparison shows that the deep learning model performs very well on big datasets. The predictors improved the accuracy by adding new features, using a variety of machine learning algorithms or adding new datasets. The precision of the predictors is approximately 0.8. In this study, we propose the DeepUbi predictor and apply a deep learning framework with more accuracy. The AUC close to 0.9 and other indicators of accuracy, sensitivity and specificity are also better than those of existing methods. These results suggest that DeepUbi learned deeper characteristics.Table 5\nComparison of DeepUbi and other ubiquitination prediction tools\nTo eliminate the impact of data volume differences and make a more vivid comparison, we conduct additional experiments. We randomly select the same number of positive and negative samples as the existing predictor from our data\u00a010 times. Each sample set is tested with 10 cross-validations, and the average results are listed in Table\u00a06. Comparison of Table 5 and Table 6 shows that the DeepUbi results are much higher than those of other predictors for the same number of samples. For example, the data in UbiPred has an Acc of 84.44%, Sn of 83.44%, Sp of 85.43%, AUC of 0.85 and MCC of 0.69. Selecting the same number UbiPred data as the test set 10 times, the average result for DeepUbi is an Acc of 98.77%, Sn of 98.87%, Sp of 98.67%, AUC of 0.99 and MCC of 0.98. The AUC values of DeepUbi are close to 0.9, illustrating the performance of deep learning.Table 6\nThe DeepUbi results for the same number of samples as the other existing tools\nTo illustrate the performance of our predictor, we also conduct an analysis using the training data. First, the probabilistic histogram of composition of flanking amino acids surrounding the ubiquitination candidate sites is generated, as shown in Fig.\u00a03a and b. Amino acid residues Ala (A), Glu (E), Leu (L), Arg (R) and Ser (S) appear more ratio in positive data (ubiquitination fragments), while Cys (C), Phe (F), His (H), Ile (I) and Val (Y) are more enriched in negative data (non-ubiquitination fragments). Next, a well-known tool, Two Sample Logo [31], is applied to detect the position-specific amino acid composition difference between the training data, and the sequence logo is shown in Fig. 3c. The results reveal the dependencies of flanking amino acids around the substrate sites.\n\nFig. 3\nDifferent sequence analysis charts about ubiquitination and non-ubiquitination peptides. a A bar chart to compare the number of flanking amino acids surrounding the ubiquitination and non-ubiquitination peptides. b A circular chart to compare the percentage of flanking amino acids surrounding the ubiquitination and non-ubiquitination peptides. c Two Sample Logos web-server to calculate and visualize differences between ubiquitination and non-ubiquitination peptides\nIn this study, the ubiquitination data is collected from the PLMD (v3.0, June. 2017) database [32], which is the biggest online data repository designed for protein lysine modification. The original data contains 121,742 ubiquitination sites from 25,103 proteins. If the data contains homologous samples, it would increase the bias of results. We remove the redundant protein sequences to eliminate homology bias using the CD-HIT web server [33], which is freely available at http://weizhongli-lab.org/cd-hit/, and obtains 12,053 different proteins with \u226430% sequence identity. A sliding window with the length of 15\u2009\u00d7\u20092\u2009+\u20091\u2009=\u200931 is used to intercept the protein sequences with lysine residues in the centre. If the upstream or downstream residues of a protein are less than 15, the lacking residue is filled with a \u201cpseudo\u201d residue \u2018X\u2019. There are too many negative peptides compared to the positive peptides. To obtain a better predictor, we select the negative samples by deleting the redundant segments using 30% identity to ensure that none of the segments had \u226530% pair-wise identity in the negative peptides [24]. Finally, we obtain a training dataset containing 53,999 ubiquitination and 50,315 non-ubiquitination fragments. A detailed flow chart of these steps is shown in Fig.\u00a04.\n\nFig. 4\nFlow chart of the data collection and processing. Firstly, collecting the raw proteins and then removing the redundant protein sequences with CD-Hit; secondly, intercepting the protein sequences with a 31 sliding window to get the positive and negative fragments; at last, using 30% identity in negative samples to get a balanced training data\nThe CKSAAP encoding scheme is the composition of k-spaced residue pairs (separated by k amino acids) in the protein sequence, which is useful for predicting protein flexible or rigid regions [36]. For example, there are 441 residue pairs (i.e., AA, AC, ..., XX). Therefore, the feature vector can be defined as (1)where Ntotal is the total number of k-spaced residue pairs in the fragment and NAA is the number of amino acid pair AA in the fragment. Each component in the vector represents the contribution of k-spaced amino acid pairs. For instance, the AA component is represented as \\( \\frac{N_{AA}}{N_{total}} \\). In this paper, k\u2009=\u20090, 1, 2, 3, 4, and a 441\u2009\u00d7\u20095\u2009=\u20092205 vector was obtained by the CKSAAP encoding scheme.Deep learning, which evolved from the acquisition of big data, and the power of parallel and distributed computing have facilitated major advances in numerous domains such as image recognition, speech recognition, and natural language processing [38]. Every protein is a sentence, and residues in the protein sequence can be seen as \u201cwords\u201d. The prediction of ubiquitination can be seen as a \u2018natural language prediction\u2019 (NLP) task. Therefore, we propose a convolutional neural network (CNN) deep learning model and obtain good prediction performance on a large data set. A convolutional neural network (CNN) is a deep learning framework. It is composed of a convolutional layer, a nonlinear layer and a pooling layer. Our model is constructed with six steps (Input a fragment, Construct an embedding layer, Build multi-convolution-pooling layers, Add features, Construct fully connected layers, and an Output layer), as shown in Fig.\u00a05a.\n\nFig. 5\na Flow chart of the CNN deep learning model. b An example of convolution-pooling structure. a Input a fragment and encode; construct an embedding layer; build multi-convolution-pooling layers; construct fully connected layers; and then get the output. b Use different filters with different sizes to get a series of feature maps; and then use a max-pooling and concatenating together to form a feature vector. Finally, the softmax function regularization is used to get the classification\nThe input protein fragment representation is x\u2009\u2208\u2009RL\u2009\u00d7\u200921, where L is the length of the fragment. The first layer is the embedding layer, which maps input vectors into low-dimensional vector representations. It is essentially a lookup table that we learn from data. E\u2009=\u2009xWe, where e is the embedding dimension, We is the embedding weight matrix and E\u2009\u2208\u2009RL\u2009\u00d7\u2009e is the embedding matrix, which is a continuous product. Then, we assign the embedding matrix E as an image and use the convolutional neural network to extract features. Because the adjacent residues in the fragments are always highly correlated, one dimensional convolution can be used. The width of the convolution kernel is the dimension of the embedding vector. The height is a super parameter, which is a manual set. For example, if there is a convolution filter with size ak, then a feature map is obtained by the convolution (2)where f is the activation function, which is a rectified linear unit (ReLU) [39], w is the weight vector and \\( {\\mathrm{z}}_k\\in {R}^{L-{a}_k+1} \\). The number of convolution filters of size ak is also set. The feature map obtained from different convolution kernels is a different size, so a max-pooling function is use to maintain the same dimension. The final eigenvector h is then obtained. For more intuitive understanding, see Fig. 5b. For the first model, CNNUbi, we use the features obtained from the last step without additional features, i.e., hnew\u2009=\u2009h. For comparison, the second model, DeepUbi, is built with additional features and hnew\u2009=\u2009[h,\u2009b], where b is the additional features. Finally, each of the two output units has a score between 0 and 1, illustrating by the softmax equation \\( {p}_i=\\frac{e^i}{\\sum_j{e}^j} \\). Here, i\u2009=\u2009Fcwo represents the input of class unit i, Fc is the output of the fully connected layer and wo is the weight matrix. The cross-entropy objective function is assigned as the cost function Add features (3)A confusion matrix is a visual display tool for evaluating the quality of classification models. Each column of the matrix represents the sample situation of the model prediction and each row of the matrix represents the actual situation of the sample. There are four values in the matrix, where TP represents the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives. In the literature, the following metrics based on the confusion matrix are often used to evaluate the performance of a predictor (4)\n\nAdditional file 1:\nTable S1. The 31 informative physicochemical properties and their corresponding MED (main effect difference) scores. (XLSX 42 kb)", "s12859-018-2568-5": "The utilization of high resolution genome data has important implications for the phylogeographical evaluation of non-human species. Biogeographical analyses can yield detailed understanding of their population biology and facilitate the geo-localization of individuals to promote their efficacious management, particularly when bred in captivity. The Geographic Population Structure (GPS) algorithm is an admixture based tool for inference of biogeographical affinities and has been employed for the geo-localization of various human populations worldwide. Here, we applied the GPS tool for biogeographical analyses and localization of the ancestral origins of wild and captive gorilla genomes, of unknown geographic source, available in the Great Ape Genome Project (GAGP), employing Gorillas with known ancestral origin as the reference data.Our findings suggest that GPS was successful in recapitulating the population history and estimating the geographic origins of all gorilla genomes queried and localized the wild gorillas with unknown geographical origin <\u2009150\u2009km of National Parks/Wildlife Reserves within the political boundaries of countries, considered as prominent modern-day abode for gorillas in the wild. Further, the GPS localization of most captive-born gorillas was congruent with their previously presumed ancestral homes.Currently there is limited knowledge of the ancestral origins of most North American captive gorillas, and our study highlights the usefulness of GPS for inferring ancestry of captive gorillas. Determination of the native geographical source of captive gorillas can provide valuable information to guide breeding programs and ensure their appropriate management at the population level. Finally, our findings shine light on the broader applicability of GPS for protecting the genetic integrity of other endangered non-human species, where controlled breeding is a vital component of their conservation.The importation of western gorillas (Gorilla gorilla) from their native habitat in Africa to North American zoos began over 100\u2009years ago [1]. While most wild gorillas transferred initially died shortly after arrival [2, 3], those introduced subsequently between the 1930s and 1970s survived for several decades [3]. Overall, at least 283 wild gorillas have been imported to North America [4]. However, since their inclusion under the protection of the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) in 1975 there have been no wild born gorillas added to the captive population. Notably for a majority of the gorillas in captivity sufficient information pertaining to their biogeographic origin is unavailable [5]. Gorillas were pronounced as critically endangered in 2007 [6]; in the wild their population is rapidly dwindling owing to severe habitat encroachment, the illegal bushmeat trade and susceptibility to diseases such as Ebola. The limited availability of information regarding the biogeographic ancestry of gorillas has likely constrained their management pertaining to maximizing genetic diversity at the species level, which can be achieved by preventing inbreeding among related individuals. It is noteworthy that unlike in the wild, captive gorillas have been revealed as significantly more admixed from two or more genetically distinct wild born populations [1, 4, 7].Given the strong correspondence between geography and genetics [8, 9], a number of strategies have focused on the delineation of the precise geographic origin of human populations using high-resolution genetic data. The Geographic Population Structure (GPS) algorithm is an admixture based tool that has so far been employed for the biogeographical analyses of human populations and is likely superior to other existing methods for the same [9\u201313]. It has been successfully used to reconstruct history of several human populations worldwide [9, 13\u201319]. In brief, it deduces the genomic proximity between the query and reference individuals to determine the likely biogeographical affinity of the former using the geographic coordinates (latitude and longitude) corresponding to the latter as reference.Here we aimed to assess whether the GPS algorithm, essentially designed for biogeographical analyses of human populations could be applied to non-human species with equal precision and efficiency. We investigated the whole genome sequence (WGS) information from 31 gorilla genomes available in Great Ape Genome Project (GAGP) [7] corresponding to two subspecies of western gorillas (Gorilla gorilla), namely western lowland gorilla (Gorilla gorilla gorilla) and Cross River gorilla (Gorilla gorilla dielhi), as well as the eastern lowland gorilla (Gorilla beringei graueri); using the GPS tool we localized the ancestral origins of both wild and captive gorillas of unknown geographic origins, employing those with a known provenance, as reference. Our findings suggest that GPS was successful in inferring the geographic origins and recapitulating the population history of the gorilla genomes queried. It uncovers the broader utility of biogeographical analyses tools, in particular GPS, to facilitate deeper insight into the population biology of endangered non-human species that can foster their efficacious management and conservation.We note that due to the unavailability of geographic coordinates of the reference gorillas within their ancestral countries, the precise prediction accuracy (in km) of the gorillas with known origin was not possible.Specifically, among the wild-born western lowland gorillas of unknown birthplace data, Abe, Oko, Choomba, Paki, and Suzie were assigned to central Cameroon, 100\u2013150\u2009km from Mbam et Djerem National Park. While Tzambo was positioned in Central Congo, halfway between Odzala-Kokoua National Park and Reserve de Chasse de la Lefini; Porta, Katie (B650) and Katie (KB4986) were positioned <\u2009100\u2009km from Reserve de Chasse de la Lefini.Among the captive-born western lowland gorillas, Kowali, Kokomo and Bulera were placed in Congo. The remaining individuals, Azizi, Kolo, Amani, and Sandra all were positioned in Cameroon. Finally, Dian was positioned in southwest Cameroon on the border of Cameroon and Equatorial Guinea. Interestingly, the GPS localization of captive-born gorillas, Victoria, Dian, Sandra, Kolo, and Azizi was congruent with their previously presumed ancestral homes [7].Further, to test whether presence of close relatives have any impact on the outcome of GPS, we repeated the analysis discarding Bulera, Kowali, Suzie and Oko, which were identified as 1st, 2nd or 3rd order relatives of the reference gorillas based on their genomic information [7]. We found that there is no discernible effect of \u2018relationship\u2019 in the outcome of GPS analysis. GPS positioned the remaining individuals at the same geographical coordinates as it did before (without exclusion of the relatives).Great apes such as the gorillas are faced by serious challenges impacting their population size and distribution, in the wild. This has translated into increasing focus on their preservation both in the wild as well as in captivity. While management of gorilla breeding programs in North America have afforded prominent impetus to maximizing genetic diversity in order to avoid inbreeding depression, they have been limited by sufficient information of the phylogeographic ancestries of the individuals bred in captivity. As a result, the analyses of captive born gorillas in North American zoos and sanctuaries has revealed them with high genetic heterozygosity due to admixture between two or more genetically distinct wild born populations leading to an attenuation of the phylogeographic signal [1, 4, 7]. Presently there is limited knowledge of the ancestral origins of founders of most North American captive gorillas [5]. Hence, determination of the native geographical source of captive gorillas can be a valuable tool to foster their population level management.Here we sought to evaluate whether the GPS algorithm, largely employed for biogeographical analyses of human populations [9, 13\u201316] could be applied to non-human species, and to estimate its efficacy in doing the same. We applied the GPS tool to interrogate available gorilla genomes [7] and estimated the ancestral biogeographic affinities of 18 query captive and wild born gorillas, of unknown source.Inference of the biogeographic proximity of individuals, based on genetic data has been challenging and of interest to biologists over decades. The GPS tool correlates the relative proportions of admixture in the query and reference individuals to deduce the likely geographic location of the former based on the geographic coordinates of the latter. Here, we note that our present findings are based on the coordinates corresponding to the geographic centers of the countries, where the reference individuals are documented to have originated, owing to the unavailability of precise regional locations for the same. Despite this, our trial analyses successfully assigned nine out of ten gorillas queried within the countries recorded as their places of birth, reflecting an acceptable genomic-geographic correspondence and reliable predictive accuracy. Further affirmation to the utility of our methods is evidenced in the assignment of the captive-born gorillas, Victoria, Dian, Sandra, Kolo, and Azizi, to locations within countries consistent with their previously inferred ancestral homes [7]. For query western lowland gorilla genomes of unknown source, GPS localized their ancestral origins <\u2009150\u2009km of the National Parks/Wildlife Reserves within the political boundaries of the countries, Equatorial Guinea, Cameroon and the Republic of Congo that are considered as prominent modern-day habitat for western lowland gorillas in general [20] (Fig. 4).These findings also largely resonated with previous mitochondrial haplogroup analysis (Additional file 1 Figure S1) [22]; GPS positioned Abe, Paki, and Oko of haplogroup C1 to central Cameroon and localized gorillas of haplogroup D3, Bulera, Kowali, Porta, and Katie to Congo, consistent with their previously deduced phylogeographic origins [22]. Interestingly, Azizi with mtDNA haplogroup D3, which is predominantly found in Congo [22], was assigned to Cameroon and this was further supported by the results of reAdmix analysis (Table 1) that estimated a prominent Cameroonian admixture proportion for Azizi. This potentially reflects the pedigree of Azizi, whose mother was from Congo and both father and maternal grandfather was from Cameroon. Thus, it can be surmised that while her mtDNA derives from Congo, the majority of her nuclear genome would reflect Cameroonian ancestry. Finally, our current assignment of Kokamo (haplogroup D2) to Congo and Choomba (haplogroup C3) to central Cameroon also coincided with previous results, mentioning that C3 is distributed in central Cameroon along the south bank of the Sanaga River and D2 is found in the Dzanga-Sangha region of Central African Republic, along the border of Congo. [22].It is noteworthy that our results positioned some western lowland gorillas, in close proximity (<\u200950\u2009km) to Gashaka Gumti National Park located in central Cameroon, on its border with Nigeria, in a region that is home to Nigerian chimpanzees and where gorillas have likely never been found. We surmise that this reflects an underlying limitation of the GPS strategy, which is strongly guided by the availability of appropriate reference data, such that our assumption of the geographic centers of countries corresponding to the reference dataset, likely drew the query individuals in this case, farther north from their known southern Cameroonian homeland. Similarly, it is noteworthy that our findings are bereft of any query gorillas being assigned to Gabon, a known major natural homeland for western lowland gorillas in the wild, likely due to the absence of suitable genomic references for the same.Given that the GPS framework is more error-prone for highly admixed individuals [9, 13, 21] we sought to improve our resolution into the biogeographical ancestry of the captive gorillas in our query pool, known to be admixed, by evaluating them using reAdmix [21]. Out of the two most admixed individuals, Azizi and Bulera (Table 1), GPS successfully localized the former to its previously presumed ancestral home, Cameroon, but failed to do so for the latter, manifesting its inherent limitation in interpreting highly mixed individuals. Nonetheless, it assigned the least admixed captive gorillas, Sandra, Kolo, Amani, and Dian, to locations concurrent with their earlier inferred ancestral homes (Fig. 4) [7].Our findings from PCA and ADMIXTURE suggested two prominent population clusters amidst the western lowland gorillas, this is not only reminiscent of previous studies, based on high resolution WGS [7] and microsatellite data [4, 23], but provided enhanced insight into the same. A case in point would be the grouping of Coco, whose birthplace was documented as Equatorial Guinea [7] with Cameroonian gorillas, due to its high genomic affinity with the latter. We surmise this could likely reflect the substantial genomic similarity of gorillas from Equatorial Guinea and southwest Cameroon and that they may constitute one panmictic population. However, a more conclusive understanding of the same would only be achieved with the availability of genomic data of other gorillas from Equatorial Guinea. We also note that a previous WGS based study did not identify any substructure among western lowland gorillas. This is likely because of limited knowledge regarding biogeographical affiliation of gorillas with unknown ancestral home (both captive born and wild born but unknown origin) [24]. However, similar to the present study, it could delineate two distinct clusters of western gorillas along PC1 and could deduce close affinities between gorillas from Cameroon and Equatorial Guinea and one Cameroonian gorilla (likely Helen) with Congolese gorillas.The utilization of high resolution genomic information has important implications for the phylogeographical evaluation of non-human species such as the great apes. Effective conservation of captive and wild populations of gorillas necessitates the delineation of the biogeographic affinities of their founders, so as to facilitate preservation of the population level integrity of genomic signal. This could be particularly relevant for planned introduction of animals, such as those being carried out in Central Africa [25]. Given this context, the current findings revealed the GPS algorithm to function with reasonable accuracy in localizing the ancestral source of gorilla genomes queried, to the countries which constitute their natural homeland, in the wild. When interpreted with adequate caution against the inherent limitations of the GPS tool, these results recapitulate and expand upon previous studies [4, 7, 22] to yield a better insight into the genetic relatedness and biogeography of the gorilla genomes assessed here. To the best of our knowledge this is the first application of the GPS algorithm for interrogating gorilla genomes, of unknown provenance and underscores its broader applicability for geo-localization of other endangered non-human species, particularly those bred in a controlled manner, to bolster their efficient management and conserve their genetic integrity.The dataset employed in this study comprised of 31 gorilla genomes available in Great Ape Genome Project (GAGP) [7]: western lowland gorilla (Gorilla gorilla gorilla, N\u2009=\u200927), eastern lowland gorilla (Gorilla beringei graueri, N\u2009=\u20093), and Cross River gorilla (Gorilla gorilla dielhi, N\u2009=\u20091) (Additional file 1 Table S1). The geographic origins of all individuals denoted here are as per those indicated previously [7]. Most captive gorillas have been demonstrated as highly genetically heterogeneous, when compared to wild born individuals, as a consequence of being admixed from two or more genetically distinct wild born populations [1, 7]. Therefore, we considered them to be of unknown geographic origins, regardless of their recorded geographic source. Overall, 13 wild born gorillas with known birthplace information [7] were deemed to be of known geographic origins, while the remainder whose provenance was ambiguous or undefined were considered to be of unknown origin. We note that while the documented provenance data for wild born gorillas may be largely accurate, nevertheless, discrepancies in cataloguing this information could have occurred in a minority of the cases, and therefore adequate caution is warranted in interpreting the consequent findings. The variant calling file (VCF) was obtained from the GAGP database (http://biologiaevolutiva.org/greatape/data.html). It was converted into PLINK .ped format using VCFtools v.0.1.13 [26]. We used quality control filters as employed previously for this dataset [7]. A pruned subset of Single Nucleotide Polymorphisms (SNPs) that are in linkage equilibrium with each other was generated using --indep-pairwise function in PLINK v1.9 (https://www.cog-genomics.org/plink/1.9/) [27]. During pruning, the window size for SNPs was kept at 50, 5 SNPs were allowed to shift the window at each step, and the r2 threshold was kept at 0.1 (\u2212-indep-pairwise 50 5 0.1). Further, we used 0.05 as MAF threshold and 0.1 as missing genotype threshold by employing --maf 0.05 and --geno 0.1 flags respectively as additional quality control measures. Before pruning the raw dataset comprised of 53,178,815 SNPs. Among them, 52,824,735 SNPs were pruned out for being in linkage disequilibrium (r2\u2009>\u20090.1) generating to a final dataset of 354,080 SNPs.Principal component analysis (PCA) was performed in PLINK v1.9 using --pca command. The top four principal components (PCs) of the variance-standardized relationship matrix were extracted and the top two of the same were plotted.The ancestry of the gorilla genomes was estimated using unsupervised clustering as implemented in ADMIXTURE v1.3 [28]. Admixture analyses were performed for K\u2009=\u20092 and K\u2009=\u20093 as done previously [7]. Despite Cross validation metric (CVE) [28] indicating that K\u2009=\u20092 has the lowest error rate, we chose K\u2009=\u20093 for further analysis so as to accentuate our ability to resolve the western gorilla genomes into the Congolese and Cameroonian clusters. PCA and Admixture plots were generated in R v3.2.3.Biogeographical analysis was performed using the Geographic Population Structure (GPS) algorithm as described previously [9]. The GPS algorithm correlates the admixture patterns of individuals of unknown origins using the admixture fractions (GEN file) and geographical locations or coordinates (GEO file) of reference individuals with known geographical origin. Given samples of unknown geographic origin and admixture proportions that correspond to putative ancestral populations, GPS can convert the genetic distances between the query and the most proximal reference populations into geographic distances. Comparing the admixture proportions of the query with the reference populations, it extrapolates the genomic similarity of the former and infers its geographic origins using the known biogeographical information of the reference. Here we curated the reference gorilla dataset using the \u2018leave-one-out\u2019 procedure at the individual level, as described previously [9] using all available gorillas with a known geographical source.First we sought to trial the accuracy of GPS mediated biogeographical analyses using gorilla genomes of known provenance. Accordingly, we analyzed the genomic information pertaining to ten western lowland gorillas from Cameroon, Republic of Congo, and Equatorial Guinea (Additional file 1 Table S1) and estimated their admixture proportions with respect to the three admixture components corresponding to the reference gorilla genomes.Subsequently we mapped 18 gorilla genomes (17 western lowland gorilla and one eastern lowland gorilla) of unknown geographical origin with respect to the reference dataset, and interpreted their admixture fractions and geographic locations (latitudinal and longitudinal coordinates). Therefore, our GEN file contained three admixture coefficients corresponding to the reference genomes and the GEO file contains the associated geographic coordinates (latitude and longitude). We note that given the unavailability of precise geographical locations for our reference dataset, the coordinates for the geographic centers of the countries where the reference individuals originated, namely Cameroon, Republic of Congo, and Equatorial Guinea [7], were employed for our analyses (Additional file 1 Table S2).Given that the GPS tool is likely less efficient in interpreting the biogeographical affinity of highly admixed individuals [15, 21] and since our query dataset consists of captive gorillas, known to be discernibly genetically more admixed from two or more wild populations [1, 4, 7], we supplemented our analyses by using the reAdmix algorithm [21]. reAdmix treats the tested individual and N reference populations as points inside the standard simplex in K-dimensional space of admixture proportions [21]. It represents the tested individual T as a convex combination reference ancestries: \\( T=\\sum \\limits_{i=1}^N{w}_i{R}_i \\), where Ri\u00a0are admixture vector of the ith reference population, and wi is its contribution. reAdmix tool has been effectively used in estimating biogeographical origin of highly admixed individuals [18, 19, 21].We used the same admixture vectors in the K\u2009=\u20093 dimensional space as we used for the GPS analysis. We have four super-populations of wild western lowland gorillas [7]: Congo, Cameroon, Equatorial Guinea, and West Africa. However, the wild gorillas are not genetically homogeneous within these large regions, and we have identified genetically validated groups of reference individuals within each super-population. There were three groups in Congo, two in Cameroon, and two in West Africa. The groups are denoted by a subscript (e.g. Cameroon_2). These reference populations were used to estimate proportions of wild ancestries in all nine captive gorillas of unknown origin. Note that the cases where the admixture proportions do not sum to 100% could be likely attributed to the absence of one or more wild ancestral populations in the reference dataset. reAdmix analysis (see Table 1) confirms that Victoria is a pure Congo gorilla, Kolo and Amani are predominantly West African, while Azizi and Bulera are highly admixed.Not applicable.Publication costs were funded by Manipal Academy of Higher Education (MAHE), Manipal, India.Great Ape Genome Project (GAGP) data are publicly available through its website http://biologiaevolutiva.org/greatape/. GPS codes and other data necessary for replicating this study is available upon request.This article has been published as part of BMC Bioinformatics Volume 20 Supplement 1, 2019: Selected articles from BGRS\\SB-2018: bioinformatics. The full contents of the supplement are available online at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-1.Not applicable.The authors declare no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Principal Component Analysis (PCA) was performed in PLINK v1.9 and the top two PCs were plotted in R v3.2.3. Our PCA results concur with previous observations of an eastern gorilla - western gorilla contrast along the horizontal principal component (PC1) and vertical differentiation (PC2) among western gorilla genomes [7] (Fig. 1). Two distinct clusters were found among western gorillas along PC1: one predominantly composed of Cameroonian gorillas and the other largely consisted of Congolese gorillas. Notably, Coco whose birthplace was recorded as Equatorial Guinea [7] appeared to group with Cameroonian gorillas, owing to its high genomic proximity with them. This likely alludes to the substantial genomic affinity of gorillas from Equatorial Guinea and southwest Cameroon. However, validation of the same would be feasible only with the availability of high resolution genomic data of other gorillas, from Equatorial Guinea. Among gorillas with unknown birthplace information, Katie (B650) and Katie (KB4986) were clustered at one extreme of Congolese-Cameroonian cline, while Choomba and Amani appeared at the other end.\n\nFig. 1\n Principal Component Analysis (PCA) of gorilla genomes. PCA plot showing genetic differentiation among gorilla genomes. PCA was performed in PLINK v1.9 and the top four principal components (PCs) were extracted. Top two PCs (PC1 and PC2), explaining the highest variance of the data were plotted in R v3.2.3. The colors of the gorilla genomes in the PCA plots corresponds to the location geographic location, they belonged to. The red squares represent gorilla genomes with unknown geographic origin. The X-axis (PC1), depicting eastern-western gorilla differentiation, explains 45% variance while the Y-axis (PC2), indicating clustering among western gorillas, explains 23% variance of the data\nAt K\u2009=\u20093, the eastern lowland gorillas were homogeneously assigned to a unique cluster (k1) while most western gorillas appeared to be a genomic admixture of Cameroonian (k2) and Congolese (k3) components in varying proportions (Fig. 2). Interestingly, the wild-born Akiba-Beri, Choomba, Paki, Oko, and the captive-born Kolo and Amani have maintained their genomic integrity, such that their entire genome consisted of the Cameroonian admixture component. Similarly, Katie (B650) and Katie (KB4986) also appeared as pure-bred and are composed of the Congolese admixture component.\n\nFig. 2\nAdmixture analysis of gorilla genomes. Admixture plots showing the ancestry components of gorilla genomes. Admixture proportions were generated through an unsupervised admixture analysis at k\u2009=\u20092 and k\u2009=\u20093 using ADMIXTURE v1.3 and plotted in R v3.2.3. Each individual is represented by a vertical line partitioned into colored segments whose lengths are proportional to the contributions of the ancestral components to the genome of the individual. At k\u2009=\u20092, with lowest cross-validation error, eastern and western gorillas were homogeneously assigned to two distinct clusters. Nyango, the cross-river gorilla, expectedly had high western gorilla ancestral component with small fraction of eastern gorilla ancestry. At k\u2009=\u20093, while the eastern gorillas maintained their genomic integrity, most western gorillas appeared to be genomic admixture of Cameroonian and Congolese ancestral components of varying proportions\nPrior to applying GPS to elucidate the biogeographical affinity of the query gorilla genomes with unknown geographical origin, we sought to trial its accuracy for all ten reference western lowland gorillas, of known geographic origins [7]. Assignment accuracy was determined for each individual based on whether the predicted geographical coordinates localized within the political boundaries of the known country or regional location of origin. We note that GPS assignments were consistent with the recorded geographic source for nine out of ten individuals assessed. It positioned Coco to Equatorial Guinea, all Cameroonian gorillas to Cameroon and three out of four Congolese gorilla to Congo (Fig. 3). One Congolese gorilla, Vila, did not correspond to its documented ancestral origin, instead it was positioned with other Cameroonian gorillas near Pangar Djerem Researve, Mbam et Djerem National Park, Cameroon, which is home to the northernmost known population of the western lowland gorillas. This mismatch in the assignment of Vila could be likely attributed to its high genomic proximity with a Cameroonian gorilla, Helen (Fig. 3). Overall these results demonstrate a strong geographic-genomic correlation and delineate the expected assignment error for our analyses.\n\nFig. 3\nGPS prediction of the biogeographical affinities of gorilla populations of known geographic source from the Great Ape Genome Project (GAGP). Relevant National Parks/Wildlife Reserves are shown in green ovals (not to scale). The geographic coordinates ascertained by GPS for gorillas from Equatorial Guinea, Cameroon and Congo are shown in yellow, blue and green triangles, respectively. Maps were plotted using the R package rworldmap v1.3\u20131\nNext, we applied the GPS algorithm to infer the biogeographical affinity of 18 query gorillas of unknown provenance (Fig. 4). It is noteworthy that eastern gorilla populations are known to occur in the Democratic Republic of Congo, Uganda, and Rwanda, whereas western gorilla populations reside primarily in Cameroon, Equatorial Guinea, Gabon, Congo, and the Central African Republic [20]. In agreement with their known homelands, GPS positioned all western lowland gorilla genomes within Equatorial Guinea, Cameroon and in the Republic of Congo, while, Victoria, an eastern lowland gorilla, born in captivity was assigned to east-central Democratic Republic of Congo (Additional file 1 Table S3).\n\nFig. 4\nGPS predictions for the gorillas of unknown geographic origin from the Great Ape Genome Project (GAGP). A map depicting the GPS predicted locations for gorillas with unknown geographic origin. The red and orange triangles depict western gorillas and eastern lowland gorillas, respectively of unknown geographic origins. Relevant National Parks/Wildlife Reserves are shown in green ovals (not to scale). Note: in some cases, multiple individuals were assigned to the same geographic location and therefore appeared as a single individual. Maps were plotted using the R package rworldmap v1.3\u20131\nSince gorillas born in captivity have been revealed as discernibly more admixed when compared to wild born populations [1, 4, 7] and GPS is limited in its ability to localize highly admixed individuals [15, 21], we interrogated nine captive gorillas (one eastern lowland gorilla and eight western lowland gorillas) via reAdmix analysis [21], to estimate the composition of captive gorillas as a weighted mix of wild populations (Table 1). Azizi and Bulera were found to be highly admixed with higher Cameroonian (48.5 and 54.4% respectively) compared to Congolese (46.1 and 36.1% respectively) admixture proportions. While the GPS tool appropriately localized Azizi to Cameroon, it failed to do so for Bulera, reflecting that the predictive precision for GPS is curtailed in case of highly admixed individuals. Among others, one or more wild ancestral populations are likely missing in this study, for Kowali and Kokomo. However, both are assigned to Congo, potentially based on their genomic proximity to Congolese gorillas. Finally, Sandra, Kolo, Amani, and Dian had significantly high Cameroonian admixture proportions (86\u2013100%) and were the least admixed, among the captive gorillas included in this study, likely leading them to be positioned with enhanced precision.Table 1\nAdmixture proportions of various captive gorillas employed in this study, as revealed by reAdmix analysis\n\nThe reference populations mentioned in the \u2018Potential origin\u2019 column has been generated through leave-one-out procedure using the genomic information of gorilla individuals with known geographic origins. The \u2018Fractions\u2019 depict the ancestry proportion of the query individuals for each of the reference populations\n\n\nAdditional file 1\n: Table S1. Biogeographic information about the gorilla individuals employed in current study. Table S2. Latitudes and Longitudes of reference gorillas. Table S3. Latitudes and Longitudes of query gorillas. Figure S1. Distribution of mtDNA haplogroups among the gorilla genomes used in the current study (after Soto-Calderon et al. [23]). (DOCX 41 kb)", "s12859-018-2570-y": "De novo assembling of large genomes, such as in conifers (~\u200912\u201330 Gbp), which also consist of ~\u200980% of repetitive DNA, is a very complex and computationally intense endeavor. One of the main problems in assembling such genomes lays in computing limitations of nucleotide sequence assembly programs (DNA assemblers). As a rule, modern assemblers are usually designed to assemble genomes with a length not exceeding the length of the human genome (3.24 Gbp). Most assemblers cannot handle the amount of input sequence data required to provide sufficient coverage needed for a high-quality assembly.An original stepwise method of de novo assembly by parts (sets), which allows to bypass the limitations of modern assemblers associated with a huge amount of data being processed, is presented in this paper. The results of numerical assembling experiments conducted using the model plant Arabidopsis thaliana, Prunus persica (peach) and four most popular assemblers, ABySS, SOAPdenovo, SPAdes, and CLC Assembly Cell, showed the validity and effectiveness of the proposed stepwise assembling method.Using the new stepwise de novo assembling method presented in the paper, the genome of Siberian larch, Larix sibirica Ledeb. (12.34 Gbp) was completely assembled de novo by the CLC Assembly Cell assembler. It is the first genome assembly for larch species in addition to only five other conifer genomes sequenced and assembled for Picea abies, Picea glauca, Pinus taeda, Pinus lambertiana, and Pseudotsuga menziesii var. menziesii.The de novo assembling of large genomes, such as in conifers, that have\u00a0the length of 12 to 30 Gbp and consist of about 80% of highly repetitive elements (repeats), is a rather complex task [1\u201312]. The main problem of assembling such genomes is the limitations of assembler programs. As a rule, modern assemblers are designed to assemble genomes shorter or equal to the length of the human genome (3 Gbp). Most assemblers cannot handle the amount of input sequence data required to provide the coverage needed for a high-quality assembly or take too much time and computer resources. This prompts the development of new approaches in assembling large genomes, including Siberian larch (Larix sibirica Ledeb.), which together with Siberian stone pine (Pinus sibirica Du Tour) are the main objects of the genome project \u201cGenomics of the key boreal forest conifer species and their major phytopathogens in the Russian Federation\u201d funded by\u00a0the research grant No. 14.Y26.31.0004 from the Government of the Russian Federation.High sequence coverage is always needed for high-quality de novo genome sequencing and assembly. For a given average genome coverage, the coverage of individual genome regions is approximately described by the Poisson distribution according to the Lander-Waterman theory [13]. Insufficient coverage increases the probability of zero coverage of some genome regions. Meanwhile, even a single coverage of genome regions is sufficient for their assembling using De Bruijn graph based methods [14] assuming no errors and repeats.To solve the problem, a new stepwise approach to assembling large genomes \u201cin parts\u201d was developed. The idea of partitioning data to perform assembly is not new. For example, in the article [15] it was proposed to apply a similar two-step hierarchical approach with the aim of improving the quality of assembly of bacterial genomes with very high coverage. However, the approach presented in [15] does not solve the problems of assembling large and super-large genomes, especially if DNA was obtained from diploid tissue.In our case the assembly is also done in two steps.\u00a0In the first step, the entire input pool of the sequence reads is divided into several sets (parts). The size of each set is within\u00a0the\u00a0 limit for the number of reads that can be handled by the assembler program. Each set is assembled separately, then the\u00a0contigs obtained for each part are combined and used as the input data for the second step of assembling.With this approach, the genome coverage by the input contigs no longer obeys the Poisson distribution in the second step of assembling. However, the level of coverage will not be greater than the number of parts by which the original pool of reads has been partitioned, which allows to bypass the limitation for the maximum amount of input data in the second step.The challenge of the approach is the lower tolerance to sequencing errors and polymorphisms. The ambiguity in the input sequences in the second step could lead to generating duplications in the output. Therefore, the pipeline for the assembly with this approach should also include verification of the assembly for redundancy to exclude potential duplicates. We used the UCLUST package [16] and self-blasting for this task.To test the applicability of the proposed method of stepwise assembling for de novo assembling of large genomes, such as in L. sibirica (12.03 Gbp), a genome assembly of the model plant species Arabidopsis thaliana obtained by the proposed method was compared with the standard de novo assembly of this species genome. A relatively small subset of A. thaliana genomic reads was selected to get a genome coverage comparable to L. sibirica.As an additional argument supporting the\u00a0applicability of the method, the histograms of genome coverage obtained for A. thaliana and L. sibirica were compared for similarity. To construct the histograms, the genomic reads used for assembling were mapped to the assembled genomes using the bowtie software [17] for A. thaliana and the CLC read mapper for L. sibirica.It should also be noted that in the area of maximum coverage its distribution is more accurately described by the corrected Poisson distribution expressed by the formula \\( \\frac{bL^{bx}e\\left(- bL\\right)}{\\Gamma \\left( bx+1\\right)} \\), where L - average coverage, x - coverage value, b - correction parameter (inversed value of extended variation) (Fig. 3, dotted line, b\u2009=\u20090.3).The observed coverage histogram followed the Lander-Waterman theory in general, and the degree of coverage can be approximately described by the Poisson distribution for the\u00a0most of the genome with the left side maximum peak equalling 16 reads (Fig. 3). The exact fitting of the coverage histogram to the Poisson distribution and the corrected (over-dispersed) Poisson distribution were estimated using\u00a0the iterative maximum likelihood-based procedure implemented in the R statistical package. The results of these tests confirmed the fitting of the histogram to the over-dispersed Poisson distribution around the peak value, with the reservations about semi-qualitative description of the distribution. The left and right tails of the distribution do not obey the provided model and should be described using other approaches. Because of this, the goodness of the fitting depends on the\u00a0selection of limits around\u00a0the peak value of distribution. In reasonable limits between 0.5X and 2X of peak value, the match to over-dispersed Poisson distribution was significant based on the Kolmogorov-Smirnov (KS) test (P\u2009<\u20090.01), but the estimated values of parameters should be anyway considered as approximate to avoid an excess of accuracy.The clearly observed \u201cheavy tail\u201d in the right part of the distribution for contigs with high coverage (more than 100 reads) could be explained by the highly repetitive elements that represented different parts in the original genome, but were aligned and mapped together to the same single contigs. Therefore, the observed coverage histogram can be divided into two parts, with a coverage of\u00a0less or more than 100 reads, respectively. The key observation was that the observed coverage histogram for the L. sibirica genome followed the same trend that further confirms the applicability of the proposed method (respective larch data and figures are presented and discussed below in Results). The \u201cheavy tails\u201d were also observed in the coverage histograms in metagenomics [25] and medical DNA sequencing [26].The number of copies of different types of repeats in the genome is governed by different evolutionary factors, and the simplest way to explain the heavy tail of the distribution is to use the Zipf\u2019s law to describe the frequencies of different types of repeats [27]. According to the Zipf\u2019s law, the frequencies of different types of repeats, sorted by the degree of occurrence, should be distributed in proportion to 1/n, where n is a consecutive number of the type of repeat in the list of observed types.As it can be seen from Fig. 4, the Zipf\u2019s law is approximately satisfied for the coverage of more than 200 reads per site, which agrees with the abovementioned conclusion about the\u00a0assembling repeats that occurred with different frequency in the genome. For a more accurate description of the observed dependence, it is recommended to use a distribution based on the Zipf-Mandelbrot law formulated as \\( \\frac{1}{n^k} \\), where k is generally different from unity [27]. Nevertheless, the applicability of this law to genomic nucleotide sequences requires further study.There are a few studies of the A. thaliana genome that identified different types of repeats, using, in particular, the method of clustering repeat sequences (for example, [23, 24]). According to these studies, while there was a general tendency to meet the Zipf\u2019s law for regions with a high degree coverage, individual peaks also appeared in the coverage distributions, such as in our case (Fig. 4), which can be interpreted as a manifestation of the similarity between individual types of repeats.As shown in Fig. 3, the A. thaliana genome coverage was mostly described by\u00a0the Poisson distribution with an average value of about 16 reads. To test the suggested stepwise assembling method, four sets were generated from the original pool of about 13 million reads. The first three sets included the first, second and third thirds of the original pool of reads, respectively. The fourth set also included one third of the original pool of reads, but was generated by random sampling from the mixed original pool of reads.Table 1 shows that insufficient coverage led to a significant decrease in the average contig length compared to the data in Fig. 2 and Additional file 1: Table S1, but in the second step of assembling this parameter was corrected, and with the increase in the number of parts it\u00a0was stabilized at the level of values close to the values obtained by the different assemblers used to assemble the entire pool of reads simultaneously.The identity of assembly obtained using parts and the stepwise method with assembly based on assembling simultaneously all reads was tested by the NUCmer software (http://mummer.sourceforge.net), and the highest similarity was obtained\u00a0for alignments generated by the CLC Assembly Cell (90.14%) and Abyss (95.24%) software, respectively (Fig.\u00a05 and Additional file 2: Table S2), but the former software computed the assembly with\u00a0a fewer number of contigs and\u00a0a more realistic total length, and it computed it\u00a0seven times faster than the latter one with the same computer hardware resources (31 vs. 217\u2009min, Fig. 5 and Additional file 2: Table S2).The ambiguous positions in the A. thaliana sequencing data were estimated by aligning original A. thaliana reads to the assembly by Bowtie2. They represented 0.7% of genome size. The duplications of contigs were not detected in the final assembly, thus indicating a low level of ambiguity for the assembly\u00a0obtained by the suggested method.Also, a fifth set of reads was added to the analysis. This set included all reads, but the PE and MPE reads were decoupled and used as single reads. This set was generated because we found experimentally that the CLC Assembly Cell assembler was able to process the entire volume of the L. sibirica sequence data, but only if the information about the length of the insertion was not indicated. In this case the \u201cOptimization of the graph using paired reads\u201d step is skipped. In this step long repeats are allowed, and scaffolding is not performed which turns out to be too much computationally intense and practically prohibitive for large volume data. Therefore, this set increased the representation of all reads, but they all could be used only as the single end reads at this step.The length of the L. sibirica genome is about 12.03 Gb [28], about 82% of which consists of repeats [6\u20138]. The volume of the larch sequencing data obtained (11 billion paired 100\u2009bp long reads) was hardly manageable by the available genome assemblers and more than twice the maximum amount of data that the best performing software in our test with the Arabidopsis data CLC Assembly Cell can handle. Therefore, we developed a new stepwise assembly method for assembling this and other large genomes and demonstrated its consistency in computer experiments on assembling the model plant A. thaliana genome.The total length of contigs assembled separately for each of the five sets varied from ~\u20092.5 to ~\u20096 Gb. The N50 parameter varied from ~\u2009300 to ~\u20091300\u2009bp. In\u00a0the second step, individual assemblies were combined by specifying them as unpaired reads and changing the k-mer parameter length from 35 to 60. In addition, the mate pair (MP) reads generated from the MP libraries with 2000\u201310,000\u2009bp long inserts were added to the CLC Assembly Cell input data. These reads were used at the stage of scaffolding (joining contigs into scaffolds with gaps of the\u00a0known expected length).Additional scaffolding was done using BESST [29], and 228,571 additional scaffolds were generated. The scaffolding was also improved by using larch transcriptome reads and RaScaf + Bowtie2 software [30]. About 92% of\u00a0reads were mapped to the genome assembly and allowed us to connect 3622 contigs into scaffolds. The assembly was finished with gap-closing using the Sealer program implemented in the last part of the Abyss pipeline [31], and 61,037 gaps were closed.Thus, the contigs of all five assemblies were processed and the obtained statistics is\u00a0presented in Table 2.The assembly was tested for redundancy using a custom pipeline specially developed for this task, which checks for duplication taking into account possible erroneous nucleotide substitutions and indels. As a result, 74,851 scaffolds were excluded. The assembly was additionally checked for vector contamination and redundancy using the UniVec database (https://www.ncbi.nlm.nih.gov/tools/vecscreen/univec) and the BLAST program, and as a result, 10,681 sequences were deleted.The correlation presented in Fig.\u00a011b was completely linear in the region of sufficient coverage, as expected from the Zipf\u2019s law, in contrast to the correlation for A. thaliana, in which many individual peaks were observed (Fig. 4). This is consistent with the results of the analysis of genomic repeats in Norway spruce [1], where it was difficult to cluster repeats and separate some types of repeats, as it can be done for many other genome sequences of eukaryotes. It also follows from our results that the\u00a0distribution of repeats in conifers\u00a0is continuous. The presence of a large number of repeats and discontinuities in assembling associated with them\u00a0can explain the smaller average contig length in comparison with the results of the A. thaliana genome assembling.The negative binomial distribution or the over-dispersed Poisson distribution is often used to describe genome coverage histograms, but, to our best knowledge, the effect of overdispersion was not systematically studied in the context of genome assemblies (but see [25, 32]). However, the similar values of the over-dispersion parameter for \u00a0the\u00a0three assembled genomes confirmed by the KS tests could serve as an additional argument that the proposed method could be adequately scaled to the assembly of large genomes.The testing of the proposed stepwise approach for assembling genomes in parts on the model plant species A. thaliana showed that, despite some deterioration of the distribution parameters of the contig lengths in the final assembly compared to normal assembling using the CLC Assembly Cell, the result of the stepwise assembling was comparable with the results of assembling all data simultaneously using different assemblers. The comparison of the lengths of the obtained genomes and histograms of the coverage obtained by different methods also allows us to state that the stepwise assembling by parts generates a consistent and reliable genome assembly corresponding to the original biological material.The analysis of the coverage histograms carried out for A. thaliana, Prunus persica (peach) and larch showed a tendency to satisfy the Zipf\u2019s law for the frequency of repeats and provided additional grounds for concluding that the stepwise assembly approach by parts is applicable for assembling large genomes, such as the Siberian larch genome. The interpretation of the coverage histograms using the Zipf\u2019s law made it also\u00a0possible to clarify the idea of statistical regularities characterizing the evolutionary mechanisms of multiplication of repeats in different plant species.Using the new stepwise de novo assembling method presented in the paper, the genome of Siberian larch, Larix sibirica Ledeb. (12.34 Gbp) was for the first time completely assembled de novo by the CLC Assembly Cell assembler. It is the first genome assembly for any larch species in addition to only five other conifer genomes sequenced and assembled for Picea abies [1], Picea glauca [2], Pinus taeda [3\u20135, 9, 11], Pinus lambertiana [10], and Pseudotsuga menziesii var. menziesii [12]. The presented approach makes assembling feasible for very large genomes with a reasonable computing time and without engaging huge computing resources. The assemblies produced\u00a0using this approach are still of reasonable quality allowing their annotation and further use.Base pairGiga baseHigh performance computingThis study was funded by a research grant No. 14.Y26.31.0004 from the Government of the Russian Federation. No funding agency played any role in the design or conclusion of this study. Publication costs are funded by the BioMed Central Membership of the University of G\u00f6ttingen.This manuscript describes published software and a new developed pipeline (source code is available from the authors on request). The sequence reads and obtained scaffolds are publicly available under the NCBI Genbank BioProject accession number PRJNA393226.This article has been published as part of BMC Bioinformatics Volume 20 Supplement 1, 2019: Selected articles from BGRS\\SB-2018: bioinformatics. The full contents of the supplement are available online at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-1.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.It should be noted that not all assembly programs allow generating contigs with a coverage below the threshold value. To overcome this obstacle in the second step of the stepwise assembly, either the program codes should be changed or the\u00a0software that does not have these limitations, such as the CLC Assembly Cell (QIAGEN, Hilden, Germany), should be used. This software takes into account possible sequencing errors during assembling. Thus, if there are sequencing errors in the input reads, most of them will not be incorporated in the contigs generated in the first step for each part of the pool. However, the problem of the stepwise assembling could be insufficient coverage for each part, which can lead to shorter contigs. Since there is a restriction on the minimum length of contigs in the assembling programs, such short contigs with insufficient lengths will be excluded from the assembly. Therefore, to reduce the probability of gaps due to excluding short contigs in the second step, one of the sets in the first step included all reads from the original data pool, but to make computing possible, they were used as single end reads, and they were also multiplied. All steps are presented as a workflow chart in Fig.\u00a01.\n\nFig. 1\nStepwise assembly workflow chart\nThe A. thaliana genome contains 5 chromosomes and 135 Mbp [18]. We used the SPAdes [19], AbySS [20], CLC Assembly Cell (https://www.qiagenbioinformatics.com/products/clc-assembly-cell), and SOAPdenovo [21] assemblers for the traditional de novo assembly of the A. thaliana genome. The genomic paired-end reads of A. thaliana were downloaded from the Genbank SRA database (accession number SRR492411 [22]). The results of assembly at the level of contigs by different assemblers are presented in Fig.\u00a02 and Additional file 1: Table S1.\n\nFig. 2\nThe results of the traditional de novo Arabidopsis thaliana genome assembly generated using four different assemblers. Minimum contig length used for assembling was 200\u2009bp\nThe result of assembling repetitive regions of the genome depends on the number and similarity of copies of a particular type of repeat. With a small and divergent number of copies, the assembler program, as a rule, is able to separate individual copies, so that all variants of this repeat will be presented in the final contigs. With a large number of identical or nearly identical copies of the same type, it would be difficult for an assembler to separate them. The number of repeats in the genome of A. thaliana represents quite a significant part, according to different estimates, from 23 to 32% [23, 24]. As a result, in the final assemblies, identical repeats of the same type can be represented by a single contig. This was reflected in the histogram of the contig coverage based on the distribution of mapped reads used for assembling and presented in Fig.\u00a03.\n\nFig. 3\nHistogram of the Arabidopsis thaliana genome coverage by the mapped reads used for the genome assembly generated by the CLC Assembly Cell software (solid line). Expected and corrected Poisson distributions are represented by dashed and dotted lines, respectively. The number of reads (degree of the genome coverage) is on the horizontal axis; the logarithmic proportion of the genome with such degree of coverage is on the vertical axis\nThe number of repeats with a given degree of coverage can be expressed as the derivative of this dependence, that is, in proportion to 1/n2, where n is the degree of coverage. If the value of \\( Z=\\frac{1}{\\sqrt{Y}} \\) is calculated for a coverage histogram same as in Fig. 3, where Y is the percentage of the genome with a given degree of coverage, then according to the Zipf\u2019s law, the value of Z should directly and proportionality depend on the degree of coverage. This dependence is demonstrated in Fig.\u00a04 for the histogram of the observed coverage presented in Fig. 3.\n\nFig. 4\nDependence of the transformed value of the fraction of the genome coverage Z on the level of coverage. Solid line represents linear dependency calculated by the least square fit\nThus, four sets of reads were generated from the original pool of reads used in the tests presented in Fig. 2 and Additional file 1: Table S1. Figure\u00a05 and Additional file 2: Table S2 presents the results of the stepwise assembly by four assemblers when each of the sets (parts) was assembled separately in the first step and then finally assembled by pooling all contigs from all four sets. It can be seen from the table that the CLC Assembly Cell demonstrated the best performance.\n\nFig. 5\nResults of the Arabidopsis thaliana genome stepwise assembly by different assemblers using raw reads partitioned into four sets. Minimum contig length used for assembling was 200\u2009bp\nTable\u00a01 presents the results of assembly of each of the sets (parts) separately (the first step), as well as based on the pooling of contigs obtained respectively from two, three, and four sets (parts) using the CLC Assembly Cell software.Table 1\nResults of the Arabidopsis thaliana genome stepwise assembling in four sets (parts) using the CLC Assembly Cell software\n\naRepresents approximately 1/3 of all original reads; bRepresents also approximately 1/3 of all original reads, but randomly selected\nFigure\u00a06 compares the genome coverage histograms for the A. thaliana genome assembly based on assembling the entire pool of reads simultaneously, such as in Fig. 3, and the\u00a0assembly based on the stepwise assembling in two steps of four parts (Table 1). It is clearly seen in Fig.\u00a06 that the stepwise assembled genome was adequately covered by the original set of reads.\n\nFig. 6\nComparison of the Arabidopsis thaliana genome coverage histograms obtained for the genome assembly assembled by the CLC Assembly Cell using all reads simultaneously (solid line) and the stepwise method with two steps and four parts (dotted line)\nFor the assembly of the L. sibirica genome, four PE and three MP libraries with different insert size were used (Fig.\u00a07 and Additional file 3: Table S3).\u00a0In the first step, MPE libraries were decoupled and used as single reads to complete a pool of reads. The pool of reads was split to four parts and four sets of contigs were obtained, respectively. The CLC Assembly Cell software was selected for assembling the larch genome as the best performing software.\n\nFig. 7\nSequence coverage for seven sequencing libraries used for the Larix sibirica genome assembly\nUnlike the inbred highly homozygous plant used for the genome sequencing and assembly, such as A. thaliana, the L. sibirica tree used for genome sequencing in our study represented a common forest tree with a relatively high level of individual heterozygosity and, respectively, high within individual biallelic variation. The number of ambiguous positions in the L. sibirica sequencing data was estimated at\u00a0the level\u00a0of 3.0% of the\u00a0genome size. The presence of duplicate contigs was detected in the preliminary draft assembly of L. sibirica obtained in the second step, thus revealing the higher data ambiguity in the L. sibirica sequencing data compared to the A. thaliana data. To resolve the ambiguities in the second stage, the total number of all contigs resulting from the fifth set was increased by 16 folds by multiplying each contig 16 times, respectively. This trick allowed the CLC assembler to apply the majority rule when picking one of the alternative alleles, using the alleles selected in the fifths set in the first step of assembly. The same approach was used also for the Arabidopsis thaliana genome stepwise assembly by four different assemblers (Fig.\u00a08 and Additional file 4: Table S4). The CLC Assembly Cell again demonstrated the best performance.\n\nFig. 8\nResults of the Arabidopsis thaliana genome stepwise assembly by four different assemblers using raw reads partitioned into five sets following the\u00a0approach used for assembling of the Larix sibirica genome. Minimum contig length used for assembling was 200\u2009bp\nIn addition, to verify the accuracy of the stepwise CLC Assembly Cell assembly the medium size genome (265\u2009Mb, 2n =16) of Prunus persica (peach) was also assembled by both the traditional method using 24,324,216 sequence reads (~15X coverage) available on https://www.ncbi.nlm.nih.gov/bioproject/PRJNA31227 and the same stepwise approach that was used for the larch genome assembly and based on the five parts (Fig.\u00a09 and Additional file 5: Table S5). The traditional and stepwise assemblies were similar\u00a0to 95.64% based on the NUCMER comparison.\n\nFig. 9\nThe traditional and stepwise CLC Assembly Cell genome assembly parameters for peach (Prunus persica). Minimum contig length used for assembling was 200\u2009bp\nThe original Siberian larch sequencing data were partitioned into five sets following mainly the procedure described for A. thaliana in Methods with an additional fifth set. Each set was separately assembled\u00a0using the CLC Assembly Cell program. The assembly results are presented in Table\u00a02 for each set. Only contigs with a minimum length of 200\u2009bp were included in the final assembly.Table 2\nThe assembly results of the five sets generated from the original Larix sibirica genome sequencing data\nAdding the last two assemblies based on the 4th and 5th sets (Table 2) improved the final parameters (Table\u00a03) and increased the total contig and scaffold lengths from 7.18 to 7.99 Gb and from 11.04 to 12.34 Gb, respectively. The N50 parameter remained unchanged compared to the best values of partial assemblies. This is inconsistent with the results for A. thaliana assembly tests, but could be explained by the additional scaffolding procedure with the MP reads for the L. sibirica assembly.Table 3\nThe final stepwise Larix sibirica genome assembly based on five sets and the MP reads\n\naMinimum contig length used for assembling was 200\u2009bp\nFinally, after scaffolding, a complete Siberian larch genome of 12.34 Gb was assembled de novo. The computing time taken to assemble the\u00a0larch genome using 40 cores is presented in Fig.\u00a010 and Additional file 6: Table S6. In total, it took about 529\u2009h or 22\u2009days. Therefore, the larch genome computing using the next best assembler SOAPdenovo could predictively take more than 100\u2009days.\n\nFig. 10\nThe computing time (number of hours) taken to assemble each set and the complete Larix sibirica genome using 40 cores\nThe histogram of the coverage for the obtained genome corresponded to the Poisson distribution with extended variation in the regions with low coverage (Fig.\u00a011a) and to the Zipf\u2019s law in the region of high coverage (Fig.\u00a011b) and was similar to the one obtained for A. thaliana (Fig. 3). The values for the inversed over-dispersion parameter were nearly the same for both genomes (0.3\u2009\u00b1\u20090.1), as it was confirmed by likelihood-based parameter estimates.\n\nFig. 11\na The\u00a0observed distribution of the\u00a0Siberian larch genome coverage (solid line) and the\u00a0expected one\u00a0from the corrected Poisson distribution (dotted line) with the\u00a0average coverage value equalling 7 and correction parameter b\u2009=\u20090.3. b Dependence of the transformed degree of genome coverage Z on the Siberian larch genome coverage (solid line). The dashed line represents linear dependency calculated by the least square fit and fully coincides with the solid line\nThe accuracy of the stepwise CLC Assembly Cell assembly was also verified by assembling the medium size genome (265\u2009Mb, 2n=16) of Prunus persica (peach) using both methods. The assembly parameters are presented in Table 3 and the histogram of the coverage -\u00a0in Fig.\u00a012. Both\u00a0the observed and\u00a0the expected distributions of the peach genome coverage were similar to those for Arabidopsis (Figs. 2, 3 and 5) and Siberian larch (Fig.\u00a011) genomes.\n\nFig. 12\na The\u00a0observed distribution of Prunus persica (peach) genome coverage (solid line) and the\u00a0expected one\u00a0from the corrected Poisson distribution (dotted line) with\u00a0the average coverage value equalling 15 and correction parameter b\u2009=\u20090.3. b Dependence of the transformed degree of genome coverage Z on the peach genome coverage (solid line). The dashed line represents linear dependency calculated by the least square fit and fully coincides with the solid line\n\n\nAdditional file 1:\nTable S1. The results of the traditional de novo Arabidopsis thaliana genome assembly generated by four different assemblers. (DOCX 13 kb)\n\n\n\nAdditional file 2:\nTable S2. Results of the Arabidopsis thaliana genome stepwise assembly by different assemblers using raw reads partitioned into four sets. (DOCX 13 kb)\n\n\n\nAdditional file 3:\nTable S3. Sequencing libraries and generated sequence data used for the Larix sibirica genome assembly. (DOCX 14 kb)\n\n\n\nAdditional file 4:\nTable S4. Results of the Arabidopsis thaliana genome stepwise assembly by four different assemblers using raw reads partitioned into five sets following approach used for assembling of the Larix sibirica genome. (DOCX 14 kb)\n\n\n\nAdditional file 5:\nTable S5. The traditional and stepwise CLC Assembly Cell genome assembly parameters for peach (Prunus persica). (DOCX 13 kb)\n\n\n\nAdditional file 6:\nTable S6. The computing time taken to assemble each set and the complete Larix sibirica genome using 40 cores. (DOCX 13 kb)", "s12859-019-2637-4": "We previously reported on CERENKOV, an approach for identifying regulatory single nucleotide polymorphisms (rSNPs) that is based on 246 annotation features. CERENKOV uses the xgboost classifier and is designed to be used to find causal noncoding SNPs in loci identified by genome-wide association studies (GWAS). We reported that CERENKOV has state-of-the-art performance (by two traditional measures and a novel GWAS-oriented measure, AVGRANK) in a comparison to nine other tools for identifying functional noncoding SNPs, using a comprehensive reference SNP set (OSU17, 15,331 SNPs). Given that SNPs are grouped within loci in the reference SNP set and given the importance of the data-space manifold geometry for machine-learning model selection, we hypothesized that within-locus inter-SNP distances would have class-based distributional biases that could be exploited to improve rSNP recognition accuracy. We thus defined an intralocus SNP \u201cradius\u201d as the average data-space distance from a SNP to the other intralocus neighbors, and explored radius likelihoods for five distance measures.We expanded the set of reference SNPs to 39,083 (the OSU18 set) and extracted CERENKOV SNP feature data. We computed radius empirical likelihoods and likelihood densities for rSNPs and control SNPs, and found significant likelihood differences between rSNPs and control SNPs. We fit parametric models of likelihood distributions for five different distance measures to obtain ten log-likelihood features that we combined with the 248-dimensional CERENKOV feature matrix. On the OSU18 SNP set, we measured the classification accuracy of CERENKOV with and without the new distance-based features, and found that the addition of distance-based features significantly improves rSNP recognition performance as measured by AUPVR, AUROC, and AVGRANK. Along with feature data for the OSU18 set, the software code for extracting the base feature matrix, estimating ten distance-based likelihood ratio features, and scoring candidate causal SNPs, are released as open-source software CERENKOV2.Accounting for the locus-specific geometry of SNPs in data-space significantly improved the accuracy with which noncoding rSNPs can be computationally identified.Human genome-wide association studies (GWAS) have led to the discovery of genetic variant-to-trait associations in thousands of studies collectively involving millions of individuals [1]. Functional interpretation of genetic loci identified through GWAS has primarily focused on coding regions in which single nucleotide polymorphisms (SNPs) can be mapped to consequence predictions based on amino acid changes [2]; however, 90% of human GWAS-identified SNPs are located in noncoding regions [3]. Within a noncoding trait-associated region, it is difficult to pinpoint the regulatory SNP (or rSNP) that is causal for trait variation [4]. Various types of SNP annotations that correlate with functional rSNPs are known [5], for example, phylogenetic sequence conservation [6] and expression quantitative trait locus (expression QTL, or eQTL) association [7]. But the general problem of how to integrate various types of genomic, phylogenetic, epigenomic, transcription factor binding site (TFBS), and chromatin-structural rSNP correlates in order to identify rSNPs is a fundamental challenge in computational biology. Progress on this problem has been spurred by the growth of literature-curated databases of experimentally validated rSNPs such as the Human Gene Mutation Database [8] (HGMD), ORegAnno [9] or ClinVar [10]. While various approaches to the rSNP recognition problem have been proposed that do not involve training based on an example set of experimentally validated rSNPs (we call such methods \u201cunsupervised\u201d approaches) [11\u201321], converging lines of evidence from our work [22] and others\u2019 [23\u201326] suggest (but are not entirely consistent on this point [21]) that approaches that are supervised by example sets of experimentally validated rSNPs significantly improves accuracy with which rSNPs can be discriminated from nonfunctional noncoding SNPs.Many types of genomic data have been used to derive SNP annotation features that have proved useful in supervised models for rSNP recognition [22]. The picture emerging from dozens of studies over the past ten years is that increasing the breadth and diversity of such SNP annotation features improves rSNP detection, and thus there has been a steady increase in the number of features that are used in machine-learning approaches for this problem, from 23 features [23], to 28 features [27], to 158 features [28], to 175 features [24], to 246 features in our previous work [22]. The dimensionality of feature-spaces has rapidly increased in the last few years, with reports of rSNP recognition models that incorporate 919 features [16, 26, 29] derived from epigenomic data from the Encyclopedia of DNA Elements (ENCODE) project [30] and 2132 features [25] derived from the Gene Ontology [31]. However, in our previous work [22] we found that a model with a 246-dimensional feature space clearly outperformed models [25, 26, 29] with significantly higher-dimensional feature spaces. This suggests that feature-feature correlation within, and sparsity of, high-dimensional feature-sets may lead to diminishing returns in terms of improving rSNP detection accuracy.A variety of supervised classification algorithms have been proposed for identifying functional noncoding SNPs, including the support vector machine (SVM) [17, 19, 23, 32], na\u00efve Bayes [27], ensemble decision tree algorithms [24, 25, 28], probabilistic graphical models [18, 33], deep neural networks [20, 26, 29], weighted sum of feature ranks [34], and our work using regularized gradient boosted decision trees [22] and deep residual networks [35]. Recently, there have been several proposals of hybrid methods such as combining recurrent and convolutional neural networks [26] and integrating deep neural networks with regularized gradient boosted decision trees [29]. Beyond binary classification, regression-based approaches have been proposed for detecting rSNPs, including linear regression [36] and a mixture-of-regressions model [37]. Overall, there has been a shift toward models with higher parametric complexity as the sizes of example sets of experimentally validated rSNPs has increased [22].In our previous work [22], we described CERENKOV (Computational Elucidation of the REgulatory NonKOding Variome), a machine-learning approach for rSNP recognition that incorporated four key innovations. First, CERENKOV incorporated a within-group-rank-based measure of classification accuracy, which we called AVGRANK. AVGRANK more realistically models the costs associated with incorrect predictions in post-GWAS SNP analysis than typical measures of accuracy like area under the receiver operating characteristic (AUROC) curve or area under the precision-vs-recall (AUPVR) curve. We found that optimizing a model to maximize AUPVR does not guarantee optimality for AVGRANK, and thus, that both measures should be taken into account in evaluating the performance of a computational model for rSNP recognition. Second, in CERENKOV we used a state-of-the-art regularized gradient boosted decision tree (xgboost) classification algorithm [38], which improved upon the rSNP recognition performance that could be achieved (on an identical feature-set) using the previously-proposed classification algorithms Random Forest and Kernel Support Vector Machine [22]. Third, for CERENKOV we engineered 246 SNP-level features from phylogenetic, genomic, epigenomic, chromatin structural, cistromic, population genetic, replication-timing, and functional genomic datasets. Fourth, we trained, validated, and performance-benchmarked CERENKOV using a reference set of 15,331 SNPs (the OSU17 SNP set) comprising 1659 experimentally validated human rSNPs and 13,672 neighboring \u201ccontrol\u201d SNPs (cSNPs) that are each in strong linkage disequilibrium with at least one rSNP. We selected the OSU17 SNPs to represent noncoding loci that would be expected to be encountered in a post-GWAS analysis, based on population minor allele frequency [22]. We compared the accuracy of CERENKOV to nine other published rSNP recognition models (DeltaSVM [19], RSVP [25], DANN [20], fitCons [18], CADD [17], DeepSEA [29], DANQ [26], Eigen [21], and GWAVA [24]) and found that CERENKOV\u2019s performance significantly improved upon the current state-of-the-art, by AUPVR, AUROC, and AVGRANK.In this work we report on CERENKOV2, a next-generation machine-learning approach for rSNP recognition that improves upon our previous approach, CERENKOV [22] in terms of accuracy and insights into the data-space geometry of the problem. In addition to using a significantly expanded reference set of SNPs [the OSU18 SNP set (see \u201cThe OSU18 reference SNP set\u201d section), which has 39,083 SNPs for model benchmarking], we have incorporated new engineered features into CERENKOV2 that are based on likelihood ratios of average SNP-to-neighboring-SNPs distances for various types of distance measures, as described below. By taking account geometric properties of the distribution of SNPs in data space (as described in detail in the next section), CERENKOV2 achieves significantly better rSNP recognition performance than CERENKOV.It is a well-established principle in machine-learning that understanding the manifold structure of cases in data-space can help guide appropriate selection of a classification model and/or geometric features that enable more accurate classification [39, 40]. Data-space inter-sample distance measures are fundamental to many machine-learning algorithms such as k-Nearest-Neighbors [41] (k-NN), and in the case of k-NN, the choice of distance measure can be a key determinant of the accuracy of the classifier [42]. Given that (1) rSNPs and cSNPs are grouped into genetic loci in which the within-locus SNPs are in linkage disequilibrium with one another (making rSNP recognition a grouped machine-learning problem), and (2) in the reference SNP set, each associated locus has at least one rSNP in it and usually many cSNPs (such that the problem has a \u201csparse positive bag\u201d structure [43, 44]), we hypothesized that within-locus SNP-SNP distances in data space may be informative for discriminating rSNPs from cSNPs. But despite the importance of the choice of data-space metric in many machine-learning applications and in clustering [45], the potential utility of data-space metric-based features for improving accuracy of computational recognition of rSNPs has not to our knowledge been systematically explored. Here we report on the first effort (of which we are aware) to improve rSNP detection performance by systematically incorporating data-space geometric features, specifically, intralocus SNP-SNP distances in feature space.Based on this initial observation, we systematically calculated intralocus radii for each SNP in the OSU18 reference SNP set, using five different distance measures (Canberra [48], Euclidean [49], Manhattan [50], cosine [51], and Pearson) applied to both scaled and unscaled feature data (for a total of ten combinations). We found significant differences between the distributions of the ten intralocus radius values conditioned on the two classes (rSNPs and cSNPs). Based on this, we parametrically modeled the intralocus radius distributions (see \u201cAnalysis of intralocus radius distributions for rSNPs and cSNPs\u201d section) and thereby obtained log-likelihood ratios that we incorporated into the feature set for CERENKOV2 (see \u201cUsing data-space geometric features in CERENKOV2\u201d section). We quantified the relative importance of the distance based features in the context of the CERENKOV2 base feature-set (see \u201cCERENKOV2 feature importance\u201d section). Finally, we compared the classification performance of CERENKOV2\u2014including the new distance-based features\u2014with that of CERENKOV on the OSU18 reference SNP set (see \u201cComparison of CERENKOV2 vs. CERENKOV performance\u201d section) and found that CERENKOV2 had significantly better performance than CERENKOV, by AUROC, AUPVR, and AVGRANK. The complete feature data for the OSU18 training and validation SNP set are available online and the software code for CERENKOV2 is freely distributed to the scientific community online under an open-source license (see \u201cAvailability of data and materials\u201d section).For cosine and Pearson distances, the intralocus radius distributions for rSNPs are slightly more skewed to the left and more platykurtic than the distributions for cSNPs; in terms of Euclidean and Manhattan distances, the intralocus radius distributions for rSNPs are left-skewed and more leptokurtic, while the cSNPs\u2019 are right-skewed and less leptokurtic; for the rest distances, the intralocus radius distributions for cSNPs are slightly more skewed to the right and more leptokurtic than the distributions for rSNPs (see also Additional file\u00a01: Table S2).Next, we extracted features from intralocus radii for use in the CERENKOV classifier, using sets of SNPs that were reserved for training within a cross-validation framework (see \u201cGradient boosted decision trees\u201d section). In order to avoid issues with zero-count bins associated with the limited number of SNP loci within a single cross-validation fold, we used a parametric approach: instead of empirically estimating likelihood ratios, for each of the ten methods for computing intralocus radii we fit parametric distributions to the radius values (conditioned on the class label of the reference training SNP). We then applied the fitted parametric models to compute log-likelihood ratios for both the training and validation sets of SNPs and integrated those ten log-likelihood ratios as feature vectors, yielding a 258-column feature matrix input for classification which we compared to performance (using the same classification algorithm) of the original 248-column feature matrix.On an identical starting set of reference SNPs (OSU18, see \u201cThe OSU18 reference SNP set\u201d section) and identical assignments of SNPs to cross-validation folds, we compared the performance of the CERENKOV classification algorithm incorporating the 248-column feature matrix (without intralocus radii-based features) with the performance of the CERENKOV algorithm incorporating a 258-column feature matrix (including intralocus radii-based features). Using ten independent replications of five-fold cross-validation with grouped sampling based on locus (\u201clocus-based sampling\u201d, see \u201cGradient boosted decision trees\u201d section) and using three metrics (AUPVR, AUROC, and AVGRANK [22]), we measured performance separately for classification using the two feature matrices and using xgboost hyperparameters selected to maximize training-set AUPVR (see \u201cHyperparameter tuning\u201d section). For the classification algorithm we used a high-performance implementation of regularized gradient boosted decision trees (xgboost [38], hereafter, xgboost-GBDT). For the two models, the inputs to xgboost were thus a 39,083 \u00d7248 feature matrix and a 39,083 \u00d7258 feature matrix, respectively. We trained and tested xgboost-GBDT (using ten independent replications of five-fold [52] cross-validation with locus-based sampling [22]) with the optimal xgboost hyperparameters (see \u201cHyperparameter tuning\u201d section).To illustrate the biological utility of CERENKOV2, we used CERENKOV2 to compute rSNP prediction scores for noncoding SNPs in the Genome-Wide Repository of Associations Between SNPs and Phenotypes (GRASP) database. We identified two noncoding SNPs that are trait-associated in GRASP and that have CERENKOV2 rSNP prediction scores greater than 0.7: rs2239633 (associated with acute lymphoblastic leukemia), and rs11071720 (associated with mean platelet volume in circulation, and with gene expression of TPM1 in blood. This illustrates how CERENKOV can be used to filter GWAS summary results to identify noncoding SNPs that have high potential to have a mechanistic (gene regulatory) interpretation.We anticipate that CERENKOV2\u2019s performance may be improved through several possible enhancements. An appealing extension would be to combine deep neural network-based approaches based on the local 1 kbp sequence haplotype (recognizing that the local haplotype provides important correlates of functional SNP alleles [55]), with CERENKOV2\u2019s current set of 258 SNP features. Our previous work [35] has demonstrated that a classifier (Res2s2am) based on a deep residual network architecture has state-of-the-art performance on the related problem of discriminating trait-associated noncoding SNPs from control noncoding SNPs. While the present work significantly improves rSNP recognition accuracy, the validation-set AVGRANK performance values (averaging nearly 11) clearly demonstrate that further gains in accuracy are needed in order to fully realize the potential of integrative, data-driven computational approaches to substantially accelerate the search for causal noncoding GWAS variants. Undoubtedly, precision values are dampened by \u201clatent positives\u201d in the training dataset, i.e., high-scoring cSNPs that are simply undiscovered rSNPs. Using machine learning techniques that are specifically designed to address \u201cpositives-plus-unlabeled\u201d problems [56] (such as the rSNP detection problem studied here) or semi-supervised learning algorithms [57] would seem to offer a principled approach to handling the issue of latent positives among the cSNPs. Given the extent to which common features (e.g., replication timing, local GC content, phylogenetic sequence conservation, chromatin accessibility, and transcription factor binding sites [22]) are used by many supervised tools for rSNP recognition, the results from our analysis of the performance of CERENKOV2 suggest that accounting for the intralocus data-space geometry of SNPs may be broadly useful for advancing bioinformatics for post-GWAS SNP analysis.CERENKOV2 significantly improves upon our previous framework and classifier, CERENKOV, in its ablity to score noncoding SNPs based on their regulatory potential. CERENKOV2\u2014by virtue of its training-set construction criteria (locus-based, MAF \u2265 0.05) and its novel feature set including geometric ones\u2014is specifically designed for the problem of identifying candidate causal noncoding SNPs in GWAS summary regions. We have demonstrated, using side-by-side comparisons on identical assignments of SNPs to cross-validation folds, that CERENKOV2\u2019s performance exceeds that of our previous CERENKOV, by both classical global rank-based measures (AUPVR and AUROC) and by the GWAS-oriented performance measure (AVGRANK) that we previously proposed. In particular, CERENKOV2\u2019s validation-set AUPVR performance, 0.404, is a significant improvement over CERENKOV\u2019s AUPVR of 0.359 on the same reference SNP set (OSU18). The results reported in this work are based on a significantly expanded reference SNP set (OSU18, which has more than double the number of SNPs in the OSU17 reference set), which should increase the generalizability and robustness of the performance results reported herein.The source code, feature data files, and instructions for installing and running CERENKOV2 are freely available online (see \u201cAvailability of data and materials\u201d section). By making the software, the data files, and in particular the OSU18 SNP set (with benchmark results) available, we hope to accelerate development of methods for functional analysis of noncoding SNPs and ultimately increase the yield of molecular insights from GWAS.We obtained minor allele frequencies (MAFs) for all SNPs from the dbSNP-based [58] snp146 SQL table hosted at the UCSC Genome Browser [59] site. For the representative set of rSNPs for training/evaluation, we obtained 2,529 rSNPs in total from HGMD (Rel. 2017.2), ORegAnno (Rel. 2015.12.22) and ClinVar that satisfied all of the following criteria: (i) for all SNPs from HGMD, they were marked as regulatory in HGMD and the disease field did not contain cancer; the other SNPs from ORegAnno or ClinVar were of GRCh37 (hg19) assembly; (ii) MAF\u22650.05; (iii) the SNP was not an indel and not contained within a coding DNA sequence (CDS; based on the complete set of transcripts from the Ensembl 75 gene annotation build); and (iv) the SNP was not exclusively mapped to the Y chromosome (due to the lack of phased haplotype data available for proxy SNP searching). For each of these rSNPs, we used the SNP Annotation and Proxy Search (SNAP) tool [60] to identify SNPs that are in LD (r2\u22650.8 in 1,000 Genomes (1KG) Phase 1 [61], with data from the International HapMap Project [62] used instead of 1KG for chromosome X), and we filtered to include only SNPs within 50 kbp of an rSNP, that were not contained within a CDS, that have MAF\u22650.05, and that are not themselves on the list of rSNPs. Overall, this filtering procedure produced a list of 36,554 cSNPs. The combined set of 39,083 SNPs (which we call the OSU18 reference SNP set) was thus designed as an appropriate reference set for the application of post-GWAS SNP analysis. Overall, the class imbalance of OSU18 is \u223c14.454 (cSNP/rSNP).We used the snp146 UCSC SQL table as the initial source for SNP annotations (GRCh37 assembly coordinate system). We extracted additional SNP annotation information by (i) coordinate-based joins to other genome annotation tracks in the UCSC database; and (ii) by joining with non-UCSC data sources using the SNP coordinate. For triallelic and quadrallelic SNPs, we used the two most frequent alleles, for the purpose of obtaining features that depend on allele-dependent scores. We derived DNase I hypersensitive site (DHS) features from data tracks from published genome-wide assays with high-throughput sequencing-based detection (DNase-seq) from the ENCODE project [63] (the master peaks are summary peaks combining data from DHS experiments in 125 cell types; the uniform DHS peaks are from DHS experiments in individual cells, processed using the ENCODE uniform peaks analysis pipeline [64]). The ENCODE_TFBS feature is presented in Table\u00a01 as a single feature for conciseness, but in fact it is 160 separate binary features, one for each transcription factor (TF) for which genome-wide TFBS data (from chromatin immunoprecipitation with high-throughput sequencing readout, or ChIP-seq) and peak data (from the ENCODE Uniform Peaks analysis) are available [64]. For replication timing features, we processed track-specific BigWig files for Repli-seq [65] and Repli-chip [66] experiments from UCSC to obtain the timing scores at individual SNP positions. For ChromHMM [67], Segway [68] and lamina-associated domains (LAD) [69] annotations, we used the SQL tables from UCSC. We used BED file downloads to obtain annotations for DNA repeat elements predicted by RepeatMasker [70], DNA repeat elements predicted by Tandem Repeats Finder [71], epigenome-based CpG island predictions produced by the Bock et al. software pipeline [72], and VISTA enhancer predictions [73].We used the BioMart tool to download (i) TFBS motif occurrences (based on the 2014 release of the Jaspar database [74]) and ChromHMM chromatin segmentation labels from Ensembl Regulation 75 and (ii) GENCODE transcription start sites (TSS; from Ensembl Genes 75) with which we computed signed TSS distances.We obtained SNP-to-gene associations for 13 tissues (adipose, artery/aorta, artery/tibial, esophagus/mucosa, esophagus/muscularis, heart left ventricle, lung, skeletal muscle, tibial nerve, sun-exposed skin, stomach, thyroid, and whole blood) from the Genotype Tissue Expression (GTEx) Project [75] Analysis Version 4 from the GTEx project data portal. For each SNP, we selected the minimum association p-value across genes and tissues.For each distance metric d(\u00b7,\u00b7), we first computed the intralocus radius \u03bbs|d for each SNP s in our OSU18 dataset, in the data-space of all the 248 features (categorical data were binary-encoded which expanded the dimension of the data space to 587); then we separated those intralocus radii according to SNP classes, making two sets \u039br|d={\u03bbr|d|r is an rSNP} and \u039bc|d={\u03bbc|d|c is a cSNP}. For empirical estimation of likelihoods, we used the R hist function with 11 bins on \u039br|d and \u039bc|d and then gathered the bin counts, \\(\\left \\lbrace N_{r \\vert d}^{(1)}, \\dots, N_{r \\vert d}^{(11)}\\right \\rbrace \\) and \\(\\left \\lbrace N_{c \\vert d}^{(1)}, \\dots, N_{c \\vert d}^{(11)}\\right \\rbrace \\) respectively. The empirical likelihood ratio for bin i can be computed with formula \\({LR}_{d}(i) = \\frac {N_{r \\vert d}^{(i)}}{N_{c \\vert d}^{(i)}}\\). For fitting parametric density distributions for intralocus radii, we used the fitdistrplus package (version 1.0.9) [76] in R and we used the normal distribution for cosine and Pearson distances and the log-normal distribution for the other eight combinations of distance function and data scaling/non-scaling. Akaike information criterion was leveraged (AIC) [77] for distribution selection. Then for each distance metric d(\u00b7,\u00b7), 2 probability density functions, pr|d(\u00b7) and pc|d(\u00b7), can be estimated from \u039br|d and \u039bc|d. And for any given SNP s, its likelihood ratio is defined as the ratio of its probability densities, i.e. \\({LR}_{d}(s) = \\frac {p_{r \\vert d}(\\lambda _{s \\vert d})}{p_{c \\vert d}(\\lambda _{s \\vert d})}\\). This likelihood can be interpreted as the extent to which SNP s inclines to be an rSNP, observing its intralocus radius.For loci where only one SNP (rSNP in all cases) was located, we set its likelihood ratio to 1. For each of the OSU18 SNPs, and using the parametric distributions fitted as described above, we computed log-likelihood-ratio scores for each of the ten combinations of distance metric and scaled/unscaled data listed in \u201cData-space geometric features for rSNP recognition\u201d section. [The rationale for using min-max scaling for the data matrix for Canberra, Euclidean, and Manhattan distances was to reduce the impact of high-variance continuous features]. The ten columns of log-likelihood-ratio data were then appended to OSU18 dataset as ten new features during our machine learning processes.The feature extraction and distance computation were done in Python 3 under Ubuntu 16.04 and would take about two hours with a single core of an Intel Core i7-4790 CPU. Peak RAM usage was approximately 12 GB.For the machine learning framework, we used the R statistical computing environment (version 3.4.4) [78], also under Ubuntu 16.04. The complete machine-learning process required 25 min in total for the three models (GWAVA, CERENKOV and CERENKOV2).In order to compare CERENKOV2 with GWAVA [24], we annotated OSU18 dataset with the GWAVA program and then applied Random Forest algorithm to the gained GWAVA feature matrix. Specifically, we used the R package ranger [79] version 0.6.0 with the published hyperparameters. To make a fair comparison, we adapted the same cross-valiation settings and performance measurements to CERENKOV2\u2019s (see \u201cGradient boosted decision trees\u201d section below). In addition, Random Forest is also applied to illustrate CERENKOV2 feature importances (see \u201cCERENKOV2 feature importance\u201d section).For the gradient boosted decision trees (GBDT) classifier, we used the R API for xgboost [38] version 0.6.4.1. We used gradient boosted trees (booster=gbtree) and binary logistic classification as the objective, with the default loss function (objective=binary:logistic). We used ten-fold cross-validation [52] with locus-based sampling, in which we assigned rSNPs to folds (stratifying on the number of cSNPs per rSNP), and then assigned cSNPs to the same fold to which it\u2019s LD-linked rSNP was assigned. Thus, in the case of locus-based sampling, an rSNP and its linked cSNPs are always assigned to the same cross-validation fold. Especially for those 10 geometric features, distribution parameters were estimated only on training data to prevent data leakage. For every prediction performance metric we report, the fold composition was exactly the same across all of the rSNP recognition models studied. We set base_score = 0.06918531 (the rSNP/cSNP class imbalance). We estimated 95% confidence intervals on the sample mean using 1,000 iterations of bootstrap resampling [52].We tuned the xgboost-GBDT classifier with a hyperparameter septuple grid size of 3,888, with locus-based sampling. The tuning hyperparameter tuple that maximized the validation AUPVR was: \u03b7=0.1, \u03b3=10, nrounds = 30, max_depth = 7, subsample = 1.0 and scale_pos_weight = 1; we used these hyperparameter values for all subsequent analyses using xgboost-GBDT. (In contrast, the hyperparameter tuple that minimized the validation AVGRANK was: \u03b7=0.1, \u03b3=100, nrounds = 30, max_depth = 6, subsample = 0.85, colsample_bytree = 0.85, and scale_pos_weight = 8).We downloaded the full GRASP 2.0.0.0 catalog in tab-delimited value (TSV) format and joined the GRASP data with the CERENKOV2 prediction matrix using the dbSNP refSNP ID as the join key. We then filtered the resulting data matrix to include only SNPs whose GRASP trait-association P-values were less than the accepted human genome-wide significance level (5 \u00d710\u22128) and whose CERENKOV rSNP prediction score was at least 0.7.Akaike information criterionArea under precision-vs-recall curveArea under receiver operating characteristic curveAverage ranks of the prediction scores for all ground-truth rSNPs within their lociComputational Elucidation of the REgulatory NonKOding VariomeCoding sequenceControl SNPDNase I hypersensitive siteEncyclopedia of DNA elementsGradient boosted decision treeGenomic evolutionary rate profilingGenotype tissue-expression projectGenome-wide association studyHuman gene mutation databaseHuman-mouse-ratk-nearest-neighborsLamina-associated domainLog-likelihood ratioMinor allele frequencyNetherlands cancer instituteQuantitative trait locusRegulatory SNPSNP annotation and proxy searchSingle nucleotide polymorphismsSupport vector machineTranscription factorTranscription factor binding siteThis work was supported by the Medical Research Foundation of Oregon (New Investigator Award to SAR), Oregon State University (Health Sciences award to SAR), the PhRMA Foundation (Research Starter Grant in Informatics to SAR) and the National Science Foundation (awards 1557605-DMS and 1553728-DBI to SAR).The source code and instructinos for installing and running CERENKOV2 are available on GitHub at \nhttps://github.com/ramseylab/cerenkov\n under the Apache 2.0 open-source software license, and the feature files that were used in the comparative analysis of CERENKOV are freely available online (and hyperlinked from the CERENKOV2 project README on GitHub).Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Based on our initial observation that SNPs within the same locus tend to be clustered in data space, we investigated whether there are class label-specific biases in the locus-based average SNP-to-neighboring-SNPs distances that could be exploited to improve accuracy for discriminating rSNPs from cSNPs. In mathematical terms, for a SNP s, we denote by L(s) the set of SNPs within the same locus as s (for details on the selection of cSNPs that are within the same locus as an rSNP, see \u201cMethods\u201d section). Then, for a given locus s and a given distance metric d(\u00b7,\u00b7), we define an intralocus average SNP-to-neighboring-SNP distance or \u201cintralocus radius\u201d \u03bbs|d by \n (1)One such metric would be the Pearson distance defined as d(s,s\u2032)=1\u2212r(s,s\u2032), where r(s,s\u2032) is the Pearson correlation coefficient [46] between the feature vectors of SNPs s and s\u2032. With Pearson distance being applied, we found that the distribution of intralocus radii for rSNPs were markedly different from cSNPs\u2019, with rSNPs often having higher intralocus radii than cSNPs, i.e., \u03bbr|Pearson>\u03bbc|Pearson. Given the sparsity of rSNPs in the genome (cSNPs outnumber rSNPs 14.5 to one in the OSU18 SNP set) and the typically large linkage disequilibrium-defined locus sizes in the human genome [47], the locus neighborhood for any given s in general mostly contains cSNPs. Together, these observations suggest that in feature-space, the SNPs of a given locus have an \u201catom\u201d-like structure with respect to Pearson distance\u2013a core rSNP and a \u201ccloud\u201d of cSNPs with higher average distance from the it (Fig.\u00a01).\n\n\nFig. 1\nThe geometric idea behind the intralocus distance features that are used in CERENKOV2. Top panel, SNPs from the same locus form a data-space \u201ccloud.\u201d Triangles and circles, SNPs; black lines, distances between a central SNP and the other SNPs within the locus. Bottom panel, SNPs shown in their chromosomal context\n\nWe computed intralocus radii for each of the OSU18 SNPs (see \u201cComputing the geometric features\u201d section) using ten combinations of distance measures and data matrices: Canberra distance, Euclidean distance (L2 norm), Manhattan distance, cosine distance (defined as 1.0 minus the cosine similarity) and Pearson distance, each on unscaled data and min-max scaled data (the latter set of distance measures will be designated with the suffix \u201c(scaled)\u201d in each case). We first analyzed the intralocus SNP-SNP radius distributions for the two SNP classes (rSNPs and cSNPs) within 248-dimensional feature-space using kernel density estimation for radius values conditioned on the class label (rSNP or cSNP) of the reference SNP. As seen in Fig.\u00a02 (see also Additional file\u00a01: Table S2), there are class label-dependent differences in the skewness and kurtosis, indicting that geometric biases exist between rSNPs and cSNPs in data-space.\n\n\nFig. 2\nDistributions of intralocus radii computed using five different distance measures (Canberra, Euclidean, Manhattan, cosine, and Pearson) applied to scaled and unscaled feature data, conditioned on the type of reference SNP (rSNP or cSNP) for the intralocus radius calculation. Results shown are for all OSU18 SNPs (see \u201cThe OSU18 reference SNP set\u201d section). Significant differences in the rSNP likelihoods vs. cSNP likelihoods are evident for Canberra, Canberra (scaled), Euclidean (scaled), Manhattan (scaled), cosine, and Pearson methods for computing intralocus radii. Modest differences in rSNP vs. cSNP likelihoods were evident for the cases of Euclidean and Manhattan methods for computing intralocus radii\n\nThe intralocus radius distribution analysis suggested that taking account of the intralocus radius likelihood for the SNP conditioned on a possible class label (rSNP or cSNP) would be useful for discriminating rSNPs from cSNPs. To visualize the potential class-label discriminating power of each of the ten methods for computing intralocus radii, we empirically estimated the rSNP/cSNP log-likelihood ratios (LLRs) for the ten different methods for computing intralocus radii using binned counts of SNPs for posterior probability estimation (Fig.\u00a03). Consistent with the differences seen in the density distributions (Fig.\u00a02), we found that log-likelihood ratios were significantly different from zero for the majority of bins for intralocus radii computed, for each of the ten distance measures except for cosine (unscaled) and Pearson (unscaled).\n\n\nFig. 3\nEmpirically estimated log-likelihood ratios (rSNP/cSNP) based on intralocus radii computed using ten methods. Results shown are for all OSU18 SNPs (see \u201cThe OSU18 reference SNP set\u201d section). LLR, log-likelihood ratio (natural logarithm); ln, natural logarithm\n\nWithin the above-described cross-validation framework, we found that the inclusion of the ten geometric features improved validation-set AUPVR from 0.358 to 0.402 (p<10\u221225), AUROC from 0.830 to 0.839 (p<10\u221218), and AVGRANK from 11.172 to 10.994 (lower is better for AVGRANK [22]; p<0.004) (Fig.\u00a04 and Additional file\u00a01: Table S1). From these results, we concluded that the addition of the ten geometric features based on the intralocus radius of SNPs in data-space significantly improved performance for rSNP recognition.\n\n\nFig. 4\nPerformance of GWAVA, CERENKOV and CERENKOV2 on the OSU18 reference SNP set, by three performance measures. Marks, sample arithmetic mean of validation-set performance; bars, estimated 95% confidence intervals (see \u201cGradient boosted decision trees\u201d section); GWAVA, based on the GWAVA\u2019s Random Forest model with 174 features [24]; CERENKOV, our previous model with the base 248-column feature matrix; CERENKOV2, our current model consisting of the base feature matrix plus ten log-likelihood features derived from intralocus radii and fitted using training data only; AUPVR, area under the precision-vs-recall curve (higher is better); AUROC, area under the receiver operating characteristic curve (higher is better); AVGRANK, intralocus average score rank (lower is better [22])\n\nIn order to better understand the contributions of different categories of features\u2014particularly geometric features\u2014to rSNP recognition accuracy, we separately trained a Random Forest algorithm on the 258-column feature matrix for the OSU18 reference SNP set (see \u201cThe OSU18 reference SNP set\u201d section) and then obtained permutation [53] and Gini impurity [54]-based estimates of the importance of each of the 258 features (Fig.\u00a05). Consistent with findings from the Peterson et al. study [25], SNP annotations based on replication timing experimental measurements (\u201crepliseq\u201d) had highest overall feature importance; however, the ten log-likelihood-ratio features that were based on data-space geometry strongly contributed to accuracy for rSNP recognition.\n\n\nFig. 5\nGini and permutation importance values of 258 features in 14 categories (colored marks). Feature category labels as follows: \u201cLLR\u201d, log-likelihood ratio (the new data-space geometric features); \u201crepliseq\u201d, replication timing; \u201cgeneannot\u201d, gene-model annotation-based; \u201cepigenome\u201d, epigenomic segmentation [67, 68]; \u201cfeatdist\u201d, SNP location-related; \u201cchrom\u201d, the chromosome; \u201ceigen\u201d, based on the Eigen [21] score; \u201cphylogenetic\u201d, phylogenetic interspecies local sequence conservation [6, 80, 81]; \u201callelism\u201d, allele and MAF-related; \u201cDHS\u201d, DNase I hypersensitive site; \u201cDNAContent\u201d, local nucleotide frequences; \u201ceQTL\u201d, expression quantitative trait locus [75]; \u201crepeats\u201d, genomic repeat annotation; \u201cTFBS\u201d, transcription factor binding site (see \u201cExtracting the nongeometric features\u201d section and Ref. [22] for details)\n\nThe CERENKOV feature extraction software is based on Python and SQL. We extracted 248 SNP features for each of the OSU18 SNPs, using information and measurements from SNP annotation databases, epigenomic and chromatin datasets and phylogenetic conservation scores (Table\u00a01).\nTable 1\nThe 248 SNP features used in CERENKOV\n\nAbbreviations are as follows: UCSC, UC Santa Cruz Genome Browser portal; 1KG, 1,000 Genomes Project; Ensembl75, Ensembl Release 75 [82]; GENCODE, the GENCODE project release 19 [83]; ENCODE, Encyclopedia of DNA Elements [30]; FSU, Florida State University; UW, University of Washington; NKI, Netherlands Cancer Institute; GTEx, the genotype tissue-expression project; GERP, the Genomic Evolutionary Rate Profiling score; CDS, coding DNA sequence; UTR, untranslated region; MAF, minor allele frequency; HMR, human-mouse-rat; TSS, transcription start site\n\n\n\nAdditional file 1\nSupplementary Tables. This PDF file contains 2 supplementary tables. The first one provides a view of comparison of validation-set performance measures between GWAVA, CERENKOV and CERENKOV2 on the OSU18 reference SNP set. The second one lists the skewnesses and kurtoses of intralocus radii computed using Canberra, Euclidean, Manhattan, cosine, and Pearson distances, applied to scaled and unscaled feature data, and conditioned on the type of reference SNP (rSNP or cSNP). (PDF 90 kb)", "s12859-019-2605-z": "Cell counting from cell cultures is required in multiple biological and biomedical research applications. Especially, accurate brightfield-based cell counting methods are needed for cell growth analysis. With deep learning, cells can be detected with high accuracy, but manually annotated training data is required. We propose a method for cell detection that requires annotated training data for one cell line only, and generalizes to other, unseen cell lines.Training a deep learning model with one cell line only can provide accurate detections for similar unseen cell lines (domains). However, if the new domain is very dissimilar from training domain, high precision but lower recall is achieved. Generalization capabilities of the model can be improved with training data transformations, but only to a certain degree. To further improve the detection accuracy of unseen domains, we propose iterative unsupervised domain adaptation method. Predictions of unseen cell lines with high precision enable automatic generation of training data, which is used to train the model together with parts of the previously used annotated training data. We used U-Net-based model, and three consecutive focal planes from brightfield image z-stacks. We trained the model initially with PC-3 cell line, and used LNCaP, BT-474 and 22Rv1 cell lines as target domains for domain adaptation. Highest improvement in accuracy was achieved for 22Rv1 cells. F1-score after supervised training was only 0.65, but after unsupervised domain adaptation we achieved a score of 0.84. Mean accuracy for target domains was 0.87, with mean improvement of 16 percent.With our method for generalized cell detection, we can train a model that accurately detects different cell lines from brightfield images. A new cell line can be introduced to the model without a single manual annotation, and after iterative domain adaptation the model is ready to detect these cells with high accuracy.Identifying and counting individual cells from cell cultures form the basis of numerous biological and biomedical research applications [1, 2]. Determining numbers of cells reflecting the growth, survival, and death of cell populations form the foundations of e.g. basic cancer research and early drug development. Currently, the most commonly used methods for counting cells in cultures are based on either biochemical measurements, or on fluorescent stainings or markers. These methods are often either far from optimal in accuracy, costly, or time-consuming. For example, biochemical measurements are indirect measurements in terms of cell numbers. With fluorescent-based imaging, accurate cell numbers can be obtained with well-established image analysis solutions [3]. The fluorescent methods are, however, often problematic, as they require either 1) fixation and staining of cells, being costly and also limiting the number of data obtained per assay and culture, 2) live stains that are toxic to cells, limiting the time-frame of experiments [4], or 3) are based on expression of fluorescent markers in cells, severely limiting the number of cell lines available for use. In addition, the use of fluorescence requires specified imaging equipment and facilities, not at hand in all laboratories.To avoid the need for fluorescence-based imaging, methods for brightfield imaging are used. Imaging with brightfield microscopy is straightforward with standard facilities available in almost any laboratory, and requires no labeling, making it an efficient and affordable choice. Also the drawbacks from the use of fluorophores on living cells are avoided. However, these benefits come at the cost of inferior contrast compared to fluorescence microscopy.Most of the current brightfield-based methods rely on feature extraction from single in-focus images, or calculating the area which the cells have covered from the imaged surface. While the former works well for sparse cultures where the cells have individual profiles clearly separated from their background, these methods often do not perform well with dense cultures or cell lines with growth patterns of low contrast. Calculating the area, on the other hand, is once again an indirect estimate for cell count, and also performs more poorly the denser the cultures get. Thus, more accurate brightfield-based methods are desired for cell identification and cell number determination. Especially, improvement in identification of individual cells in dense cell clusters, as well as of cell lines with low contrast growth patterns, are required.Various cell detection methods for brightfield images in focus have been developed in recent years [5\u20138]. Unfocused brightfield images or whole brightfield z-stacks have also been applied to cell detection. In our previous study, a z-stack with 25 focal planes was used as input to count PC-3 prostate cancer cells [9]. The method was based solely on the intensity values in images combined with logistic regression classifier. Selinummi et al. used z-stack for creating contrast-enhanced two-dimensional images that provided segmentation results comparable to fluorescence based segmentation [10]. Z-stacks were found to provide useful information for especially cell boundary detection. With a pinhole aperture, one can acquire unfocused images with bright spots marking the cells, which also provides results matching fluorescence based methods [11]. Ali et al. utilized unfocused images for cell and nucleus boundary detection for robust automatic segmentation procedure [12]. Their method is based on differences between two out-of-focused images from opposite directions in z-stack. A similar method with two unfocused, opposite images was proposed by Dehlinger et al. [13]. Z-stacks can also be used to handle slight variations in focal depth, which can be very useful when using autofocus algorithms. This was shown in the research performed by Sadanandan et al., where three consecutive focal planes were used as an input to deep CNN [14].Convolutional neural networks (CNNs) are the state-of-the-art in machine learning research. After the success of CNNs in ImageNet competition [15], they have been adopted for classification tasks in biomedical imaging [16, 17]. By discarding the fully connected layers of CNN, the network becomes fully convolutional (FCN) which outputs a heatmap instead of single class value [18]. FCNs have previously been successfully used in cell detection tasks [19]. In a recent study, FCNs have been applied to class-agnostic counting using only a single training example from new domain [20]. A more sophisticated version of basic FCN is the U-Net, especially designed for biomedical image segmentation where localization has high importance [21, 22]. Usually, neural networks are trained in a supervised manner, meaning that large amounts of annotated training data is required.Domain adaptation can solve machine learning problems where high amount of labeled training data from source domain is present, but there is only little or no labeled data for target domain [23]. Many domain adaptation methods are based on creating a transformation between source and target domains [24, 25]. Domain adaptation can also be performed by learning feature representations shared by both the source and target domains [26, 27].We propose a method for generalized cell detection from brightfield z-stacks using single annotated cell line (PC-3) for supervised training step. U-Net is chosen as the deep learning model due to its exceptional performance in related tasks, and also due to its fully convolutional and resolution preserving nature. We test the generalization capabilities with LNCaP, BT-474 and 22Rv1 cell lines. Each of these cell lines has their own unique appearance in brightfield images, and these cell lines can be considered as target domains which are somewhat similar to the source domain. The model is trained first with annotated PC-3 samples, which results in high precision but sometimes low recall for other cell lines. We use the predictions from the pre-trained model for generating targets for unseen domains (cell lines) in unsupervised domain adaptation step. In contrast to many transfer learning approaches, we do not use any manually annotated training data for the target domain. However, we preserve some of the original training data to prevent excessive influence of the imperfections in predictions. Thus, training is performed in semi-supervised manner while domain adaptation is unsupervised.In this study, we explore methods for improving generalization in cell detection. We use brightfield z-stacks of PC-3 cell line for supervised training of U-Net-based model for cell detection, and apply an unsupervised domain adaptation step to improve the detection accuracy of cell lines lacking annotated training data. Implementation was programmed with Python language and Keras and TensorFlow modules for deep learning.The data consists of brightfield focus stacks (a.k.a. z-stacks) of monolayer cultures of cancer cell lines. Images were acquired with QImaging Retiga-2000R camera using Olympic IX71 microscope and Objective Imaging Surveyor scanning and imaging software. The z-stack range was 240 \u03bcm, and distance between adjacent focal planes was 10 \u03bcm. Images have a pixel resolution of 1596\u00d71196 pixels, corresponding to an area of 1190.8\u03bcm\u00d7891.4\u03bcm. Autofocus method was used to detect most focused image, above and below of which 12 focal planes were imaged. Thus, each stack consists of 25 focal planes in which the 13th focal plane is in focus.Four images from each cell line were annotated for the purpose of validating results. The annotated cell count in testing data is 1975 for PC-3 cells, 2183 for LNCaP cells, 1022 for BT-474 cells and 2883 for 22Rv1 cells. Annotations for validating PC-3 cells were not used in training, and the images of PC-3 cells for validation are from a separate cultivation than the training data. In the domain adaptation step, none of the annotated images of other cell lines were used to prevent any distortion of results from over-fitting.When selecting the best method for this task, the first criterion was that the resolution of prediction has to be similar to input image resolution, to ensure best possible separation of the cells. Second criterion was that the prediction should be performed to the whole image at once, since pixelwise prediction would require excessive computational resources when fulfilling the resolution requirement. Thus, many deep learning architectures were discarded. Deep learning architectures similar to U-Net fulfill these requirements. Also other, more basic fully convolutional networks without maxpooling layers were taken into consideration, however, the results were inferior to U-Net-like architectures. Two chosen methods from literature were included for comparison: subtraction between opposite z-stack planes [12], and image processing based method from one unfocused image [7]. Some experiments were also performed with pixel intensity based logistic regression classification [9], but the results for other cell lines than PC-3 were not comparable to other methods.Best overall results were acquired with U-Net architecture taking focal planes 13, 14 and 15 as input. Example images of these input focal planes for each cell line are shown in Additional file\u00a01. Most focused focal plane in z-stack has index 13 according to autofocus algorithm used in imaging. However, when looking at the planes in question, one can argue that actually the most focused focal plane is the one with index 14. Then, our conclusion for best suited planes would be the same as in [14].Initially, we trained the deep learning model with PC-3 exclusively. For training target, a binary mask was created. Each cell is presented with a disk shaped structuring element with a radius of 8, but if these circles touch each other, the radius is reduced for better separation of the cells. The maximum radius of 8 is a good trade-off between cell separation capability of the model and class balance in training. With this radius, we get approximately 20 to 1 relation between background and cell pixels, which is still a manageable class balance. We use a total of 12 1596\u00d71196 images for training the network. One quarter of the training set was set aside for validation during training.The most difficult cell line from cell detection point-of-view is 22Rv1. These cells are much smaller than the cells from other cell lines. Based on this knowledge, we augmented the training data by resizing PC-3 images to 75 percent of their original size, which doubled the amount of training data. To match the resized images, maximum radius for circles in training targets was reduced to 6 pixels, using the same proportion.Since training target is a binary mask, binary crossentropy was chosen as a loss function. We used stochastic gradient descent (SGD) optimizer with a Nesterov momentum of 0.8, and batch size was 5 samples. We set the learning rate to 0.1 at the beginning of training, and after every 10 epochs reduced it to half. Since convolution discards a small proportion of information from the border of the image, and the fully convolutional nature of U-Net architecture allows changing input size, the size of input patches was switched after each set of 10 epochs for better utilization of the training data. After loading each training set, image transformations were applied randomly to each patch of training data. These transformations include rotation, translation, small intensity shifts and adding noise. The model was trained for a total of 60 epochs. However, the weights were saved only when validation loss decreased, resulting in an actual amount of epochs less than 60.In this study, domain adaptation is used to fit the model trained with PC-3 cell line (source domain) to other cell lines (target domains). Not a single cell from target domains LNCaP, BT-474 and 22Rv1 was annotated for training purposes.Auto-labeling was performed in the following manner. First, prediction was calculated for four randomly selected images of the target cell line. The images that were annotated for testing purposes were not available for the selection. Then, local peaks were detected from the predicted heat map with a threshold of 0.2 and a minimum distance of 5 pixels between peaks. Threshold was set low to include also weak predictions to training data. Each peak was marked as a cell, and these cell points were transformed to training targets with binary dilation with a disk shaped structuring element with a radius of 6 pixels. The model was trained for another 60 epochs in total. New prediction-based training data was calculated after each set of 10 epochs, simultaneously decreasing the learning rate to half and changing patch size, as was performed in supervised training.In the task of cell counting, true negatives are ambiguous. Since true negatives are not included in F1-score, it is a suitable metric for validating our results. To find matching cells between prediction and ground truth, detected cell coordinates were compared to ground truth coordinates with Euclidean distance, and distance below 20 was considered as detection. If multiple coordinates were found within the threshold distance, only the closest of these was accepted as detection.We count true positives (TP), false positives (FP) and false negatives (FN) for each image in the test set. With these values, we calculate precision (positive predicting value), \\( \\frac {TP}{TP+FP}\\), and recall (sensitivity), \\( \\frac {TP}{TP+FN}\\). Finally, F1-score is calculated via equation \\(2 \\times \\frac {precision \\times recall}{precision + recall}\\).In density-based accuracy calculations, a density map was created using kernel density estimation with normal kernel (\u03c3=50 pix) for smoothing discrete cell locations into local neighborhood. This density map was then divided into five areas covering equal density range. Note that this does not correspond to dividing the areas based on equal area coverage, nor will there be equal number of cells within the density areas.To demonstrate the performance of the proposed cell detection methodology, we present results for different experimental setups. First, we show how deep learning masters the challenge of label-free cell detection from bright field focus stacks in a very accurate manner. Second, we show how precision remains high for cell lines never seen by the classifier. Finally, we present how the high precision can be used for iterative unsupervised domain adaptation. Furthermore, we present detection accuracy in relation to cell growth density. While small example images are presented in result figures, examples of whole image level detections are given in Additional file\u00a01.Then, we tested how the cell detector generalizes from PC-3 to multiple cancer cell lines. The scores acquired with the model trained with PC-3 cell line only are shown in first halves of plots in Fig.\u00a06. We achieve a high accuracy with 0.89 F1-score for LNCaP cell line. An example detection is shown in Fig.\u00a04. When moving to cell lines of dense populations and low contrast, namely BT-474 and 22Rv1, the scores are decreased. Precision still remains high for both of these cell lines, but recall drops drastically when compared to LNCaP and PC-3. Also, recall fluctuates a lot between each set of 10 epochs. We could say that the model does not actually fit to BT-474 and 22Rv1 as it does to PC-3 and LNCaP. Indeed, the best score is acquired after only 20 epochs of training, which implies that the more the model is fitted towards PC-3, the farther it goes from fitting to BT-474 and 22Rv1. At this stage, heavy data augmentation had already been applied and this improved especially the 22Rv1 detection. For this cell line, the F1-score is about 0.1 higher when we double the training data by resizing it to 75 percent of the original size. 22Rv1 cells are the smallest in our set, so smaller cells in training data naturally improve the accuracy.The convolutional neural network based label-free cell detection was applied to data from PC-3 cancer cell line with the accuracy of F1-score 0.95. PC-3 cell line has a clear profile in brightfield focus stacks and high detection accuracy is acquired with as few as 10 to 20 epochs of training of a deep learning model, as shown in Fig.\u00a06. With a U-Net-like model, we obtain a clean and sharp heat map as an output, where each cell is represented with a circle. It should be noted that we aim at cell detection, not segmentation, and the model outputs circular detections since it was trained with a binary mask where circles represent the cells. A high accuracy can be achieved also with a single image in focus, but with z-stacks we can improve especially the precision of detections. Non-cell objects are often not as similar to cells in images out-of-focus as they are in only focused image. For example, the artifacts caused by impurities in camera lens do not change their appearance when going out of focus.Generalization to other cell lines was analyzed by applying the cell detection model trained with PC-3 cell line to data from other cancer cell lines, which were LNCaP, BT-474 and 22Rv1. In brief, the results obtained for LNCaP were of high accuracy, while the accuracy dropped for BT-474 and 22Rv1. The LNCaP cell line somewhat resembles PC-3 since the cells tend to grow separately. Especially the cell lines that grow in dense populations do not receive very high accuracy with the network trained with PC-3 only. One reason for this might be that there often is no background around cells that grow close to each other, while each non-stacked cell of PC-3 has at least some background around them. Also the height and shape of the cells in z may affect their contrast properties. In Fig.\u00a05, on top row, we see detections of BT-474 and 22Rv1 after supervised training. BT-474 is detected with reasonable accuracy, achieving an F1-score of 0.74. For the dense population of 22Rv1 cells, most cells in the center of the example image have not been detected, and the F1-score is only 0.64. However, the score is high enough for successful domain adaptation.With domain adaptation, domain being another cell line, we can greatly increase the accuracy of the cell detection for the unseen cell lines, especially those growing in dense populations. The worse the accuracy is before domain adaptation, the more it is improved with domain adaptation. In Fig.\u00a05, we show how multiple previously undetected cells get clear detections after domain adaptation (bottom row). Especially in 22Rv1 (Fig.\u00a05 on the right), the improvement is drastic. Even though some of the cells in the dense center have non-zero confidence that is not registered to the score before domain adaptation, there are several clear detections for these cells after domain adaptation (compare top and bottom row).For cell lines that already get very accurate predictions with F1-scores around 0.9 after initial training with PC-3, the domain adaptation step does not result in a significant change. Thus, we can apply the domain adaptation step to any cell line with reasonable confidence of not reducing the detection accuracy. In Fig.\u00a04, we can see very little difference in the detections for PC-3 and LNCaP cells, even though in the predicted heat map, the detection signals are visibly more distinct.In order to gain more insight into the effect of cell density on the detection accuracy, we created a pooled test set by using data from all cell lines for determining the detection accuracy. It should be noted that this time, the absolute number of cells and the relative fraction of cells from each cell line varies from density area to another (see Fig.\u00a07, center panel for the cell type distributions among areas). From results for the whole test set (Fig.\u00a07, right panel), we see that with the model trained with PC-3 only, the accuracy is low on dense areas. In addition, when adapting to LNCaP domain, the score remains low although slight improvement is apparent. When training with the densely growing cell lines, BT-474 and 22Rv1, the scores are considerably improved. Even though both of these cell lines grow in dense populations, the BT-474 cells are bigger than 22Rv1 cells. Thus, 22Rv1 cell line is able to grow more dense than the other cell lines, resulting in that they are the only cell line present in densest of areas (Fig.\u00a07, center panel). Yet, the improvement in scores for BT-474 is comparable to 22Rv1-trained model. This implies that the size of cells is not a property that greatly differentiates the cells in the model\u2019s point of view. However, the height of cells affects the contrast of cells in z-stacks. This is a property that also affects the similarity between cell lines. In addition, the cells within dense populations do not have any visible background surrounding them, which is a joint property of BT-474 and 22Rv1 cell lines.According to these results, the overall accuracy never decreases when adapting to a new domain. Thus, the new features learned during domain adaptation cannot be just cell line specific. In addition, since PC-3 data is also used when adapting to a new domain, the PC-3 detection accuracy does not decrease.Many applications of biological and biomedical research require accurate cell detection and counting. Our results show that with deep learning we can accurately detect PC-3 cells from brightfield z-stacks without the need for fluorescence imaging. Furthermore, the model generalizes well for cell lines similar to PC-3. In case of densely growing cells with low contrast, properties that differentiate these cells from PC-3, we achieve lower recall but high precision. High precision enables automated generation of suitable training targets for domain adaptation. With iterative unsupervised domain adaptation, we can increase the accuracy of previously poorly detected cell lines considerably. The higher the dissimilarity is between the source and the target cell lines, the more improvement can be achieved via domain adaptation.Our contribution to research fields depending on cell counting is a framework for unsupervised domain adaptation, including a pre-trained model, for accurate detection of various cell lines unseen by the classifier. Manual annotation for these cell lines is not required due to automated labeling of new training data. Since our method is based on brightfield images, it is available for all laboratories with just basic imaging equipment.Convolutional neural networksFully convolutional networkFalse negativeFalse positiveTrue positiveThe authors gratefully acknowledge Academy of Finland projects #314558 & #313921 (PR) and #317871 (LL) for funding.Python implementation is available at https://github.com/BioimageInformaticsTampere/domain-adaptation-cell-detection. All data and annotations are available via the same link.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Four cancer cell lines were used in this research: prostate cancer cell lines PC-3, 22Rv1 and LNCaP, and breast cancer cell line BT-474. All cell lines were obtained from American Type Culture Collection (ATCC, Rockville, MD, U.S.A.) and cultured under the recommended conditions. Example images from these cells are shown in Fig.\u00a01. These cell lines were chosen due to their differential appearance in z-stacks, varying from separately growing, high contrast PC-3 and LNCaP to dense and low contrast populations of BT-474 and 22Rv1. The networks were trained with PC-3 using the same data that was used in our previous study [9]. This data was acquired from one cell cultivation, where the cells grew for six days and were imaged daily. For training, we used two images from each day. Thus, twelve images of size 1196\u00d71596, including 5878 annotated cells in total were used.\n\n\nFig. 1\nExamples of each studied cell line in z-stack with 25 focal planes. The grid represents focal planes, circles marking the planes in the figure in corresponding order\nIn addition to the actual method, the most suitable focal planes from z-stack were defined. Each method was tested with various degrees of unfocusing and, if possible, various amount of input focal planes. The results of these experiments are shown in Table\u00a01.\nTable 1\nMethod and input comparison. For methods by Ali [12] and Buggenthin [7] the results from focal plane producing best overall score are given\n\nBest result for each cell line is marked with boldface\nWhen studying the intermediate outputs of U-Net model, we noticed that cell detections were already present in multiple intermediate layers before the last. Thus, the question rose whether some of the layers could be discarded without loosing accuracy. One residual layer set was removed from the architecture while keeping the symmetry, and in the last column of Table\u00a01 we can see that the results are as good as with whole U-Net. Thus, to reduce computational burden and memory requirements of the model, this smaller architecture was chosen as the method. This network architecture, with some example layer outputs, is shown in Fig.\u00a02. More detailed description of layers is presented in Additional file\u00a01. After selecting the best method, the training pipeline was further improved for better accuracy.\n\n\nFig. 2\nArchitecture of the network. The network is a reduced version of U-Net, with one set of layers removed to maintain symmetry of the U-Net. Image patches are real examples of inputs and outputs, selected by maximum activation. More intermediate outputs are presented in Additional file\u00a01. Each image patch is normalized for better visibility before adding to stack of patches\nAfter achieving reasonable recall and, more importantly, high precision for all cell lines via training with only PC-3, the domain adaptation step was applied. In Fig.\u00a03, the pipeline of the method is presented. Domain adaptation is marked with a blue dashed square and it is repeated six times in total. Domain adaptation was performed in an unsupervised manner; half of the training data was generated by auto-labeling from the target cell line, while the other half represented randomly selected patches of previously used PC-3 training data. Since the domain adaptation step is based on predictions of unseen cell lines, annotated PC-3 training data is required to prevent the model from fitting to false positives or negatives. Without the annotated data, the false predictions can get amplified during training since auto-labeling is performed iteratively during training.\n\n\nFig. 3\nPipeline for iterative unsupervised domain adaptation for cell detection\nWe performed convolutional neural network based label-free cell detection of PC-3 cancer cells. We acquire F1-score of 0.95 for this cell line. An example prediction is shown in Fig.\u00a04, on the left half. Even though the prediction is often close to perfect, the stacking cells are not always well separated. This slightly reduces the overall accuracy, but with a score of 0.95, our method can still be used to e.g. accurately count the growth curve of PC-3 cell line.\n\n\nFig. 4\nUnprocessed predictions and detected cells of PC-3 and LNCaP cell line. The figures in upper row are results before domain adaptation, and the bottom row shows the very similar results after domain adaptation step with corresponding cell line. The heat maps present unprocessed results of detection. Results are presented with cubehelix colormap [28]\nNext, we applied unsupervised domain adaptation to improve generalization to unlabeled data from unseen cell lines. In Fig.\u00a05, we show how domain adaptation step improves the results. F1-score for 22Rv1 rises from 0.65 to 0.84, and the score for BT-474 rises from 0.74 to 0.87.\n\n\nFig. 5\nBT-474 (left) and 22Rv1 (right) detections before (top) and after (bottom) domain adaptation step\n\n\n\nFig. 6\nF1-score, precision and recall as a function of training epochs for all cell lines. First 60 epochs (x-axis) the model trained with PC-3 only, and next 60 epochs the model was trained also with the corresponding cell line\nIn Fig.\u00a07, we show detection accuracies divided into groups of different cell densities, and cell amount within those areas. Density areas are illustrated as contour overlays in Additional file\u00a01. Accuracies are calculated for models adapted to each unseen domain, and also for the model before domain adaptation. In Fig.\u00a07, left panel, the dashed lines represent accuracy before domain adaptation. The accuracy decreases considerably when moving to denser areas, but after domain adaptation (solid lines), the accuracies of the densest areas are comparable to sparser areas. It should be noted that in the second to last densest group, there are only 4 LNCaP cells, which results in sudden drop in accuracy.\n\n\nFig. 7\nAccuracy for cell lines before (dashed) and after domain adaptation with corresponding cell line (left). No score is given when cell line is not present in density group. Amount and type of cells in density groups (center). Accuracy for whole test set, including all cell lines (same set for all models), calculated with models before domain adaptation and after adapting to each unseen domain (right)\n\n\nAdditional file 1\nSupplementary Figures S1\u201313 and Supplementary Table S1. (PDF 18200 kb)", "s12859-019-2634-7": "Understanding the genetic networks and their role in chronic diseases (e.g., cancer) is one of the important objectives of biological researchers. In this work, we present a text mining system that constructs a gene-gene-interaction network for the entire human genome and then performs network analysis to identify disease-related genes. We recognize the interacting genes based on their co-occurrence frequency within the biomedical literature and by employing linear and non-linear rare-event classification models. We analyze the constructed network of genes by using different network centrality measures to decide on the importance of each gene. Specifically, we apply betweenness, closeness, eigenvector, and degree centrality metrics to rank the central genes of the network and to identify possible cancer-related genes.We evaluated the top 15 ranked genes for different cancer types (i.e., Prostate, Breast, and Lung Cancer). The average precisions for identifying breast, prostate, and lung cancer genes vary between 80-100%. On a prostate case study, the system predicted an average of 80% prostate-related genes.The results show that our system has the potential for improving the prediction accuracy of identifying gene-gene interaction and disease-gene associations. We also conduct a prostate cancer case study by using the threshold property in logistic regression, and we compare our approach with some of the state-of-the-art methods.Cancer is a disease that is partially genetic, and the reason behind many genetic diseases and disorders is mutated genes. Mutations in genes lead to harmful consequences and genetic diseases [4]. Genes generally code for proteins. A single protein holds the responsibility of many functions within the cell. Thus, genetic mutations would lead to the creation of nonfunctional proteins. For instance, for genes coding for proteins involved in cell division, a mutation will interrupt the normal process of cell proliferation and death [5]. Genes that control cell division and growth are usually referred to as Tumor suppressor genes. Any alteration or mutation to these genes will disrupt the normal cell division process resulting in cell division over-activation, and will eventually lead to the development of a tumor (cancer).Since abnormal proteins functions are highly associated with the occurrence of cancer, a large number of cancer studies focus on protein/gene functions. Such studies provide the critical knowledge needed in designing cancer diagnosis and treatment interventions. Over the two past decades, a large body of bioinformatics research was directed towards protein function predictions (PFP). Bioinformatics researchers focused their efforts on developing computational methods that assign and interpret the functions of proteins.The PFP techniques are varied depending on the source of information (i.e., sequence-based, structure-based, text mining, and protein-protein interactions). These methods also influenced disease-gene association studies and disease gene prediction [6]. In general, the huge growth in biological data influenced a similar evolution in the biomedical literature. A huge effort in bioinformatics is directed towards the use of the rapidly growing biomedical literature to infer the disease-related genes by extracting this information directly from the text [7, 8]. The biomedical text mining approaches also referred to as BioNLP approaches, employ different Natural Language processing (NLP) techniques to extract descriptive information on biological entities and disease.In this paper, we propose a simple yet powerful disease-gene association identification method based on analyzing a co-occurrence genetic network. We combine the information extraction method with rare event classification and then perform network analysis. We first construct a gene-gene-interaction network based on the number of times the genes and their Gene Ontology (GO) terms appear in the PubMed articles. We extract several features from the text to represent each pair of genes in a vector of variables. We employ two rare-event classification models to optimize the prediction accuracy and to consider the rareness of possible positive gene connections. We trained our data with linear and non-linear classifiers, and we present the results obtained for each one. Following the prediction of gene-gene interactions, a subnetwork is extracted to represent the disease-related network. We then use a network analysis tool to identify the network parameters, properties, and centrality measures. We use the centrality measure scores to rank the top n genes and evaluate them using a disease-gene association benchmark. In this work, we evaluate our approach for three cancer types (i.e., Prostate, Breast, and Lung).http://ecesrvr.kustar.ac.ae:8080/humangene/index.html.A well-known way to study proteins is through identifying similar proteins that interact with each other. A typical feature of proteins is the fact that they don\u2019t work alone. Proteins interact or bind with each other to carry through a certain function [9]. Predicting the protein/gene interactions at their abstract level for the whole genome (i.e., the human genome, the yeast genome, etc.) results in constructing genetic interaction networks. Several approaches use previously known knowledge about the protein/gene to construct PPIs/GGIs. Among these various approaches, many have used the information within the biomedical articles to accomplish this goal. Although various literature analysis approaches have been presented in the past decade, the rapid growth of the biomedical publications encourages the continuous development of methods that automatically extract the information presented in the biomedical articles.Studying the genes or proteins functions has proven to have a direct link to the detection of disease and the discovery of drugs. A missing or mutated protein in the cell is responsible for the cause of a disease. Therefore, the study of disease-gene association (DGA) has been widely conducted, especially in the field of biomedical literature mining. Similarly to the basic text mining approaches, DGA approaches can take a simple or a complex direction. In general, a relation extraction algorithm needs to be implemented in order to use the biomedical literature to find genes related to a certain disease [10]. Extracting DGA could depend on the mentions of both the disease and the genes, or analysis of already constructed genetic networks. Network analysis method is used in many text mining approaches [11\u201313]. One of the earliest approaches that extract disease-gene association based on text mining techniques and network analysis is proposed by \u00d6zg\u00fcr et al. [14]. This method starts with the assumption that the central genes in their constructed disease genetic network are highly associated with the disease. After the gene-gene-interaction network is constructed, centrality measures are applied to rank the top genes in the network that are more likely to be associated with the target disease (i.e., Prostate Cancer). Another very similar approach by Quan & Ren targets the study towards Breast Cancer [15]. It also applies centrality measures to analyze the constructed network, but the difference is in the technique followed for building the network. Quan & Ren select only important sentences that include interaction verbs between genes or diseases.There are much simpler approaches that depend only on the co-occurrence frequency among biological entities (genes, proteins, and diseases) [16]. GO terms are proven to improve the overall performance of the DGA approaches like in [17]. This application applies proximity relation between genes and diseases mentioned in the biomedical text, while also identifying the GO terms annotating the genes and diseases (calculate the semantic similarity). Another approach by Sun et al. uses GO annotations as one source for predicting disease-gene associations [18]. BioNLP has been engaged in many disease/Network-based prediction algorithms, and that is shown in details in the review study by Zhu et al. [19]. Including several Natural Language Processing techniques in the development of these applications can make a complex system. However, using NLP with text mining has shown to perform more efficiently to extract relevant information [20]. Some researchers focus on the identification of disease-related genes without predicting new candidate genes like in DigSee [21]. This application is a search engine that finds and highlights the associations among Cancer genes.In this paper, we tackle some of the limitations that the above studies have by first identifying the GO terms in the abstract text along with the gene name rather than calculating the GO terms semantic similarities between the genes or diseases mentioned in the text. Also, we extract features at three levels of text (i.e. abstract, sentence, and semantic), rather than limiting the search for interacting genes in the sentences or abstracts only. One of the key contributions of this work is to utilize rare-event classification which has many advantages over other classification methods. With this classification method, we can use small datasets to train and test the classifier [22\u201324]. To the best of our knowledge, this is the first work that utilizes rare-event classification with the use of biomedical text mining approach. Recognizing the sparsity of biomedical data when designing a text mining prediction system is crucial since the possible negative connections between genes outnumber the possible positive connections. We also use the threshold property of the classifier to rank the predicted genes which presents novel observations.In this section, we explain the process of constructing the co-occurrence genetic network for the human genome (\u201cCo-occurrence network\u201d section). Our research focus is on using the GO terms as biological terms to help with the information extraction step. We also present linear and non-linear rare-event classifiers. In \u201cDisease-gene association\u201d section, we then describe the process of extracting disease-gene associations based on network analysis.Constructing the co-occurrence genetic network consists of the following main steps:We used UniProtKB/SwissProt [25] to download the primary/official list of genes in order to build the gene-gene-interaction network. We downloaded a total set of 20,183 human genes. In this work, we also identify the Gene Ontology (GO) terms from the text. Gene Ontology is one of the most popular bio-ontology [26]. It annotates genes based on the three main functionalities of genes, i.e., cellular location, molecular function, and biological process. A gene is annotated by one or many GO terms and thus, GO terms are highly descriptive of the genes functionality. We downloaded the list of GO terms that are associated with each gene retrieved from UniProtKB/SwissProt using QuickGO [27]. Therefore our system mainly looks for the gene names and GO terms in the text of biomedical articles. Each gene in the list of genes should be annotated by at least one GO term and should also be mentioned in at least one PubMed article. As for the extraction text, we have used a set of PubMed abstracts retrieved from the National Center for Biotechnology Information (NCBI) [28]. We use abstracts as they are publicly available data and they usually hold the main outcomes of the biomedical experiments. We used the E-utilities provided at NCBI to search and download the abstract texts that mention at least one human gene. We used two main e-utilities that are \"e-search\" to search the PubMed IDs associated with a target gene, and \"e-fetch\" to retrieve and download the PubMed abstract text using the abstract ID from the previous e-utilities query. We retrieved a total of 7,894,920 abstracts in February 2017 and saved them into a local SQL database.Our proposed system automatically extracts different features from the text based on co-occurrence the biological terms \u201cgene-gene\" or \u201cgene-GO term\". In addition, the system looks for the co-occurrence frequency at three different levels of text (i.e., abstract level, sentence level, and semantic level). The abstract and sentence levels respectively indicate the number of times the two terms appear in the same abstract and the same sentence. The semantic level expresses the number of times the two terms appear to have a semantic relationship in the text. That is, the two terms show a positive relationship when we look closely at the sentence. Accordingly, we look for phrases which indicate that the biological terms are interacting or related to each other (e.g., \u201cbinds with\", \u201cinteracts with\", \u201cand\", \u201cor\", etc.). We study the semantic level to have a better understanding of the relation between two biological entities, specifically in the sense of inferring if they are related/connected to each other. The semantic level expresses the \u201csemantic similarity\u201d which is defined as the measure of resemblance between two biological entities.We used the Java APIs provided by LingPipe [29] to develop name entity recognition. Through LingPipe, we identified biological entities (i.e., genes, and GO terms), developed sentences tagging, and word tokenization. Each abstract is parsed through LingPipe library. The features for each pair of genes is then extracted and analyzed by updating the occurrence status of each biological entity according to the three levels of text (i.e., abstract, sentence, semantic).The table of vectors (X) that is produced by the information extraction step is fed to a rare-event classification model. Due to the fact that the possible negative relations among genes (non-events) outnumber the possible positive relations (events), we chose to employ a rare-event classifier that will address the rarity of positive connections. In this work, we use a linear rare-event classifier (Weighted Logistic Regression (WLR) [22]), and we also employ a non-linear classifier alternative (Weighted Kernel Logistic Regression (WKLR) [30]). Both classifiers optimize the prediction accuracy and reflect the sparsity of the biomedical data by using a reasonable sample size [31]. The linear classifier (WLR) is particularly more effective than WKLR is terms of tuning the hyperparameters for large datasets. Moreover, WKLR could be slower than WLR since it represents the data in a high dimensional space. However, it can better capture the data behavior since it separates the data non-linearly [32].The best \u03b2 and \u03b1 vectors are estimated by maximizing the log-likelihood. The difference between the two models is presented in estimating the log-likelihood where it is expressed in Eqs. 5 and 7. In both equations: yi is 1 if the ith training example pair was related and 0 otherwise, n is the total number of training examples, and \u03bb is the regularization parameter. The log-likelihood is adjusted using the weight wi that represents the proportion of events to non-events. This weight introduces rare-event classification and reflects the imbalanced data problem.We trained our system using STRING training dataset that provides the information of experimentally verified related genes [34]. Although STRING is a source for interacting genes/proteins based on experimental and computational methods, we only retrieved the experimentally verified interactions. Each pair of genes represented by the nine features (recall \u201cInformation extraction\u201d section), is assigned the value \u201c1\" to indicate that the pair of genes is confirmed to be experimentally related according to STRING. We assigned the value \u201c0\" to pairs that do not appear to be related, but both genes have to be appearing in STRING experimentally verified interactions network.We use Bootstrapping to train the classifiers and to adjust the regularization parameter (\u03bb) and the kernel parameter (\u03c3). Bootstrapping is a re-sampling method that allows the generation of a large number of samples over multiple rounds. It is a simple and effective technique for approximating the true error measure and for generating a confidence interval for the accuracy [35]. We evaluate the accuracy at each round and by tuning the parameters (\u03bb and \u03c3). The best accuracy is found by comparing all the accuracies obtained by the different values of the parameters. The best accuracy indicates that we found the best fit parameters \u03b2 and \u03b1 that will be used for the prediction.Using either classifier, we can predict the interacting genes and, hence, construct the human gene-gene-interaction network. In the next section, we describe the process of identifying disease-related genes using network analysis.Building disease-related subnetwork: Using the seed genes as a start for building the network, we retrieved from our previously predicted network all the genes that are related to at least one seed gene. All the pairs in the generated subnetwork include at least one seed gene. The subnetwork is then analyzed to get further candidate genes that could be directly related to the disease of study. The list of related genes for the three cancer networks (Breast, Prostate and Lung Cancer) by using either WLR as a classifier or by using WKLR as a classifier are available via the demo link provided in \u201cBackground\u201d section.\nDegree centrality\nThe degree of a node is the number of nodes that are connected to it. Alternatively stated, it is the number of edges adjacent to the node as well. The degree centrality indicates the popularity of the node, hence, the more neighbors a node has, the more important the node is.\nEigenvector centrality\nThis centrality measures the extent of effect a node has in a network. Similarly to the degree centrality, the eigenvector centrality scores the number of neighbors of a node. However, the difference is that the neighbors, in this case, are only considered if they have the characteristics of being high quality or high scoring nodes. A node will score a high eigenvector value if it is also connected to nodes with high eigenvector values. Based on this, the node centrality is dependent on the quantity and the quality of its connections. A node is said to be well-connected if it has more prestigious nodes connected to it.\nCloseness centrality\nThis centrality is a measure of how close a node is to all other nodes in the network. A node with a high closeness value is of interest, as it implies that the node is closer to the center of the network. It also implies that the node has a high effect on the nodes surrounding it. Closeness centrality is computed by calculating the inverse of the sum of the shortest distances between each node and every other node in the network. It can be simply put that higher closeness means a smaller total distance of a node to the other nodes.\nBetweenness centrality\nBetweenness indicates the extent to which a node affects the flow of data within the network. It measures the number of times a node serves as a channel in the shortest paths between two other nodes. The higher the betweenness value is, the more important the node is in controlling the network connections. Betweenness is computed by calculating the number of shortest paths between other nodes passing over this node.Results Evaluation: All the previous centrality measures give us a summary of the network properties, by reporting a score for each node (gene) in the network. In order to test the prediction quality of our method, we ranked the genes based on their score values with each of the described standard centrality measures. That is, for each centrality measure we evaluated the top 15, 25, 45, etc. genes by using different benchmarks that hold already known disease genes. The tests and results validation are reported in the next section.We implemented this system in Java, and we run it on Intel(R) Core i7 processor, with a CPU of 3.4 GHz and 16GB RAM, under Windows10. We used Ling Pipe APIs for the information extraction algorithm and implemented the classification model in MATLAB. We determine the interactions among human genes based on their frequency in the biomedical texts.MalaCards [38]:MalaCards is a database of human diseases, their related-genes annotations, and the database is affiliated with GeneCards [38]. It holds about 20,000 disease entries integrated from more than 70 data sources. In a study by Rappaport et al. MalaCards is shown to outnumber OMIM and UniProt in the average number of disease-gene associations [39]. In this experiment, we retrieved from MalaCards the gene-disease associations that are marked as \u201celite\" genes. An elite gene in the framework of MalaCards is defined to be that from sources that are manually curated and contains strong and reliable association to the disease.NCI\u2019s GDC [40]:NCI\u2019s GDC is short for the National Cancer Institute\u2019s Genomic Data Commons. It is a data portal that holds a collection of descriptive information on cancer genomics. It is part of the National Institutes of Health (NIH), which is a research agency governed by the U.S. Department of Health and Human Services. We retrieved from the GDC portal cancer-related genes that are marked as being part of the Cancer Gene Census (CGC), which is an ongoing effort to categorize genes involved directly to cancer [41].As can be seen from Table\u00a08, degree centrality achieves the highest precisions in most of the models (WLR and WKLR) and cancer types. Betweenness and eigenvector centrality are second to degree centrality in terms of performance, as they achieve an average precision score of 86.86% and 82.23% respectively, where the highest precision is 100%, and the lowest is evaluated to 80%. The precision achieved by closeness centrality is the lowest across all models (average precision of 60%). Regarding the top 15 breast-cancer-genes predicted by WKLR model, the achieved precisions by betweenness and eigenvector show that all 15 predicted genes are considered associated to breast cancer with reference to MalaCards (Both precisions are 100%). To analyze the centrality precisions based on the classifier models, we noticed that in overall, WLR performs slightly higher than WKLR as the latter model tends to hold more interactions in the cancer-related genes subnetwork (number of interactions are reported in Table\u00a06). Comparing the cancer types, breast cancer results show that our model(s) predicted most of the breast cancer genes according to MalaCards.Table\u00a09 show the precision results for four centrality measures evaluated against NCI\u2019s GDC Data. Eigenvector centrality achieves the highest precisions for all cancer types (average precision is 75.57%), with the highest value being evaluated 86.7% and the lowest to 60% which is considerably higher than most scores by other centrality measures. Betweenness and closeness centrality perform relatively worse with average precisions of 47.8% and 48.9%. With GDC, WKLR achieves higher average precision than WLR with both breast-related and lung-related genes. Out of the three cancer types, WLR predicts correctly 80% of prostate-related genes using both closeness and eigenvector centrality. With both benchmarks: MalaCards and GDC, the proposed system predicted correctly most genes using degree and eigenvector centrality.Seed genes: We downloaded the initial list of genes that are related to prostate cancer using the gene/phenotype map in OMIM. We used this list to build the co-occurrence interaction network for prostate cancer.Downloading PMC articles: We used PMC which is an electronic catalog of full-text PubMed articles. It offers free access to view and to download the articles via an FTP service. We downloaded all the PubMed articles that are associated with prostate cancer.CGDA [14]: CGDA identifies disease-gene associations by analyzing the disease-related network. It builds the network by extracting the information on interacting genes from the biomedical literature. It then employs centrality measures to rank and identify disease-related genes.EDC-EDC [42]: EDC infers disease-gene association by extracting this information from the biomedical text. It proposes novel linguistic computational techniques to extract genes interactions. It employs a hybrid constituency\u2013dependency parser for developing a biological NLP information extraction.MCforGN [43]: MCforGN determines related genes based on their co-occurrence in MEDLINE abstracts. It employs both the standard centrality measures and Monte Carlo simulation to identify genetic networks and disease-gene associations.The second observation is that our system has comparable results with the other approaches, which not only indicates good performance, but it also shows the system can predict disease-related genes from gene interaction networks. Some of the genes that were predicted by the system were not found to be disease-related according to the benchmarks. These genes can still be good candidates for experimental verification because the benchmarks that were used are still under an ongoing effort of research. For example, our system has predicted 80% of prostate cancer genes correctly according to PGDB (recall Table\u00a013). The remaining 20% of genes were not verified by PGDB. However, their relation to prostate cancer can be verified further by another benchmark or by working with a biologist to conduct an experimental test. Working with a biologist is one of the main directions that we would like to follow to evaluate our system.In this work, we presented a system for the identification of disease-gene associations. We used the initial set of seed genes known to be related to the disease to retrieve their neighbor genes from the human co-occurrence network generated by the system. Network analysis was then applied to the constructed subnetworks (disease-related networks) using a network analysis visualization tool. We applied closeness, betweenness, degree and eigenvector centrality measures to rank the genes in the subnetworks and to identify new candidate genes that could be linked directly to the diseases. In this study, we focus on studying cancer-related genes as cancer is one of the top 10 leading causes of death in the world. We evaluate the performance of the system by using disease-gene related benchmarks against the top 15 ranked genes. Degree and eigenvector centrality achieves the highest precisions for identifying breast, prostate, and lung cancer genes. According to one benchmark, betweenness and eigenvector centrality predicted correctly 100% of the breast-cancer-related genes. Our system predicted 80% of prostate-related genes using both closeness and eigenvector centrality. We also evaluated the system in terms of recall performance measures, and we report the percentage of initial seed genes that are retrieved among the top 15-20 ranked genes by each centrality measure.One of the main directions that we would like to follow to evaluate our system, and show the significance of our work is through working with a biologist. Turning to a biologist to conduct an experimental test can help us verify the prediction genes. Some of the genes that were predicted by the system were not found to be disease-related according to the benchmarks we used. These genes, however, can still be good candidates for experimental verification because the benchmarks that were used are still under an ongoing effort of research.There are few directions to consider for improving the results produced by the proposed system. The first is to increase the accuracy for predicting the connected and un-connected genes, as well as, the recall and precision. In this study, we only considered the primary names of genes (official gene symbol). Perhaps the use of gene names like synonyms, or gene numbers (referred to as Ordered Locus Names by UniProt [25]) could enhance the quality of performance as some authors refer to genes using alias names in the biomedical articles.Another direction related to the information extraction component is to follow new structural linguistics principles and Natural Language Processing methods. For example, our system\u2019s linguistic model does not consider the long distance relationship between genes or gene-GOterms as the algorithm looks at each sentence in the abstract at a time. In the future, we intend to investigate more descriptive linguistic theories and different NLP techniques to allow for a better extraction of the genes relation.Another aspect to consider is the extension of the steps followed by this approach to further include the context of the study. The cancer type of study could be added as part of the extracted features, since improving the results of the system in constructing the network will directly be reflected in the identification of disease-gene associations. Towards the same directions, the set of abstracts chosen in this study could have affected the prediction accuracy. Therefore, for future work, we could take into account the full-text articles provided by reliable resources.Biological natural language processingDisease-gene associationGene Ontology termThe national institutes of healthNational center for chronic disease prevention and health promotionNatural language processingOnline mendelian inheritance in manProtein function predictionProtein-protein interactionsThe Gaussian radial basis functionWeighted kernel logistic regressionWeighted logistic regressionThis work was funded by Abu Dhabi Educational Council (ADEC) as part of the ADEC Award for Research Excellence (AARE), grant # 843401.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.According to NCCDPHP (National Center for Chronic Disease Prevention and Health Promotion), cancer is among the top 10 causes of deaths for 2014 in the United States [1]. Cancer affected about 8.8 million deaths in 2015 worldwide, with Lung cancer being the leading cancer cause of death according to the World Health Organization. The National Institutes of Health (NIH) in association with the American Cancer Society (ACS) reported the common cancer types in 2016 [2, 3], which is illustrated in Fig.\u00a01. There are many efforts directed towards the treatment of this chronic disease, but the most important direction for more effective treatments starts with enhancing the understanding of cancer and the roots of its cause.\n\n\nFig. 1\nNumber of new cases and deaths for each common cancer type from NIH [2]\n\nWe provide a demo that outputs the set of genes that are related to an input gene from the gene-gene-interaction network that the system has constructed. The demo also provides the list of related genes for three cancer types mentioned in this work by allowing the user to choose either classification models. The last option is to view the gene-gene-interaction networks constructed by a software for network analysis and visualization. The demo is available at: \n\nhttp://ecesrvr.kustar.ac.ae:8080/humangene/index.html.\nWe represent each pair of genes by the previously extracted information in a vector of features. In the framework of this study, a pair of genes Xi is represented by nine features. \nEach feature measures the likeliness between the two genes in the pair. Also, Each feature will represent either the direct (gene-gene) or the indirect (gene-Go term) co-occurrences of the two genes. Since we keep track of the occurrence frequency of the biological terms at three levels of text, each feature will indicate a level (i.e., abstract, sentence, semantic). The way to compute each feature is by calculating the number of times the two biological terms are co-occurred over their individual appearance in the level of the text. Table\u00a01 shows a description of the nine features for the pair of genes (g1,g2), with regards to the biological terms they are representing and the level of text they are targeting. The information extraction component will result in a table of vectors (pairs) referred to as Table(X), where Xi is a row in the table. Further details on this information extraction technique are introduced in a recent study [23].\nTable 1\nDescription of features for the pair (g1,g2)\n\nEach feature measures the number of times the two biological terms are co-occurred over their individual appearance in the level of text\n\nWe used a regularization parameter (\u03bb) in both classifiers to avoid singularities and overfitting. Next, we provide a general description of the classifiers, and we list all their related equations in Table\u00a02. In both models (WLR and WKLR), the vector of features is represented in a logit transformation function defined by Equation 4 for WLR and Equation 6 for WKLR. pi is the probability of the pair of genes being interacting, \u03b2 in Equation 4 is a vector of parameters that differentiate the events and the non-events (the positive class and the negative class). \u03b1 in Equation 6 is the dual variable (vector) that also indicates the separation of events and non-events. Xi is a row in Table(X), and it is just the vector of features for a pair of genes. ki also represents a pair of genes, but the difference is that WKLR transforms the data to a higher dimensional space, hence ki is the ith row in the kernel matrix k(Xi,Xj)=K (see Eq.(5)). The kernel used in WKLR is the Gaussian Radial Basis Function (RBF) kernel [33] as shown in the equation below. \u03c3 is the kernel parameter that defines the width of the kernel. This parameter along with the regularization parameter (\u03bb) are chosen from a range of values and are tuned using bootstrapping.\nTable 2\nThe logit transformation and regularized log-likelihood for both classifiers (WLR and WKLR)\n\nThe detailed description for each equation is reported in \u201cRare-event classification:\u201d section\n\n (5)For the WLR classifier, we found the best \u03b2 vector at \u03bb=4328, and we predict the relation for the pairs of genes using the following equation: 0.5 is the default threshold for prediction in logistic regression. (6)As for the WKLR classifier, the best \u03b1 vector was found at \u03bb=5.7\u00d710\u22123 and \u03c3=0.5, the relation prediction is evaluated using the following equation: \n (7)We show the Receiver Operating Characteristic (ROC) curve in Fig.\u00a02 to assess the quality of our system. ROC curve is a plot of the true positive rate (TPR) against false positive rate (FPR) at different thresholds. We also computed the Area Under the ROC Curve (AUC) measure in Tables\u00a03 and 4 to show how well our system can separate the connected and unconnected genes using WLR and WKLR respectively. With WKLR, we achieved higher accuracy than WLR for both classes as seen in Table\u00a04. In Figs.\u00a03 and 4, we show how our system balances both recall and precision by identifying the performance measures (true positives, false positives, etc.) according to STRING, and by using WLR and WKLR.\n\n\nFig. 2\nROC curve for Training the data using WLR. TPR is increased at low FPR\n\n\nFig. 3\nPrecision-Recall Curve Using WLR\n\n\nFig. 4\nPrecision-Recall Curve Using WKLR\nTable 3\nAccuracy measures from training a data of pairs of genes using WLR\nTable 4\nAccuracy measures from training a data of pairs of genes using WKLR\n\nIn \u201cCo-occurrence network\u201d section, we constructed the genetic co-occurrence network for the entire human genome. We are going to use this network to extract disease-related subnetworks. In this work, we are focusing the study on different Cancer types (i.e., Breast, Prostate, Lung, etc.). As shown in Fig.\u00a05, we follow a process of steps to construct disease subnetworks, analyze these networks and identify new candidate genes that could be linked directly to the disease. The steps are as follows: \n1\nInitial list of seed genes: The process of constructing the disease subnetwork starts with retrieving the genes related to the disease under consideration according to a high-quality reference source. We used Online Mendelian Inheritance in Man (OMIM) to download the seed genes that we are going to use to build the subnetwork [36]. OMIM is a comprehensive collection of human genes and diseases that is being updated daily and publicly available. Moreover, it is commonly used in most of the methods that identify disease-gene associations. OMIM provides the access to its database through an API. The OMIM API URLs consists of handlers, parameters and a unique API key that is given upon request to the user. We used the \u2019geneMap\u2019 handler to search and retrieve all the data related to a certain disease entry.\n\n\nFig. 5\nThe process of network analysis and disease-gene identification\n\n\n\u00a02\nBuilding disease-related subnetwork: Using the seed genes as a start for building the network, we retrieved from our previously predicted network all the genes that are related to at least one seed gene. All the pairs in the generated subnetwork include at least one seed gene. The subnetwork is then analyzed to get further candidate genes that could be directly related to the disease of study. The list of related genes for the three cancer networks (Breast, Prostate and Lung Cancer) by using either WLR as a classifier or by using WKLR as a classifier are available via the demo link provided in \u201cBackground\u201d section.\n\u00a03\nNetwork Analysis (centrality measures): We used Cytoscape network analyzer to perform the analysis for the constructed subnetwork. Cytoscape is an open-source visualization tool that offers interactive network analysis [37]. It computes the network parameters such as the number of nodes and edges, and it reports several properties of the network such as the network flow. Cytoscape computes different centrality measures to rank all the genes in the network and identify the most relevant to the disease. Centrality measures identify how important each node is and how does it affect the network. In this work, we applied several centrality measures, and each is defined below: \n(a)\n\nDegree centrality\n\nThe degree of a node is the number of nodes that are connected to it. Alternatively stated, it is the number of edges adjacent to the node as well. The degree centrality indicates the popularity of the node, hence, the more neighbors a node has, the more important the node is.\n\u00a0(b)\n\nEigenvector centrality\n\nThis centrality measures the extent of effect a node has in a network. Similarly to the degree centrality, the eigenvector centrality scores the number of neighbors of a node. However, the difference is that the neighbors, in this case, are only considered if they have the characteristics of being high quality or high scoring nodes. A node will score a high eigenvector value if it is also connected to nodes with high eigenvector values. Based on this, the node centrality is dependent on the quantity and the quality of its connections. A node is said to be well-connected if it has more prestigious nodes connected to it.\n\u00a0(c)\n\nCloseness centrality\n\nThis centrality is a measure of how close a node is to all other nodes in the network. A node with a high closeness value is of interest, as it implies that the node is closer to the center of the network. It also implies that the node has a high effect on the nodes surrounding it. Closeness centrality is computed by calculating the inverse of the sum of the shortest distances between each node and every other node in the network. It can be simply put that higher closeness means a smaller total distance of a node to the other nodes.\n\u00a0(d)\n\nBetweenness centrality\n\nBetweenness indicates the extent to which a node affects the flow of data within the network. It measures the number of times a node serves as a channel in the shortest paths between two other nodes. The higher the betweenness value is, the more important the node is in controlling the network connections. Betweenness is computed by calculating the number of shortest paths between other nodes passing over this node.\n\u00a0\n\u00a04\nResults Evaluation: All the previous centrality measures give us a summary of the network properties, by reporting a score for each node (gene) in the network. In order to test the prediction quality of our method, we ranked the genes based on their score values with each of the described standard centrality measures. That is, for each centrality measure we evaluated the top 15, 25, 45, etc. genes by using different benchmarks that hold already known disease genes. The tests and results validation are reported in the next section.\n\u00a0Initial list of seed genes: The process of constructing the disease subnetwork starts with retrieving the genes related to the disease under consideration according to a high-quality reference source. We used Online Mendelian Inheritance in Man (OMIM) to download the seed genes that we are going to use to build the subnetwork [36]. OMIM is a comprehensive collection of human genes and diseases that is being updated daily and publicly available. Moreover, it is commonly used in most of the methods that identify disease-gene associations. OMIM provides the access to its database through an API. The OMIM API URLs consists of handlers, parameters and a unique API key that is given upon request to the user. We used the \u2019geneMap\u2019 handler to search and retrieve all the data related to a certain disease entry.\n\n\nFig. 5\nThe process of network analysis and disease-gene identification\n\nNetwork Analysis (centrality measures): We used Cytoscape network analyzer to perform the analysis for the constructed subnetwork. Cytoscape is an open-source visualization tool that offers interactive network analysis [37]. It computes the network parameters such as the number of nodes and edges, and it reports several properties of the network such as the network flow. Cytoscape computes different centrality measures to rank all the genes in the network and identify the most relevant to the disease. Centrality measures identify how important each node is and how does it affect the network. In this work, we applied several centrality measures, and each is defined below: \n(a)\n\nDegree centrality\n\nThe degree of a node is the number of nodes that are connected to it. Alternatively stated, it is the number of edges adjacent to the node as well. The degree centrality indicates the popularity of the node, hence, the more neighbors a node has, the more important the node is.\n\u00a0(b)\n\nEigenvector centrality\n\nThis centrality measures the extent of effect a node has in a network. Similarly to the degree centrality, the eigenvector centrality scores the number of neighbors of a node. However, the difference is that the neighbors, in this case, are only considered if they have the characteristics of being high quality or high scoring nodes. A node will score a high eigenvector value if it is also connected to nodes with high eigenvector values. Based on this, the node centrality is dependent on the quantity and the quality of its connections. A node is said to be well-connected if it has more prestigious nodes connected to it.\n\u00a0(c)\n\nCloseness centrality\n\nThis centrality is a measure of how close a node is to all other nodes in the network. A node with a high closeness value is of interest, as it implies that the node is closer to the center of the network. It also implies that the node has a high effect on the nodes surrounding it. Closeness centrality is computed by calculating the inverse of the sum of the shortest distances between each node and every other node in the network. It can be simply put that higher closeness means a smaller total distance of a node to the other nodes.\n\u00a0(d)\n\nBetweenness centrality\n\nBetweenness indicates the extent to which a node affects the flow of data within the network. It measures the number of times a node serves as a channel in the shortest paths between two other nodes. The higher the betweenness value is, the more important the node is in controlling the network connections. Betweenness is computed by calculating the number of shortest paths between other nodes passing over this node.\n\u00a0The co-occurrence network generated by our system is analyzed to identify disease-gene associations. More specifically, we study cancer-related genes found in the co-occurrence network. We followed the steps mentioned in \u201cDisease-gene association\u201d section to analyze the co-occurrence genetic network. We first retrieve an initial list of genes associated with the target cancer type, using OMIM database. We then build a cancer-related subnetwork using the already generated co-occurrence network. We establish the subnetwork through a search for genes that interact with at least one seed gene. In this study, we construct subnetworks for three different types of Cancer (i.e., Prostate, Breast, and Lung). We gathered 18 prostate cancer seed genes, 23 for breast cancer, and 16 for lung cancer. Table\u00a05 lists the seed genes compiled for each cancer type. It has not escaped our notice that OMIM does not include \u201cBRCA1 gene\" in the list of breast cancer genes (MIM number: 114480). However, this gene appears to be associated with breast-ovarian cancer syndrome (Mim number: 604370). We did not manually include BRCA1 in the list of breast cancer genes for the sake of source data integrity. Using the seed genes to construct the disease-related network, we counted the predicted interactions for the three cancer types. These interactions are generated for the two classifiers used in this study (WLR and WKLR). We included the Network images for each cancer type via the demo link provided in \u201cBackground\u201d section. We also show Cytoscape\u2019s report on the subnetwork\u2019s parameters such as the network diameter, clustering coefficient, number of interactions and number of nodes in Table\u00a06.\nTable 5\nThe seed genes retrieved from OMIM\nTable 6\nThe Cancer-related gene-interaction networks properties as reported by Cytoscape\n\n\u2217 cc refers to clustering coefficient\n\nWe used Cytoscape to analyze the networks using closeness, betweenness, degree and eigenvector standard centrality measures. Each measure produces a list of genes (nodes in the network) that are ranked by the centrality score. We evaluate the quality of our system in identifying disease-related genes with reference to two benchmarks: \na\nMalaCards [38]:\nMalaCards is a database of human diseases, their related-genes annotations, and the database is affiliated with GeneCards [38]. It holds about 20,000 disease entries integrated from more than 70 data sources. In a study by Rappaport et al. MalaCards is shown to outnumber OMIM and UniProt in the average number of disease-gene associations [39]. In this experiment, we retrieved from MalaCards the gene-disease associations that are marked as \u201celite\" genes. An elite gene in the framework of MalaCards is defined to be that from sources that are manually curated and contains strong and reliable association to the disease.\n\u00a0b\nNCI\u2019s GDC [40]:\nNCI\u2019s GDC is short for the National Cancer Institute\u2019s Genomic Data Commons. It is a data portal that holds a collection of descriptive information on cancer genomics. It is part of the National Institutes of Health (NIH), which is a research agency governed by the U.S. Department of Health and Human Services. We retrieved from the GDC portal cancer-related genes that are marked as being part of the Cancer Gene Census (CGC), which is an ongoing effort to categorize genes involved directly to cancer [41].\n\u00a0For each centrality measure, we evaluated the top 15 ranked genes. In general, the top n ranked genes have the highest centrality scores. Particularly, as n increases the centrality scores decrease and sometimes approach 0, which means that it is less likely to find genes related to cancer as n increases. We show the effect of centrality scores on the percentage of related genes using MalaCards as a benchmark. In Table\u00a07, we report the precisions of all centrality measures for the top n ranked genes related to Lung Cancer where the pairs in the lung-cancer-subnetwork were predicted using WLR. The percentages of the top n genes start off with high values of up to 99% performed by eigenvector. As n increases though, the precisions go down for the four centrality measures, and they converge to each other.\nTable 7\nPercentage of top n genes related to lung cancer based on MalaCards database\n\nIn the following test, we evaluated the performance of the system in identifying the genes associated with each cancer type, using two benchmarks: MalaCards and NCI\u2019s GDC. For this test, we enumerated the top 15 genes ranked with each centrality measure and tested their precision. Tables\u00a08 and 9 show the percentage values for the three diseases against the two chosen benchmarks, and the results are discussed below. We included the datasets of the two benchmarks for each cancer type in the supported files [see Additional file\u00a01].\nTable 8\nThe precision measures of the top 15 genes by each centrality measure and against MalaCards\n\nThe highest precisions are italic\nTable 9\nThe precision measures of the top 15 genes by each centrality measure and against GDC\n\nThe highest precisions are italic\n\nTable\u00a010 shows the precision results for the four centrality measures evaluated against both MalaCards and NCI\u2019s GDC Data. As can be seen from the table, the precisions are improved extremely compared to the results in both Tables\u00a08 and 9. One noticeable improvement is that except for closeness, all other centrality measures scored above 86% with all cancer types and all classification methods. The precision scores are also seen to be almost consistent for each cancer type. Lung cancer average precision results are the most improved among the cancer types when compared to the results by each dataset individually. Although closeness measures achieved the lowest average precision, the lowest precision is at 53.3%. Combining the two datasets assists in giving more of an accurate presentation of our system\u2019s performance.\nTable 10\nThe precision measures of the top 15 genes by each centrality measure and against both GDC and MalaCards\n\nWe also evaluated the system in terms of recall performance measures. We report the percentage of initial seed genes that are retrieved among the predicted pairs from the whole human genome network (recall \u201cCo-occurrence network\u201d section). This is an indication of the original coverage of the system\u2019s predictions or connections in the co-occurrence network. The recall measure is computed by dividing the number of seed genes found in the co-occurrence network over the total number of seed genes which are 16, 18 and 23 genes respectively for lung, prostate and breast cancers. The recall scores are shown in Table\u00a011. Both WLR and WKLR perform almost equally in this test. All the breast and lung cancer seed genes were already predicted and found in the co-occurrence network. About 66.6% (12 out of 18) prostate seed genes were found in the co-occurrence network using WLR classifier. By using WKLR classifier, about 72.2% (13 out of 18) prostate seed genes were found in the co-occurrence network.\nTable 11\nThe recall of seed genes in the whole human genome network created by using either WLR or WKLR\n\nIn this section, we aim at presenting breast-cancer related genes that are uniquely predicted by our proposed system. These genes are validated by MalaCards and NCI\u2019s GDC. To the best of our knowledge, our system is the first to associate these genes with breast-cancer. We take the relatively recent proposed system by Quan & Ren [15] as a sample of the systems that miss to predict these genes. Table\u00a012 shows the 30 top-ranked breast-cancer related lists of genes predicted by our proposed system and Quan & Ren. As the table shows, our uniquely predicted genes are not included in the list predicted by Quan & Ren. 83.3% of the genes predicted by our system and shown in Table\u00a012 are validated by MalaCards and NCI\u2019s GDC. These genes are marked with \u2019YES\u2019 in the table. 70 present of our predicted genes shown in the table are seed genes and marked with \u2019Seed\u2019. As Table\u00a012 shows, there are four common genes predicted by both, our system and Quan & Ren. We consider the remaining genes predicted by our system (i.e., the genes that are not validated by MalaCards and NCI\u2019s GDC) as \u201ccandidate genes\u201d. These genes need to be validated by experts. We will investigate them in a future work. Since the datasets used by our system and Quan & Ren are different, we did not evaluate the genes predicted by Quan & Ren against MalaCards and NCI\u2019s GDC. The goal here is to show that our proposed system provides uniquely discovered genes.\nTable 12\nTo the left, the Top 30 genes predicted by our system and their relevance to breast-cancer\n\nTo the right, a list of the Top 30 genes predicted by Quan & Ren\n\nWe present in this section the Prostate Cancer Case Study in which we will compare our system with recent approaches. In order to conduct the comparison, we used the same datasets used in the other approaches and we re-constructed the co-occurrence network. The steps 1-4 are the pre-steps for the comparison (step 5): \n1\nSeed genes: We downloaded the initial list of genes that are related to prostate cancer using the gene/phenotype map in OMIM. We used this list to build the co-occurrence interaction network for prostate cancer.\n\u00a02\nDownloading PMC articles: We used PMC which is an electronic catalog of full-text PubMed articles. It offers free access to view and to download the articles via an FTP service. We downloaded all the PubMed articles that are associated with prostate cancer.\n\u00a03\nThreshold Ranking: In this experiment, we use the threshold property in our chosen classifiers (WLR and WKLR). As stated previously in Eqs. 6 and 7, 0.5 is the default threshold for prediction in logistic regression. A typical binary weighted logistic regression plot with a threshold of 0.5 is illustrated in Fig.\u00a06. A perfect scenario would have the positive connections plotted to the right of the y-axis, and the negative connections plotted the left. However, this is not always the case as some positive and negative connections might overlap during the prediction process. In this test, we predict the relation among genes using different thresholds (i.e., 0.5, 0.6, 0.7 and 0.8) as seen in Fig.\u00a06. As the threshold increases, the prediction line is moved away from the y-axis, which indicates stronger positive relations. We observed the pair of genes that keep on appearing at the different thresholds to effectively retrieve related genes (positive relations).\n\n\nFig. 6\nThe prediction is made over several thresholds. As the threshold increases, fewer pairs are assigned to the positive class\n\n\n\u00a04\nComparison with recent approaches: We evaluated our approach with CGDA [14], EDC-EDC [42] and MCforGN [43]. To compare to these approaches, we used the same ground truth data they follow (i.e., PGDB [44]). PGDB stands for Prostate Gene DataBase. It is a curated database of prostate related genes in general, and genes involved in prostate diseases. \n\nCGDA [14]: CGDA identifies disease-gene associations by analyzing the disease-related network. It builds the network by extracting the information on interacting genes from the biomedical literature. It then employs centrality measures to rank and identify disease-related genes.\n\nEDC-EDC [42]: EDC infers disease-gene association by extracting this information from the biomedical text. It proposes novel linguistic computational techniques to extract genes interactions. It employs a hybrid constituency\u2013dependency parser for developing a biological NLP information extraction.\n\nMCforGN [43]: MCforGN determines related genes based on their co-occurrence in MEDLINE abstracts. It employs both the standard centrality measures and Monte Carlo simulation to identify genetic networks and disease-gene associations.\n\nWe evaluated the performance of our system using the common centrality measures across all approaches (i.e., Closeness, Betweenness, Degree). We report the precision of the top 10 ranked genes by each centrality measure and by each approach in Table\u00a013. As can be seen from the table, The System performs well, and the results are both balanced and comparable with the other approaches. There are two main observations that can be seen from the table: \n(a)\nThe first observation is that our system scored the best precision by closeness centrality measure, and this is an expected performance improvement from applying threshold ranking. Scoring the highest in the closeness measure is also an indication of the system\u2019s ability to predict disease-related genes and the significance of using threshold ranking. In general, the closeness metric is the best metric to determine the global importance of a node in the network, whereas the degree and betweenness metrics can better determine the local importance of the node in the network. For example, in a network of criminals, each node represents a criminal. Using the degree and betweenness centrality would identify the immediate criminal leaders in the network. However, using the closeness metric would identify the main leader(s) of the whole criminal network (In our case, identify the main genes that are related to the disease).\nTable 13\nA comparison for the precision of the top 10 ranked genes by each centrality measure and by each approach\n\n\n\u00a0(b)\nThe second observation is that our system has comparable results with the other approaches, which not only indicates good performance, but it also shows the system can predict disease-related genes from gene interaction networks. Some of the genes that were predicted by the system were not found to be disease-related according to the benchmarks. These genes can still be good candidates for experimental verification because the benchmarks that were used are still under an ongoing effort of research. For example, our system has predicted 80% of prostate cancer genes correctly according to PGDB (recall Table\u00a013). The remaining 20% of genes were not verified by PGDB. However, their relation to prostate cancer can be verified further by another benchmark or by working with a biologist to conduct an experimental test. Working with a biologist is one of the main directions that we would like to follow to evaluate our system.\n\u00a0\n\u00a0Threshold Ranking: In this experiment, we use the threshold property in our chosen classifiers (WLR and WKLR). As stated previously in Eqs. 6 and 7, 0.5 is the default threshold for prediction in logistic regression. A typical binary weighted logistic regression plot with a threshold of 0.5 is illustrated in Fig.\u00a06. A perfect scenario would have the positive connections plotted to the right of the y-axis, and the negative connections plotted the left. However, this is not always the case as some positive and negative connections might overlap during the prediction process. In this test, we predict the relation among genes using different thresholds (i.e., 0.5, 0.6, 0.7 and 0.8) as seen in Fig.\u00a06. As the threshold increases, the prediction line is moved away from the y-axis, which indicates stronger positive relations. We observed the pair of genes that keep on appearing at the different thresholds to effectively retrieve related genes (positive relations).\n\n\nFig. 6\nThe prediction is made over several thresholds. As the threshold increases, fewer pairs are assigned to the positive class\n\nComparison with recent approaches: We evaluated our approach with CGDA [14], EDC-EDC [42] and MCforGN [43]. To compare to these approaches, we used the same ground truth data they follow (i.e., PGDB [44]). PGDB stands for Prostate Gene DataBase. It is a curated database of prostate related genes in general, and genes involved in prostate diseases. \n\nCGDA [14]: CGDA identifies disease-gene associations by analyzing the disease-related network. It builds the network by extracting the information on interacting genes from the biomedical literature. It then employs centrality measures to rank and identify disease-related genes.\n\nEDC-EDC [42]: EDC infers disease-gene association by extracting this information from the biomedical text. It proposes novel linguistic computational techniques to extract genes interactions. It employs a hybrid constituency\u2013dependency parser for developing a biological NLP information extraction.\n\nMCforGN [43]: MCforGN determines related genes based on their co-occurrence in MEDLINE abstracts. It employs both the standard centrality measures and Monte Carlo simulation to identify genetic networks and disease-gene associations.\nWe evaluated the performance of our system using the common centrality measures across all approaches (i.e., Closeness, Betweenness, Degree). We report the precision of the top 10 ranked genes by each centrality measure and by each approach in Table\u00a013. As can be seen from the table, The System performs well, and the results are both balanced and comparable with the other approaches. There are two main observations that can be seen from the table: \n(a)\nThe first observation is that our system scored the best precision by closeness centrality measure, and this is an expected performance improvement from applying threshold ranking. Scoring the highest in the closeness measure is also an indication of the system\u2019s ability to predict disease-related genes and the significance of using threshold ranking. In general, the closeness metric is the best metric to determine the global importance of a node in the network, whereas the degree and betweenness metrics can better determine the local importance of the node in the network. For example, in a network of criminals, each node represents a criminal. Using the degree and betweenness centrality would identify the immediate criminal leaders in the network. However, using the closeness metric would identify the main leader(s) of the whole criminal network (In our case, identify the main genes that are related to the disease).\nTable 13\nA comparison for the precision of the top 10 ranked genes by each centrality measure and by each approach\n\n\n\u00a0(b)\nThe second observation is that our system has comparable results with the other approaches, which not only indicates good performance, but it also shows the system can predict disease-related genes from gene interaction networks. Some of the genes that were predicted by the system were not found to be disease-related according to the benchmarks. These genes can still be good candidates for experimental verification because the benchmarks that were used are still under an ongoing effort of research. For example, our system has predicted 80% of prostate cancer genes correctly according to PGDB (recall Table\u00a013). The remaining 20% of genes were not verified by PGDB. However, their relation to prostate cancer can be verified further by another benchmark or by working with a biologist to conduct an experimental test. Working with a biologist is one of the main directions that we would like to follow to evaluate our system.\n\u00a0The first observation is that our system scored the best precision by closeness centrality measure, and this is an expected performance improvement from applying threshold ranking. Scoring the highest in the closeness measure is also an indication of the system\u2019s ability to predict disease-related genes and the significance of using threshold ranking. In general, the closeness metric is the best metric to determine the global importance of a node in the network, whereas the degree and betweenness metrics can better determine the local importance of the node in the network. For example, in a network of criminals, each node represents a criminal. Using the degree and betweenness centrality would identify the immediate criminal leaders in the network. However, using the closeness metric would identify the main leader(s) of the whole criminal network (In our case, identify the main genes that are related to the disease).\nTable 13\nA comparison for the precision of the top 10 ranked genes by each centrality measure and by each approach\n\n\n\nAdditional file 1\nDocument containing the list of genes for each cancer type according to MalaCards and NCI\u2019s GDC. (XLSX 35 kb)", "s12859-019-2674-z": "The identification of prognostic genes that can distinguish the prognostic risks of cancer patients remains a significant challenge. Previous works have proven that functional gene sets were more reliable for this task than the gene signature. However, few works have considered the cross-talk among functional gene sets, which may result in neglecting important prognostic gene sets for cancer.Here, we proposed a new method that considers both the interactions among modules and the prognostic correlation of the modules to identify prognostic modules in cancers. First, dense sub-networks in the gene co-expression network of cancer patients were detected. Second, cross-talk between every two modules was identified by a permutation test, thus generating the module network. Third, the prognostic correlation of each module was evaluated by the resampling method. Then, the GeneRank algorithm, which takes the module network and the prognostic correlations of all the modules as input, was applied to prioritize the prognostic modules. Finally, the selected modules were validated by survival analysis in various data sets. Our method was applied in three kinds of cancers, and the results show that our method succeeded in identifying prognostic modules in all the three cancers. In addition, our method outperformed state-of-the-art methods. Furthermore, the selected modules were significantly enriched with known cancer-related genes and drug targets of cancer, which may indicate that the genes involved in the modules may be drug targets for therapy.We proposed a useful method to identify key modules in cancer prognosis and our prognostic genes may be good candidates for drug targets.The identification of prognostic genes that can distinguish the prognostic risks of cancer patients is essential for the study of cancer. These genes could be used to predict the prognosis of cancer patients [1, 2]. Additionally, the prognostic genes may be essential in the biological process of cancer progression and metastasis and thus may be potential drug targets [3, 4]. However, most of the published signatures suffer poor generalization [5]. That is, the prognostic genes selected from one data set are not correlated with the prognostic risks in other data sets [6]. This phenomenon may be due to the high heterogeneity of cancer [7]. Therefore, the selected genes whose expression levels are correlated with the prognostic risks in one data set may be passengers rather than drivers in others.Based on the hypothesis that genes involved in a certain functional gene set (i.e., GO term or Pathway) may be more stable, a few works attempted to identify prognostic gene sets based on GO term [8], Pathway [9] and modules in the PPI (protein-protein interaction) network [10\u201312] or a gene co-expression network [11]. In addition, the prognostic modules (gene sets) outperform the gene signatures [13]. Therefore, it seems that gene modules rather than gene signatures are more promising in cancer prognosis.As we know, cross-talk among pathways is common [14], and understanding the cross-talk between pathways is essential for the study of more complex systems [15, 16]. However, most previous works have ignored the cross-talk among the modules, which may result in neglecting the driver modules in cancer prognosis.In this work, based on fact that the dense clusters in co-expression networks may serve as a functional unit to influence the prognosis of cancer patients, we first constructed a gene co-expression network using the gene expression data of cancer patients. Then, the modules that were dense clusters in the network were detected. Adopting a similar strategy as in a previous work [16], we identified cross-talks between every two modules by testing whether the number of edges between the two modules are significant compared with the background distribution of the edges\u2019 number of two random gene sets. Then, all cross-talks among the modules could constitute a module network. To identify the essential modules in cancer prognosis, we first calculated the prognostic correlation of each module by a resampling method. Then, the algorithm of GeneRank [17], which takes the module network and the prognostic correlations of all the modules as input, was applied to prioritize the prognostic modules. The prognostic modules were validated by survival analysis in various data sets. In addition, we also performed the enrichment analysis of these genes involved in modules with curated cancer genes and drug targets to validate the prognostic modules. The evolutionary information of cancer driver genes is helpful for the construction of cancer prognosis prediction models [18, 19]. Therefore, we also investigated the evolutionary feature of our genes involved in the prognostic modules. Furthermore, our method was applied in three kinds of cancers (ovarian cancer, breast cancer and lung adenocarcinoma) and was compared with the state-of-the-art methods.We applied our method to data sets of ovarian cancer, lung adenocarcinoma and breast cancer. All the data sets contain gene expression data and prognostic information (time of death and death status) of cancer patients. In this work, the data set of lung cancer from TCGA (The Cancer Genome Atlas)\u00a0was measured by RNA-seq, and the gene expression data of all the other data sets was measured by genechip. For all gene expression data, the probes were mapped to Entrez Gene ID, and the expression levels of the probes for each gene were averaged.In ovarian cancer, 1432 samples from two data sets were collected (the detailed information of the two data sets was shown in Additional\u00a0file\u00a01: Table S1). Among these data sets, 300 samples from TCGA were randomly selected for the training data set and the other 267 samples from TCGA were assigned to the test data set. The other data set was used as independent data set.In lung adenocarcinoma, 535 samples from TCGA and 443 samples from GSE68465 were used in this work. Among these samples, 300 samples from TCGA were randomly selected for the training data set, and the other 235 samples from TCGA were assigned to the test data set. All the samples in GSE68465 [20] were set as the independent data set.For breast cancer, a merged data set [21] containing 855 samples was used in this work. In this data set, GSE2034 [22] was set as the independent data set. Of the other 569 samples, 300 samples were randomly chosen for the training data set, and the others were assigned to the test data set.We collected the cancer genes from COSMIC and Sanger [23]. The adaptation diseases and the target information of drugs were obtained from the TTD (Therapeutic Target Database) [24], the DGIdb (Drug-Gene Interaction Database) [25] and DrugBank [26, 27].The Pearson correlation coefficient was applied to calculate the correlations of the expression levels between every two genes. Based on the correlation coefficient, a rank-based method was used to construct the gene co-expression network [28]. As we know, genes in one functional pathway may be strongly mutually co-expressed, but genes in another functional pathway may be weakly co-expressed [28]. Therefore, it may be reasonable to construct the gene co-expression network based on the rank-based method rather than the value-based method. The former method selects the co-expression genes of each gene by the rank of the correlations, and the latter method identifies a gene\u2019s neighbors based on a threshold of the correlations. In this work, adopting a similar strategy to the rank-based method [28], for each gene, we selected the 10 most correlated genes as its neighbors, and all the gene pairs constitute the gene co-expression network.We used Cytoscape 3.5.3 to visualize the co-expression networks and the module networks, and the MCODE [29] plug-in for Cytoscape was applied to detect the dense clusters in the network. In this work, only the modules containing no less than 5 nodes were retained.The number of the edges across the two modules in the gene expression network is calculated.Two random gene sets, which contain the same number of genes as the two real modules, are selected from the gene co-expression network. Then, the number of edges across the random gene sets is calculated.The procedure in step 2 is repeated 1000 times, and the edge numbers across the random gene sets are set as the null hypothesis distribution. Based on the null hypothesis distribution, the p-value of the cross-talk between the two modules is calculated.Based on the permutation test, all significant module pairs with p-values less than 0.05 could constitute a module network.The correlation between the gene expression levels of the modules and the prognostic risks of cancer patients could be calculated by cox regression. In order to obtain a more stable result for cox regression, a resampling method, which aims to generate more training data sets for cox regression, was proposed. For each module, the results for 400 cox regressions in the training data sets by resampling, were used to evaluate its prognostic capability, which would be used as an input in the module prioritization algorithm.Here, n is the number of genes in the module. ei is the expression level of the ith gene of the module in the corresponding patient. Therefore, the statistical value of this module in the corresponding patient could be calculated.Then, 90% of the samples in the training data set were randomly selected. In the chosen data set, the Cox proportional hazards regression was applied to calculate the relationship between each module\u2019s statistical value and the prognostic risks (time of death and death status) of the selected patients.Finally, we repeated the procedure 400 times, and the significant frequency, that is, the number of times that the module\u2019s cox p-value was less than 0.05 in the 400 runs, was set as the prognostic capability of the module. The significant frequency of each module could characterize the prognostic stability of it. Furthermore, the average Cox coefficient of each module in the 400 runs is set as the final Cox coefficient of the module.Here, \\( {r}_j^n \\) is the importance (prognostic capability) of the module j after n iterations; pj is the initial importance, which is calculated by the resampling method; wijis equal to 0 or 1, with 1 indicating the existence of cross-talk between module i and module j, and 0 indicating no interaction between the two modules; degreei is the number of neighbors of module i in the module network; N is the module number in the network; and d (0\u2009\u2264\u2009d\u2009<\u20091) is a constant, where a larger d indicates that the importance of the modules is dependent more on the topological structure of the network, and a smaller d indicates that the importance of the modules depends more on the initial importance of the modules. Here, we adopted the same strategy as a previous work [30] and set d as 0.70.Here, I is the identity matrix, W is the adjacency matrix of our module network; D\u2009=\u2009\u2009diag\u2009(degreei\u00a0), and p is a vector (N\u2009\u00d7\u20091) that contains the initial importance of the N modules in the network. By solving this equation, the vector r (N\u2009\u00d7\u20091), which contains the final importance of all the nodes in the network, could be obtained.Here, si is the statistical value (the average value of all the genes\u2019 expression levels in the module) of the module whose Cox coefficient is positive, and sj is the statistical value of the module whose Cox coefficient is negative. Then the patients in the data set were divided into two groups with the same number of patients according to their prognostic risks. Finally, the log rank test was performed to test whether there is a significant difference in the real survival risks between the two groups.In this equation,\u00a0p\u2009\u2212\u2009valuei is the log rank p-value in the ith data set, and n is the number of cancer data sets.Here, x is the number of genes in the intersection set, M is the number of genes in the universal set, K is the number of genes in the modules and N is the number of cancer genes (drug targets).The module network would reveal cross-talks among the modules. Therefore, the module network could facilitate the identification of key modules in cancer prognosis. In this work, we propose a new method to construct the module network. First, based on the gene expression data of cancer patients, a rank-based method was used to construct a gene co-expression network. Then, the dense clusters, which were communities in the network, were detected as modules. Next, a permutation test was proposed to identify cross-talks among the modules. In this work, we applied it in ovarian cancer, breast cancer and lung adenocarcinoma, respectively. The module networks of the three cancers are shown as follows.Using the gene expression profiles of ovarian cancer patients in TCGA, a gene co-expression network was constructed. In this network, there are 15,406 nodes and 154,060 edges, and the average number of neighbors of the genes in the network was 16.67. The power-law fit of the nodes\u2019 degrees with the number of nodes showed that the network was scale-free, with a correlation of 0.902 and an R-square of 0.925 (Additional file 1: Figure S1).Based on the co-expression network, 258 modules were detected. The genes within each module were densely connected with each other and rarely connected with other genes outside the module. After the identification of cross-talks among these modules, the module network of ovarian cancer was constructed (Additional file 1: Figure S2). As a result, 957 edges were identified among the 258 modules, and the average number of neighbors of the modules was 7.419, which may indicate that cross-talk among the modules were common.For breast cancer, the gene expression data of all the samples in the merged data set [21] (except for the samples in GSE2034 [22]) was used to construct a co-expression network. As a result, 170,920 co-expression pairs among 17,092 genes were obtained, and the average number of neighbors of the genes in the network was 16.92. The power-law fit of the nodes\u2019 degrees with the number of nodes showed that the network was also scale-free, with a correlation of 0.937 and an R-square of 0.945 (Additional file 1: Figure S3). The module network constructed based on the co-expression network contained 150 modules, with 614 edges among the 150 modules (Additional file 1: Figure S4).The gene expression profiles of lung adenocarcinoma patients from TCGA were used to construct a gene co-expression network. In the co-expression network, there were 12,153 nodes and 121,530 pairs. For the nodes in the network, the average number of co-expressed genes was 16.76. Similar to the co-expression networks of ovarian cancer and breast cancer, the co-expression network of lung adenocarcinoma was also scale-free, with a correlation of 0.899 and an R-square of 0.950 in power-law fit (Additional file 1: Figure S5). Based on the co-expression network, the module network of lung adenocarcinoma was also constructed. There were 181 modules and 593 edges in the network (Additional file 1: Figure S6). Furthermore, the average number of neighbors of each module was 6.701.From these results, we can see that the module networks in all three cancers are dense, which may indicate that cross-talks among the modules are common.For the modules in each of the three cancers, the modules\u2019 prognostic capabilities were calculated by the resampling method using the training data set of the corresponding cancer. Then, based on the module network and the modules\u2019 prognostic capabilities, the algorithm GeneRank was applied to prioritize the modules in cancer prognosis for the three cancers, respectively. In each kind of cancer, top 5% of all the modules in the corresponding module network were selected as key modules. As a result, 13 modules in ovarian cancer (Additional file 1: Table S2), 8 modules in breast cancer (Additional file 1: Table S3) and 9 modules in lung adenocarcinoma (Additional file 1: Table S4) were identified, respectively.To validate the prognostic modules, the survival analysis of the cancer data sets was performed for the three kinds of cancer. The results of the survival analyses of the three cancers are shown as follows.As described above, 13 modules in ovarian cancer were selected as key modules in the prognosis of ovarian cancer. Based on the gene expression data of the 13 modules\u2019 statistical values, the prognostic risks of cancer patients could be calculated (Method). Then, a survival analysis could be used to test whether the patients in the low-risk group, calculated by our method, had longer survival times than the high-risk group.In breast cancer, a merged data set [21] containing 855 samples was used in this work. In this data set, all 286 samples from GSE2034 were set as the independent data set [22]. As to the other 569 samples, 300 patients were selected for the training data set, and the others were assigned to the test data set. In the training data set, 8 modules were selected as prognostic modules.Then, the selected modules were used to calculate the prognostic risks of the cancer patients in the test data set and in the independent data set. In the test data set, the low-risk group had a significantly longer survival time, with an HR of 1.57 and a p-value of 0.0077 (Fig. 2a). Furthermore, the survival analysis in the independent data set also proved that our modules could distinguish the prognostic risks of cancer patients, with an HR of 2.35 and a p-value of 3.37e-05 (Fig. 2b), respectively.Therefore, a conclusion can be drawn that the prognostic modules could distinguish the prognostic risks of cancer patients in breast cancer.In lung adenocarcinoma, 300 samples from TCGA were selected for the training data set, and the other 235 patients were assigned to the test data set. In addition, all 443 samples in GSE68465 [20] were set as the independent data set. Using our method, 9 modules in the training data set were identified.From these results, we can see that our method could identify the prognostic modules in all three kinds of cancer. Additionally, these modules could distinguish the prognostic risks of cancer patients in a large number of patients from various data sets. As we know, the main problem of the traditional methods is that they cannot perform well in independent data sets. The good performance of our method has validated the superiority of our method.As described above, the main hypothesis of our method is that the cross-talk among modules may influence the outcomes of cancer patients. Therefore, the module network may facilitate the identification of key modules in prognosis. To validate our method, the same numbers of modules as our prognostic modules, which were ranked by the resampling method, were selected as control modules. In a previous work [33], it has been proved that the random gene set may be also prognosis in multiple cancer types. Therefore, a permutation test was applied to test whether the performance of our method was better than the random gene sets, which contained the same number of genes as our modules. At last, we also compared the performance of our prognostic modules with the published signatures.In addition, a permutation test was applied to validate our method. First of all, we randomly selected the same number of genes with that of our prognostic modules in ovarian cancer. After that, the random gene set was applied to calculate the prognosis risks of the patients in the same data sets, and a Dscore was calculated. At last, the process was repeated 1000 times, and a p-value was obtained by comparing the Dscore of our modules with the 1000 Dscores of the random gene sets. As a result, the p-value of our method is 0.10, which may indicate that our method is better than most of the random gene sets. In the meanwhile, the random gene set may be used to predict the prognosis of cancer patients with a much higher probability than expected [19]. This result may prove that the cross-talk among the modules could facilitate the identification of prognostic genes.Furthermore, a 37-gene signature, which was identified in the literature [32], was also applied to predict the prognostic risks of cancer patients in these data sets. The signature could distinguish the prognostic risks in these data sets, with the log-rank p-values of 0.0076 and 0.037 in test data set and independent data set respectively (Additional file 1: Table S6). However, the Dscore showed that the signature performed worse compared with our prognostic modules and the control modules (Table 1).In a previous work [6], 42 genes, which could predict the prognostic risks of cancer patients in multiple cancer types, were selected as prognostic markers. Here, we also compared the performance of our method with the 42-gene signature. This method performed well in the test data set (Additional file 1: Table S7). However, it couldn\u2019t discriminate the prognostic risks in the independent data set. In addition, the Dsocre (2.05) of the 42-gene signature also showed our method performed better (Table 1).For breast cancer, based on the resampling method, 8 modules were selected as control modules. Using the control modules, the patients in the test data set and the independent data set were predicted as low-risk or high-risk. The control modules could distinguish the prognostic risks in both data sets (Additional file 1: Table S8). However, our prognostic modules outperformed the control modules in both data sets. The Dscores of our prognostic modules and the control modules were 6.58 and 5.05, respectively.We also compared the performance of our modules with that of the random gene sets by permutation test. As a result, the p-value was 0.0030. That is, out of 1000 random gene sets, only three were better than ours, which may indcate our method was significantly better than the random gene sets.The 70-gene signature [34] is the most well-known gene signature in breast cancer. Here, we calculated the prognostic risks of cancer patients in the same data sets. The 70-gene signature performed well in both data sets (Additional file 1: Table S9), but the performance of our method was the better (Table 1). In addition, the 42-gene signature [6] was also used to do survival analysis in the breast cancer data sets. As a result, it couldn\u2019t distinguish the risks of cancer patients in these data sets (Additional file 1: Table S10), and its Dscore was 2.57.In lung adenocarcinoma, 9 modules were identified by the resampling method. Using the 9 modules as control modules, patients in the test data set (TCGA) and the independent data set (GSE68465) were predicted as high-risk or low-risk. The control modules performed well in the test data set but poorly in the independent data set (Additional file 1: Table S11). In the independent data set, the log rank p-value of the prognostic risks between the two groups was 0.11.In lung adenocarcinoma, 1000 random gene sets were also selected to perform survival analysis in the data sets of lung adenocarcinoma. As a result, p-value of the permutation test was 0.0080, which may validate our method.In a previous work [35], 16 genes were used as markers to predict the prognostic risks of cancer patients in lung adenocarcinoma. In this work, we used this signature to calculate the prognostic risks in the same data sets of our modules. The gene signature could not discriminate the prognostic risks in both data sets (Additional file 1: Table S12). As to the performance of the 42-gene signature [6], it performed well in the testing data set. However, it couldn\u2019t distinguish the prognositc risks of cancer patients in the independent data set (Additional file 1: Table S13) and its Dscore was 3.19 (Table 1). The Dscores of our prognostic modules, the control modules and the signatures showed that the performance of our modules was the best and that the gene signature was the worst.From these results, in all three types of cancer, our prognostic modules, which were based on both the module network and the resampling method, outperform the control modules, random gene sets and the published signatures. The performance of the control modules was better than the gene signature. The strong performance of our prognostic modules not only revealed the superiority of our method but also validated the hypothesis that cross-talks among modules may influence the outcomes of cancer patients.To validate the clinical value of the genes involved in our modules, we investigated the overlaps between our prognostic genes and the known cancer genes. Furthermore, the overlap between our prognostic genes and the targets of drugs for the corresponding cancer was also investigated. In addition, the genes involved in the control modules were assessed using the same analysis to evaluate our method.The significance of the overlap between our prognostic genes and the known cancer genes may explain the distinguishing capability of our prognostic modules in cancer prognosis. An enrichment analysis of our prognostic genes with the targets of drugs for the corresponding cancer could prove the therapeutic value of our prognostic genes.Cancer driver genes were observed to be enriched in genes originating from ancestors of multicellular organisms [36] and genes originating from Eukaryota [19]. Previous studies have shown that the cancer prognosis prediction models based on gene signatures, which are consistent with the evolutionary feature, are more accurate [18] and robust [19]. Therefore, it is of great interest to investigate that whether the origin of our prognostic genes is consistent with the evolutionary feature. The human gene age information was obtained from a previous work [37], which divided the genes into eight classes according to their origins. The origins of the genes include the first cellular organism, the common ancestor of Eukaryota and Archaea (Euk_Archaea), Eukaryota, Opisthokonta, Eumetazoa, Vertebrata, Mammals, and horizontally transferring from Bacteria (Euk\u2009+\u2009Bac).The identification of prognostic genes that can distinguish the prognostic risks of cancer patients remains a big challenge. Based on the hypothesis that functional gene sets may be more stable than the gene signature and that the investigation of the cross-talks among the functional gene sets may facilitate the prioritization of key modules (functional gene sets) in the prognosis of cancer patients, we propose a new method that involves both the interactions among modules (gene sets) and the prognostic capability of the modules to identify the prognostic modules in cancers.We applied our method in three types of cancer, and the selected modules could distinguish the prognostic risks of cancer patients in a large number of data sets, including ovarian cancer, breast cancer and lung adenocarcinoma. The results showed that our prognostic modules performed better than the control modules, which were selected without using the module network. In addition, our prognostic modules also outperformed the published gene signature. All these results validate the hypothesis that the functional gene sets may be more stable than the gene signature and that the investigation of the cross-talks among the functional gene sets may facilitate the prioritization of key modules.Furthermore, the biological meaning and the therapeutic value of the prognostic modules were also investigated. In all three cancers, the genes in the prognostic modules were significantly enriched with known cancer genes and the targets of drugs for the corresponding cancers, indicating that our prognostic genes may be good candidates as drug targets in cancer.It is of great interest to investigate the evolutionary feature of the cancer driver genes. In this work, we also investigated the enrichment pattern of our prognostic genes with the genes originating from different stages of the evolutionary process. As a result, our prognostic genes were significantly enriched by the genes originating from the eukaryote in all the three types of cancer, which is consistent with the previous work [19].The good performance of our method may be due to three reasons. First, our method is based on a reasonable hypothesis. Second, our method is data driven. Unlike the traditional method, the modules are always known functional gene sets (i.e., GO term or Pathway) and the modules in our method are dense clusters in the gene co-expression network. Therefore, our method may identify new modules. Third, our method applies suitable calculation models. For example, the algorithm of GeneRank takes advantage of both the topological structure of the module network and the statistical relationship between the modules\u2019 gene expression data and the prognostic risks of cancer patients. As we know, the modules in the co-expression network are co-expressed with each other. Therefore, the use of the average value of the genes\u2019 expression levels in the module as the statistical value of the module may remove the noise in the gene expression data.In conclusion, we proposed a useful method to identify key modules in prognosis. Our method could also be applied in the study of other biological problems as long as there are enough samples with transcriptome data.Hazard RatioThe Cancer Genome AtlasThis work has been supported by the National Natural Science Foundation of China to X.H.Z. (61602201), the Fundamental Research Funds for the Central Universities to H.Y.Z. (2662017PY115) and X.H.Z (2662018PY023), and the National Instrumentaion Program (2013YQ19046707), Shenzhen Science & Technology Program (JCYJ20151029154245758, CKFW2016082915204709) to J.H. X.All data generated or analyzed during this study are included in this published article (and the additional information files). The code for this work is available at http://ibi.hzau.edu.cn/MNA/.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.In the co-expression network, if the number of edges across two modules is significantly high, then there may be cross-talk between the two modules. The significance of the cross-talk between every two modules is calculated by a permutation test, which is shown as follows:(1)\nThe number of the edges across the two modules in the gene expression network is calculated.\n\u00a0(2)\nTwo random gene sets, which contain the same number of genes as the two real modules, are selected from the gene co-expression network. Then, the number of edges across the random gene sets is calculated.\n\u00a0(3)\nThe procedure in step 2 is repeated 1000 times, and the edge numbers across the random gene sets are set as the null hypothesis distribution. Based on the null hypothesis distribution, the p-value of the cross-talk between the two modules is calculated.\n\u00a0First, the expression levels of the genes in the modules were averaged as the statistical values of the corresponding modules. The statistical values for each module in the corresponding patient could be calculated by the follow equation. (1)The GeneRank algorithm [17] succeeded in identifying key genes from the biological network. Here, we applied it to prioritize the essential modules in prognosis from the module network. The algorithm is described as follows: (2)As proved in this work [17], the above iteration corresponds to Jacobi on the system (3)Based on the prognostic modules, GGI [31] was applied to calculate the prognostic risk of each patient: (4)Furthermore, a discrimination score (Dscore) was defined to evaluate the distinguishing capability of the prognostic modules across various data sets, which is described as follows: (5)The hypergeometric test was applied to test whether the intersection of the genes in the prognostic modules and the known cancer genes (or the drug targets) is significant, which was calculated as follows: (6)In the testint data set (267 patients in TCGA), the HR (hazard ratio) of the two groups divided by our method was 1.72, and the log rank p-value was 8.90e-04 (Fig.\u00a01a). In a previous work [32], a merged data set containing data for 1287 patients was collected to validate the prognostic signature in ovarian cancer. Here, after removing the redundant samples which were from the TCGA, we used the other 865 samples as independent data set. After that, we used our prognostic modules to predict the prognostic risks of all the patients in the independent data set. As a result, the HR of the two groups predicted by our method was 1.64, and the p-value was 6.66e-08 (Fig.\u00a01b). These results indicate that our prognostic modules could discriminate the prognostic risks of cancer patients in ovarian cancer.\n\nFig. 1\nSurvival analysis in ovarian cancer data sets. In each data set, the patients are divided into two groups according to their risk scores calculated by using the prognostic modules\n\n\nFig. 2\nSurvival analysis in breast cancer data sets. In each data set, the patients are divided into two groups according to their risk scores calculated by using the prognostic modules\nBased on these modules, the prognostic risks of cancer patients in the test data set and the independent data set were calculated. The survival analysis in both data sets showed that our modules could distinguish the prognostic risks of cancer patients significantly, with HR values of 2.10 (p-value\u2009=\u20094.79e-04) in the test data set (Fig.\u00a03a) and 1.35 (p-value\u2009=\u20090.011) in the independent data set (Fig. 3b).\n\nFig. 3\nSurvival analysis in data sets of lung adenocarcinoma. In each data set, the patients are divided into two groups according to their risk scores calculated by using the prognostic modules\nIn ovarian cancer, 13 modules with the most correlated gene expression levels with the prognostic risks of cancer patients were identified by a resampling method. Then, the control modules were used to calculate the prognostic risks of cancer patients in the test data set and in the independent data set. The survival analysis showed that the control modules could distinguish the prognostic risks of cancer patients in both of the data sets (Additional file 1: Table S5). However, the control modules performed worse in both of the data sets compared with our prognostic modules. To evaluate the discrimination capability of the modules in various data sets, a Dscore was defined to characterize it (Method). As a result, the Dscore of our prognostic modules was 10.23, and the control modules achieved a Dscore of 7.78 (Table\u00a01).Table 1\nDscores of our prognostic modules, the control modules and the published signatures\n\nThe Dscore is defined to characterize the distinguishing capability of the prognostic modules across various data sets. A higher Dscore means a better performance in prognosis\nThe significance of the overlaps was calculated by the hypergeometric test. From Fig.\u00a04, we can see that the genes involved in the prognostic modules are significantly enriched by known cancer genes and the targets of drugs for the corresponding cancer. In addition, our prognostic genes outperformed the control genes in all investigations.\n\nFig. 4\nEnrichment analysis of the genes with the curated genes. a Enrichment analysis with known cancer genes. b Enrichment analysis with the targets of cancer drugs. The p-value was calculated by the hypergeometric test\nFor each cancer, the overlaps of the genes involved in the prognostic modules and the genes of different stages were calculated. The significances of the overlaps were calculated by a hypergeometric test (Fig.\u00a05). From this result, we can see that the genes originating from the eukaryote were significantly enriched with the prognostic genes of all the three kinds of cancers. Our previous work also proved that the cancer driver genes were enriched by genes originating from Eukaryota [19]. In addition, in a latest work [38], the authors investigated the difference of expression levels (tumors vs. normal samples) of the genes originating from different stages and found that the genes from the stage of eukaryota are the most up-regulated ones, which is consistent with our results.\n\nFig. 5\nEnrichment analysis of our prognostic genes with the genes originating from different stages. The p-value was calculated by the hypergeometric test\n\n\nAdditional file 1:\nThe file includes six figures (Figures S1\u2013S6) and thirteen tables (Tables S1-S13). (DOCX 2399 kb)", "s12859-019-2654-3": "Non-coding RNAs (ncRNAs) are emerging as key regulators and play critical roles in a wide range of tumorigenesis. Recent studies have suggested that long non-coding RNAs (lncRNAs) could interact with microRNAs (miRNAs) and indirectly regulate miRNA targets through competing interactions. Therefore, uncovering the competing endogenous RNA (ceRNA) regulatory mechanism of lncRNAs, miRNAs and mRNAs in post-transcriptional level will aid in deciphering the underlying pathogenesis of human polygenic diseases and may unveil new diagnostic and therapeutic opportunities. However, the functional roles of vast majority of cancer specific ncRNAs and their combinational regulation patterns are still insufficiently understood.Here we develop an integrative framework called CeModule to discover lncRNA, miRNA and mRNA-associated regulatory modules. We fully utilize the matched expression profiles of lncRNAs, miRNAs and mRNAs and establish a model based on joint orthogonality non-negative matrix factorization for identifying modules. Meanwhile, we impose the experimentally verified miRNA-lncRNA interactions, the validated miRNA-mRNA interactions and the weighted gene-gene network into this framework to improve the module accuracy through the network-based penalties. The sparse regularizations are also used to help this model obtain modular sparse solutions. Finally, an iterative multiplicative updating algorithm is adopted to solve the optimization problem.We applied CeModule to two cancer datasets including ovarian cancer (OV) and uterine corpus endometrial carcinoma (UCEC) obtained from TCGA. The modular analysis indicated that the identified modules involving lncRNAs, miRNAs and mRNAs are significantly associated and functionally enriched in cancer-related biological processes and pathways, which may provide new insights into the complex regulatory mechanism of human diseases at the system level.MicroRNAs (miRNAs) are small (~\u200922\u2009nt), endogenous, single-stranded and non-coding RNA molecules, which play crucial roles in post-transcriptional regulation by repressing mRNA translation or destabilizing target mRNAs [1]. Many studies have revealed that the mutation and dysregulated miRNA expression may cause various human diseases [2, 3]. MiRNAs act as essential components of complex regulatory networks and are involved in many different biological processes, such as cell proliferation, metabolism, and oncogenesis [4\u20136]. Therefore, understanding the functional roles and regulatory mechanisms of miRNAs will greatly facilitate the diagnosis and treatment of human diseases [7, 8].Recently, a competing endogenous RNA (ceRNA) hypothesis has been presented by Salmena et al. [9], which has dramatically shifted our understanding of miRNA regulatory mechanism. The complex ceRNA post-transcriptional regulatory mechanism reported that by sharing common miRNA response elements (MREs), several types of competing endogenous RNAs or miRNA sponges (e.g. lncRNAs, pseudogenes and circRNAs) compete with protein-coding RNAs for binding to miRNAs, thereby relieving miRNA-mediated target repression. Numerous convincing evidence has been discovered in a variety of species by biological experiments [10, 11]. For example, the study found that lncRNA HULS plays an important role in liver cancer, which serves as an endogenous sponge by reducing miR-372-mediated translational repression of PRKACB [12]. IPS1 overexpression has also been reported to increase the expression of PHO2 by competitively interacting with miR-399 in arabidopsis [13]. In addition, numerous studies have shown that ceRNA crosstalk exists in a variety of cellular behaviors, and many diseases are affected by their disturbances [14, 15]. However, the cooperative regulation mechanisms and the roles of ceRNA\u2013associated activities in physiologic and pathologic conditions are in their infancy, and thus require further research.The development of high-throughput techniques has made a vast amount of omics data to be publicly available, thereby enabling systematic investigation of the complex regulatory networks. Great efforts have been made to decipher the interaction mechanism of numerous biomolecules in a transcriptional or post-transcriptional level, such as co-regulatory motif discovery [16], miRNA-mRNA regulatory module identification [17, 18], miRNA and TF (transcription factor) co-regulation inference [19]. Meanwhile, other methods have been developed to prioritize cancer-related biological molecules, such as miRNAs [20, 21]. Undoubtedly, all these studies provide a global perspective for the study of combinatorial effects and human complex diseases.In recent years, lncRNAs as a class of ncRNAs and miRNA sponges have been identified in many human cancers [22]. Some systematic studies on many diseases have been carried out [23\u201325]. In addition, some tools related to lncRNA, such as DIANA-LncBase [26], Linc2GO [27] and LncRNADisease [28], have been developed. However, the functions and modular organizations of most of lncRNAs are still not clear, and the novel regulatory mechanism based on ceRNA hypothesis requires comprehensive investigation. To the best of our knowledge, little effort has been devoted to methods that are specifically designed to investigate the cancer-specific regulatory patterns involved in miRNA and miRNA sponges on a large scale.In this study, we develop a novel integrative framework called CeModule to systematically detect regulatory patterns involving lncRNAs, miRNAs, and mRNAs. The proposed method fully exploits the lncRNA/miRNA/mRNA expression profiles, the experimentally determined miRNA-lncRNA interactions, the verified miRNA-mRNA interactions, and the weighted gene-gene functional interactions. Here, inspired by [29\u201331], we adopt a model with joint orthogonality non-negative matrix factorization to detect these modules. In addition, both network-regularized constraints and sparsity penalties are incorporated into the model for helping to discover and characteriz the lncRNA-miRNA-mRNA associated regulatory modules. Finally, we apply the proposed method to ovarian cancer (OV) and uterine corpus endometrial carcinoma (UCEC) datasets downloaded from TCGA [32]. The results indicate that CeModule could be effectively applied to the discovery of biologically function modules, which greatly advances our understanding of the coordination mechanisms on a system level.In the following sections, we will first introduce the mathematical formulation of CeModule. Afterwards, the modules are identified based on the decomposed matrix components. Finally, several experiments and literature surveys are performed to systematically evaluate these modules.where \u03b31 and \u03b32 are the regularization coefficients.Apart from the expression profiles, the data sources including miRNA-lncRNA interactions, miRNA-mRNA interactions and gene-gene network have also been fully utilized to improve the performance. Here, to improve the quality of identified modules, the network-based penalties are imposed on this computational model based on Hoyer\u2019s work [6, 36] and make sure that those tightly linked lncRNAs/miRNAs/mRNAs are forced to assign into the same module.where \u03bb1, \u03bb2 and \u03bb3 are the regularization parameters. In the following, we adopt an iterative updating method [37] to obtain local optimal solution for the optimization problem.The four non-negative matrices W, H1, H2 and H3 are updated according to the above rules until convergence. More details about the derivations and proof for the convergence of the optimization problem are provided in the Additional file 1.We systematically evaluate the performance of CeModule by conducting a functional enrichment analysis for genes in each module. We downloaded the GO (Gene Ontology) terms in biological process from http://www.geneontology.org/, and obtained the canonical pathways from MSigDB [41]. We removed the GO terms with evidence codes equal to NAS (Non-traceable Author Statement), ND (No biological Data available) or EA (Electronic Annotation) and those with fewer than 5 genes similar to Li et al. [18]. The hypergeometric test was used to calculate the statistical significance for genes in each module with respect to each GO term or pathway. Meanwhile, we used TAM [42], which is a free online tool for annotations of human miRNAs, to perform enrichment analysis for miRNAs in the identified modules.We also investigate the miRNA cluster/family enrichment for each module, and obtained the miRNA cluster information and miRNA families from miRBase (http://www.mirbase.org/) (release 21) [43]. Furthermore, to determine whether these modules related to specific cancer, we acquired those known cancer-related lncRNAs from LncRNADisease [28] and Lnc2Cancer [44]. The verified disease-related miRNAs and genes were collected from HMDD v2.0 [45], and DisGeNET [46], respectively.Additionally, the method contains several parameters, more detailed information about them are illustrated in Additional file 1. Here, we determined the values of reduced dimension K on the basis of a miRNA cluster analysis. The results show that the miRNAs used in this study covered 69/76 miRNA clusters with an average of about 2.7/2.3 miRNAs per cluster for OV/UCEC dataset. Therefore, we set K to 70 in the two cancer datasets, which is approximately equal to the number of miRNA clusters.We applied CeModule to ovarian cancer (OV) and uterine corpus endometrial carcinoma (UCEC) genomic data and downloaded the matched mRNA and lncRNA expression profiles from http://www.larssonlab.org/tcga-lncrnas/ [47]. Due to the expression values of many lncRNAs/mRNAs in the original data source are all zeros or close to zeros, as done in [48], we removed some lncRNAs/mRNAs in the expression profiles with a variance less than the percentile specified by a cutoff (30%) and filter those lncRNAs/mRNAs with overall small absolute values less than another percentile cutoff (60%). The corresponding Matlab functions are genevarfilter and genelowvalfilter, respectively. We obtained the miRNA expression profiles of OV/UCEC from the TCGA data portal (http://cancergenome.nih.gov/) and removed the rows (or miRNAs) where all the expression values are zeros. These expression data were further log2-transformed. Finally, the datasets contain 7982(8056) lncRNAs, 415(505) miRNAs, and 10,618(10308) mRNAs across 385(183) matched samples for OV (UCEC), which were represented in three matrices X1, X2 and X3, and then the method in [49] is adopted to ensure non-negative constraints.The experimentally verified interactions between miRNAs and lncRNAs were downloaded from DIANA-LncBase [26] and starBase v2.0 [50]. We obtained the miRNA targets from three experimentally verified databases, including miRecords (version 4.0) [51], TarBase (version 6.0) [52], and miRTarBase (version 6.1) [53]. After filtering out duplicate interactions or interactions involving lncRNAs, miRNAs, and mRNAs that were absent in the expression profiles, 12,969/6165 miRNA-lncRNA and 20,848/25447 miRNA-mRNA interactions were finally retained for OV/UCEC dataset. The weighted gene-gene network is derived from HumanNet [54], which is a probabilistic functional gene network. After filtering those genes absent from the expression data, 536,698/252021 interactions are retained for OV/UCEC. Finally, we obtained the miRNA-lncRNA matrix A, the miRNA-mRNA matrix B and the gene-gene matrix C.We identified modules in ovarian cancer and uterine corpus endometrial carcinoma by integrating multiple heterogeneous data sources, and obtained 70 modules for OV/UCEC (Additional file 2: Table S1) with an average of 68.2/46.1 lncRNAs, 6.3/5.5 miRNAs, and 55.5/48.1 mRNAs per module. The distributions of number of lncRNAs, miRNAs, and mRNAs for the identified modules for OV and UCEC datasets are displayed in Additional file 1: Figure S1 and S2.which is defined as the average absolute values of PCCs (Pearson correlation coefficients) for all lncRNA-miRNA, miRNA-mRNA, and lncRNA-mRNA pairs, where N is the number of all the possible pairs for the three types of relationships in Cv, corr is a function for calculating the pair-wise PCC based on the corresponding expression data.Base on the fact that the input data included the lncRNA, miRNA and mRNA expression profiles of OV and UCEC samples, we expect the modules indentified by our method to be related to cancers, especially OV/UCEC. Here, we obtained 82/265/4288 (116/322/4721) cancer-related lncRNAs/miRNAs/mRNAs that are involved in the expression profiles as the benchmark sets for OV (UCEC), and collected 11/5 lncRNAs, 83/75 miRNAs and 73/158 mRNAs related to OV/UCEC from several reliable databases as mentioned in the Section of Methods.For OV (UCEC) dataset, the identified modules involve 1258/171/2172 (1252/172/2498) different lncRNAs/miRNAs/mRNAs. In the results of OV, as shown in Fig. 5b, 43 lncRNAs belong to the benchmark set of cancer lncRNAs (p-value\u2009=\u20091.18e-14, hypergeometric test), and 8 of them are relevant to ovarian cancer (p-value\u2009=\u20093.93e-05). In UCEC, 47 lncRNAs in those modules belong to the corresponding benchmark set (p-value\u2009=\u20096.05e-11) and 3 of which are UCEC specific lncRNAs (p-value\u2009=\u20092.93e-02). For miRNAs, 64.9%/77.3% of the 171/172 miRNAs are known to be involved in cancer in both datasets, and 51/43 miRNAs are specifically associated with OV/UCEC (p-value\u2009=\u20092.70e-05 for OV, p-value\u2009=\u20096.29e-06 for UCEC). Meanwhile, 1058/1186 mRNAs have been verified to be related to cancer, and 27/29 mRNAs are confirmed to be associated with ovarian cancer and uterine corpus endometrial carcinoma in OV and UCEC datasets, respectively. All the cancer-related and OV (UCEC) related molecules in those modules for both datasets are listed in Additional file 6: Table S6.We also performed a differential expression analysis by two-sample t-test for those OV-related miRNAs (83 miRNAs) to investigate the cancer-specific abnormal changes in expression profile data. As a result, we identified 13 differentially expressed miRNAs (mir-200c, mir-99b, mir-183, mir-187, mir-10b, mir-625, mir-92b, mir-182, mir-449b, mir-107, mir-134, mir-98, mir-141, Additional file 7: Table S7) from those miRNAs, and found that 62.9% (44/70, Additional file 7: Table S7) of the modules contain at least one miRNAs that are differential expression. There are four modules (modules 13, 57, 60, and 69) are significantly enriched in ovarian cancer related differentially expressed miRNAs (hypergeometric test, FDR\u2009<\u20090.05, Additional file 7: Table S7). For example, module 57 contains 5 OV-related miRNAs (mir-182, mir-183, mir-200c, mir-625, mir-99b) and all of them are differential expression (FDR\u2009=\u20092.40e-05). The above observations imply that the lncRNAs/miRNAs/mRNAs in the identified modules are involved in various cancers, which confirm that the proposed method has a potential capability to discover modules related to cancers.Increasing evidence indicates that a novel competitive endogenous RNA (ceRNA) regulatory mechanism exists between non-coding RNAs and protein-coding RNAs. LncRNAs and miRNAs are two kinds of crucial regulators and participate in many important biological processes. The aberrant expression of lncRNAs and miRNAs often contribute to tumorigenesis. To utilize the tremendous amounts of heterogeneous omics data and investigate the synergistic and cooperative mechanisms involve in lncRNAs, miRNAs, and mRNAs, our method integrates lncRNA/miRNA/mRNA expression profile data in an NMF framework, and simultaneously incorporates interaction networks in a regularized manner. The results of both (OV/UCEC) datasets indicate that the modules identified by CeModule contain many lncRNAs/miRNAs/mRNAs with specific topological patterns that are involved in some crucial biological processes and may cause cancers. Meanwhile, we further investigated whether the discovered modules were associated with the survival of ovarian cancer patients. The clinical data are downloaded from TCGA, and 383 samples are retained after removing those not included in the expression data or those with unavailable survival time. Kaplan-Meier survival analysis also indicates the ability of the method to discover modules that provide useful information for the prediction of cancer prognosis (Additional file 1).In this study, we systematically investigate the efficiency of CeModule in identifying biologically functional modules that related to specific biological processes or cancers. We applied our method on the lncRNA/miRNA/mRNA expression data with matched samples of ovarian cancer and uterine corpus endometrial carcinoma from TCGA, and finally obtained 70 regulatory modules in both datasets. The observations indicate that these modules are densely connected and show specific topological characteristics. Meanwhile, these modules are significantly associated with many disease-related biological processes and pathways. Furthermore, a large number of lncRNAs/miRNAs/mRNAs in the modules are involved in various human complex diseases, such as ovarian cancer. All the results fully demonstrate the capability of CeModule for identifying of biologically functional modules. As a large number of sample-matched lncRNAs/miRNAs/mRNAs expression profile data become available, we believe that CeModule can serve as a potential tool for revealing condition-specific ceRNA regulatory patterns for cancer.Competing endogenous RNAElectronic AnnotationGene OntologyLong non-coding RNAsmiRNA response elementsNontraceable Author StatementNon-coding RNAsNo biological Data availableNon-negative Matrix FactorizationOvarian CancerThe Cancer Genome AtlasThis work was supported by the National Natural Science Foundation of China (Grant nos. 61873089, 61572180, 61602283, 61862025), Shandong Provincial Natural Science Foundation, China (Grant no. ZR2016FB10), Hunan Provincial Science and Technology Project Foundation, China (Grant no. 2018TP1018), Jiangxi Provincial Natural Science Foundation, China (Grant no. 20181BAB211016), Hunan Provincial Natural Science Foundation, China (Grant no. 2018JJ2024), and Key Project of the Education Department of Hunan Province, China (Grant no. 17A037). The funding bodies did not play any role in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.The datasets supporting the conclusions of this article are included within.the article and its additional files. The code used in the current study is available at https://github.com/xiaoqiu2018/CeModule.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.In this study, we identify the lncRNA, miRNA and mRNA-associated regulatory modules by a non-negative matrix factorization (NMF)-based framework. The corresponding objective function of standard NMF [31, 33] is formulated as follows: (1)where ||.||F denotes the Frobenius norm.Existing studies have indicated that orthogonality NMF could produce a better modularity interpretation [6, 30, 34]. Therefore, we present a integrative framework using joint orthogonality NMF to determine the module regulation and membership through simultaneously integrating multiple data sources. To clearly describe the problem, let X1\u2208R S\u2009\u00d7\u2009N1, X2\u2208RS\u2009\u00d7\u2009N2, and X3\u2208RS\u2009\u00d7\u2009N3 denote the lncRNA, miRNA, and mRNA expression matrices, respectively. Subsequently, we define an objective function of joint orthogonality NMF as follows: (2)where W(size:S\u2009\u00d7\u2009K) denotes the common basic matrix; coefficient matrices H1, H2, and H3 have dimensions N1\u2009\u00d7\u2009K, N2\u2009\u00d7\u2009K, and N3\u2009\u00d7\u2009K, respectively; \u03b1 is the hyperparameter that controls the trade-off of Hi.; dimension K represents the desired number of modules.However, many data sources often contain noise, and several investigations of NMF have been conducted to improve the performance [35]. To obtain sparse solutions and regulatory modules with better biological interpretation, the sparse constraints were incorporated into this model similar to that suggested by Hoyer [36], which can effectively make matrices Hi sparse. The objective function of joint orthogonality NMF with sparsity penalties can be written as follows: (3)Let A\u2208RN2\u2009\u00d7\u2009N1 and B\u2208RN2\u2009\u00d7\u2009N3 denote the adjacency matrices of miRNA-lncRNA and miRNA-mRNA interaction networks, respectively, C\u2208RN3\u00a0\u00d7\u2009N3 is the matrix of gene-gene functional interaction network. For the miRNA-lncRNA interaction network, we perform the network-based constraints according to the objective function as follows: (4)where aij is the entity of A; hi(2) and hj(1) represent the ith and jth rows of H2 and H1, respectively. Similarly, the corresponding objective functions of two other networks can be obtained as follows: (5) (6)Then, combining the function in Eq. (3) with three network-based regularization terms, we can mathematically formulate the optimization problem of CeModule as follows: (7)Let \u03a6\u2009=\u2009[\u03c6lk],\u03a8\u2009=\u2009[\u03c8jk], \u03a9\u2009=\u00a0[\u03c9pk], and \u0398\u2009=\u2009[\u03b8qk] be the Lagrange multipliers for constrain wlk\u2009\u2265\u20090, hjk(1)\u2009\u2265\u20090, hpk(2)\u2009\u2265\u20090, and hpk(3)\u2009\u2265\u20090, respectively. We can obtain the Lagrange function of Eq. (7) as follows: (8)where E1\u2208{1}N1\u00a0\u00d7\u2009K, E2\u2208{1}N2\u00a0\u00d7\u2009K, and E3\u2208{1}N3\u00a0\u00d7\u2009K. The partial derivatives of the above function for W and Hi are: (9)Using the KKT conditions [38, 39] \u03c6lkwlk\u2009=\u20090, \u03c8jkhjk(1)\u2009=\u20090, \u03c9pkhpk(2)\u2009=\u20090, and \u03b8qkhpk(3)\u2009=\u20090, we obtain the following equations for wlk, hjk(1), hpk(2), and hpk(3): (10)Finally, we determine the multiplicative update rules for W and Hi as follows: (11)The obtained coefficient matrices H1, H2, and H3 will guide us to detect ceRNA-associated regulatory modules. Here, similar to the way for identifying co-modules developed by Chen et al. [40], we obtain a z-score for each element based on the columns of H1, H2, and H3 as follows: zij\u2009=\u2009(xij-\u03bcj)/\u03c3j, where \u03bcj denotes the average value of lncRNA (or miRNA, mRNA) i in H1 (or H2, H3), and \u03c3j is the standard deviation. Subsequently, we assign lncRNA (or miRNA, mRNA) i into module j if zij exceeds a given threshold T, and then all the ceRNA-associated modules can be obtained. The overall workflow of the proposed CeModule framework for identifying regulatory module is shown in Fig.\u00a01.\n\nFig. 1\nOverall workflow of CeModule for detecting lncRNA, miRNA, and mRNA-associated regulatory patterns\nAccording to the constructed regulatory networks by merging those modules identified by our method, we found that a small number of nodes are more likely to be hubs or act as bridges, and tend to be involved in more competing interactions and participate in more human diseases. For instance, Fig.\u00a02a presents a global view of the regulatory network for OV, which demonstrated that the network was densely connected and a small fraction of the nodes presented significantly higher degree, betweenness centrality, and closeness centrality than other nodes. The top 10 lncRNAs/miRNAs/mRNAs for each dimension (degree, closeness, and betweenness) in the networks of OV and UCEC datasets are listed in Table\u00a01 and Additional file 1: Table S2, and there are substantial overlaps exist across the three dimensions (Fig. 2b and Additional file 1: Figure S3 and S4). Meanwhile, as shown in Fig. 2c and Additional file 1: Table S2, we found that all the top 10 lncRNAs (MALAT1, NEAT1, GAS5, H19, SNHG1, TUG1, FGD5-AS1, SNHG5, XIST, MEG3) and 8 out of the top 10 lncRNAs (MAL2, XIST, SCAMP1, C17orf76-AS1, MALAT1, C11orf95, SEC22B, UBXN8) with the highest degree participate in at least 5 or more modules in OV and UCEC datasets, respectively. The number distributions of modules for all the module members (lncRNAs/miRNAs/mRNAs) are provided in Additional file 2: Table S1.\n\nFig. 2\nTopological features of the identified modules and the ceRNA regulatory network for ovarian cancer. a View of the ceRNA module network in OV. If two nodes are members of a module and their interactions exist in the databases as mentioned in the aforementioned interaction databases, then an edge between the two nodes is displayed. Three colors (black, purple and green) correspond to three types of interactions (lncRNA-miRNA, miRNA-gene and gene-gene). Nodes with no edges are omitted to improve visualization. b Overlap of the top 10 lncRNAs across three dimensions for OV. c The distributions of number of modules identified by CeModule for the top 10 lncRNAs, miRNAs, and mRNAs with the highest degree in OV dataset\nTable 1\nThe top 10 lncRNAs, miRNAs and mRNAs with the highest degree, closeness centrality, and betweenness centrality in OV\nOn the other hand, most of the above lncRNAs are supported to be associated with different cancers by public databases or literature. For example, MALAT1 was found to be overexpressed in many solid tumors such as hepatocellular carcinoma [55] and lung cancer [56]. The downregulation of MEG3 is related to poor prognosis and promotes cell proliferation in gastric cancer [57] and bladder cancer [58]. Moreover, MALAT1, NEAT1, GAS5, H19 and XIST have been experimentally validated to be ovarian cancer-related lncRNAs [44], which were identified as hubs that connect 26, 15, 22, 20 and 9 modules in OV dataset, respectively. Additionally, MALAT1 also has been supported to be related to uterine corpus endometrial carcinoma and connected 7 modules in UCEC dataset. The above observations indicate that these lncRNAs can control communication among different functional components in the two datasets. Meanwhile, 8 (let-7b, mir-99b, mir-10b, mir-30a, mir-182, mir-183, mir-200c, mir-25) and 5 (mir-141, mir-10a, mir-200a, let-7b, mir-200b) of the 10 miRNAs with the highest degree are confirmed to be the well-known OV-related and UCEC-related miRNAs by HMDD [45]. We also found that these miRNAs are significantly enriched in cell cycle-related biological processes (Fig.\u00a03a). In addition, we performed the same analysis for mRNAs and also came to the similar observations.\n\nFig. 3\na Functional enrichment analysis for the 10 miRNAs with the highest degree using TAM in OV. b Pathway enrichment analysis of the module 15 in OV dataset. c Pathway enrichment analysis of the module 17 in OV dataset. The area proportion of each pathway presents the number of genes enriched in this pathway\nTo investigate the functional significance of the identified modules in ovarian cancer and uterine corpus endometrial carcinoma datasets, we perform GO biological process and KEGG pathway enrichment analyses using hypergeometric test for coding genes in each of the modules (FDR\u2009<\u20090.05). The enriched GO terms and KEGG pathways of all the identified modules for OV and UCEC datasets are listed in Additional file 3: Table S3 and Additional file 4: Table S4. The results show that about 88.6%/91.4% of the modules in OV/UCEC are significantly enriched in at least one GO terms, and 110/129 different enriched pathways are discovered for the identified modules. The most frequently enriched biological processes contain cell adhesion, immune response, signal transduction, cell cycle and inflammatory response. For instance, Table\u00a02 lists the representative enriched GO terms for the selected modules in OV dataset, and we found that these modules are involved in many biological processes or pathways that related to cancers [59, 60]. For example, module 2 is enriched in regulation of cell activation (GO:0050865) and immune system process (GO:0002376), and modules 7 and 15 are enriched in p53 signaling pathway (KEGG: hsa04115) and Focal adhesion (KEGG: hsa04510), respectively. As shown in Fig. 3b and c, we also found that some enriched pathways are shared by several modules, and some of them have been reported to be involved in OV [61]. Interestingly, these two modules contain three common mRNAs (EMILIN1, COL1A2, ENC1) and one of them (COL1A2) is related to cancer, suggesting that these modules (e.g. modules 15 and 17, modules 31 and 32 in OV) with many overlaps of mRNAs are more likely to have similar biological functions.Table 2\nRepresentative enriched GO terms of the selected modules for OV dataset\n\nNote: The bold letters represent the lncRNAs/miRNAs/mRNAs related to ovarian cancer; q-value represents the corrected p-value using the Benjamini-Hochberg method\nAccumulating evidence has demonstrated that miRNAs located in the same cluster or belonging to the same family are likely to function synergistically or are related to the same diseases [42]. In this study, we also conducted a miRNA cluster/family enrichment analysis for the identified modules based on TAM (http://www.cuilab.cn/tam) [42]. The results indicated that 35/27 of the identified modules are significantly enriched in at least one miRNA cluster or miRNA family for OV/UCEC (p-value<\u20090.05) (Additional file 5: Table S5). For instance (see Table\u00a03), module 1 in OV contains 9 miRNAs, 4 of which (mir-362, mir-532, mir-500, mir-501) belong to the miR-188 cluster, and three miRNAs (mir-362, mir-532, mir-501) have been supported to be associated with cancer by HMDD. Moreover, two miRNAs (mir-200b, mir-200c) in this module, which belong to the miRNA family MIPF0000019, have been shown to be related to OV [45], while another two miRNAs (mir-500, mir-501) also belong to the miRNA family MIPF0000139. As another example, two of 8 miRNAs (let-7c, mir-99a) in module 20 are from the let-7c cluster and have been shown to be dysregulated in various cancers [17]. All the findings indicate the capability of CeModule in discovering cancer-specific modules.Table 3\nOverlapping miRNAs for the identified modules and clusters/families in OV\n\nNote: a/b represent the miRNAs that overlap between modules and miRNA clusters as well as families\nWe also performed an analysis to evaluate the statistical significance of (anti)-correlations between lncRNAs, miRNAs and mRNAs within modules for both datasets. We expect that the molecules within those modules identified by CeModule are more (anti)-correlated than random sets of genes. Here, we define a correlation evaluation score to quantify the strength of competition in any given module Cv as follows: (12)To investigate the statistical significance, we adopt a permutation test by shuffling these lncRNAs, miRNAs and mRNAs according to those identified modules, and then compute the average competing evaluation score for them. As shown in Fig. 4a, the correlation evaluation scores of our method ranged from 0.072 to 0.352 for OV, and ranged from 0.100 to 0.489 for UCEC, they exhibit significantly higher correlation than the random modules (p-value\u2009=\u20091.20e-20 for OV, p-value\u2009=\u20093.03e-17 for UCEC, Wilcoxon rank sum test). We can also obtain the same conclusions on the two examples for modules 1 (p-value\u2009=\u20092.70e-06, Student\u2019s t-test) and 2 (p-value\u2009=\u20091.04e-09) (Fig. 4b). Here, the correlation evaluation scores of these identified modules are generally weak, this is mainly due to the fact that the vast majority of Pearson correlation coefficients (PCCs) of lncRNA-miRNA, miRNA-mRNA and lncRNA-mRNA pairs were weak in the used datasets of OV and UCEC (Table 4).\n\nFig. 4\na Comparison of the correlation evaluation scores between all the identified modules by CeModule and the randomly generated modules for ovarian cancer dataset. b Distribution of the correlation evaluation scores of the 1000 random modules with the same size for modules 1 and 2 in ovarian cancer dataset\nTable 4\nStatistics of the correlation coefficients in OV and UCEC datasets\n\nNote: Ave (lnc-miR), Ave (miR-mR) and Ave (lnc-mR) are the average absolute Pearson correlation coefficients of all lncRNA-miRNA, miRNA-mRNA and lncRNA-mRNA pairs, respectively; Ave-mod is the correlation evaluation score across all modules\nAs shown in Fig.\u00a05a, 45.7% (92.9%), 71.4% (90.0%) and 22.9% (100%) of all the identified modules in OV dataset contained at least two OV-related (cancer-related) lncRNAs, miRNAs and mRNAs, respectively. Meanwhile, the corresponding ratios in UCEC dataset are 1.4% (62.9%), 64.3% (91.4%) and 10.0% (100%) for uterine corpus endometrial carcinoma-related (cancer-related) lncRNAs, miRNAs and mRNAs. The significant level of overlap between every module and cancer (OV/UCEC) lncRNAs/miRNAs/mRNAs is evaluated by hypergeometric test, and Table\u00a05 lists the OV-related and cancer-related lncRNAs for several representative modules. For example, module 66 in OV dataset contains 58 lncRNAs, 9 of which are cancer lncRNAs and 6 of them are ovarian cancer lncRNAs. To take another example, module 51 in UCEC dataset contains 61 lncRNAs, 8 of which are cancer lncRNAs and 3 of them are uterine corpus endometrial carcinoma-related lncRNAs. We provided all the cancer (OV/UCEC) related modules for both datasets in Additional file 6: Table S6.\n\nFig. 5\na Percentage of modules with at least two known cancer-related (ovarian cancer-related)lncRNAs/miRNAs/mRNAs in ovarian cancer dataset. b Overlap of cancer lncRNAs, and ovarian cancer lncRNAs between the benchmark set and lncRNAs in the identified modules for ovarian cancer dataset\nTable 5\nKnown ovarian cancer-associated and cancer- associated lncRNAs for these representative modules in OV\n\nNote: Numa and Numb are the ratios of lncRNAs that associated with cancer and OV in these modules. q-value is the FDR-corrected p-value after multiple testing correction\n\n\nAdditional file 1:\nFigure S1. Topological features of the identified modules and the ceRNA regulatory module network. The distributions of number of (A) lncRNAs, (B) miRNAs, and (C) mRNAs for the identified modules in OV dataset. Figure S2. Topological features of the identified modules and the ceRNA regulatory module network. The distributions of number of (A) lncRNAs, (B) miRNAs, and (C) mRNAs for the identified modules in UCEC dataset. Figure S3. Overlap of the top 10 (A) miRNAs and (B) mRNAs across three dimensions (degree, betweenness centrality, and closeness centrality) in OV dataset. Figure S4. Overlap of the top 10 (A) lncRNAs, (B) miRNAs and (C) mRNAs across three dimensions (degree, betweenness centrality, and closeness centrality) in UCEC dataset. Figure S5 Kaplan-Meier survival curves for ovarian cancer patients classified into two groups using the module-averaged lncRNA expression levels. Table S2. The top 10 lncRNAs, miRNAs and mRNAs with the highest degree, closeness centrality, and betweenness centrality in UCEC. (PDF 525 kb)\n\n\n\nAdditional file 2:\nTable S1. The list of all the identified modules that involving lncRNAs, miRNAs and mRNAs. (XLSX 248 kb)\n\n\n\nAdditional file 3:\nTable S3. Results of the enriched GO biological processes for the identified modules. (XLSX 620 kb)\n\n\n\nAdditional file 4:\nTable S4. Results of the enriched KEGG pathways for the identified modules. (XLSX 105 kb)\n\n\n\nAdditional file 5:\nTable S5. The list of regulatory modules enriched in miRNA cluster and miRNA family. (XLSX 18 kb)\n\n\n\nAdditional file 6:\nTable S6. Known OV/UCEC-related lncRNAs/miRNAs/mRNAs and cancer-related lncRNAs/miRNAs/mRNAs in modules. (XLSX 52 kb)\n\n\n\nAdditional file 7:\nTable S7. Differentially expressed miRNAs identified in modules. (XLSX 14 kb)", "s12859-019-2604-0": "Given the importance of relation or event extraction from biomedical research publications to support knowledge capture and synthesis, and the strong dependency of approaches to this information extraction task on syntactic information, it is valuable to understand which approaches to syntactic processing of biomedical text have the highest performance.We perform an empirical study comparing state-of-the-art traditional feature-based and neural network-based models for two core natural language processing tasks of part-of-speech (POS) tagging and dependency parsing on two benchmark biomedical corpora, GENIA and CRAFT. To the best of our knowledge, there is no recent work making such comparisons in the biomedical context; specifically no detailed analysis of neural models on this data is available. Experimental results show that in general, the neural models outperform the feature-based models on two benchmark biomedical corpora GENIA and CRAFT. We also perform a task-oriented evaluation to investigate the influences of these models in a downstream application on biomedical event extraction, and show that better intrinsic parsing performance does not always imply better extrinsic event extraction performance.We have presented a detailed empirical study comparing traditional feature-based and neural network-based models for POS tagging and dependency parsing in the biomedical context, and also investigated the influence of parser selection for a biomedical event extraction downstream task.We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.The biomedical literature, as captured in the parallel repositories of PubMed1 (abstracts) and PubMed Central2 (full text articles), is growing at a remarkable rate of over one million publications per year. Effort to catalog the key research results in these publications demands automation [1]. Hence extraction of relations and events from the published literature has become a key focus of the biomedical natural language processing community.Methods for information extraction typically make use of linguistic information, with a specific emphasis on the value of dependency parses. A number of linguistically-annotated resources, notably including the GENIA [2] and CRAFT [3] corpora, have been produced to support development and evaluation of natural language processing (NLP) tools over biomedical publications, based on the observation of the substantive differences between these domain texts and general English texts, as captured in resources such as the Penn Treebank [4] that are standardly used for development and evaluation of syntactic processing tools. Recent work on biomedical relation extraction has highlighted the particular importance of syntactic information [5]. Despite this, that work, and most other related work, has simply adopted a tool to analyze the syntactic characteristics of the biomedical texts without consideration of the appropriateness of the tool for these texts. A commonly used tool is the Stanford CoreNLP dependency parser [6], although domain-adapted parsers (e.g. [7]) are sometimes used.Prior work on the CRAFT treebank demonstrated substantial variation in the performance of syntactic processing tools for that data [3]. Given the significant improvements in parsing performance in the last few years, thanks to renewed attention to the problem and exploration of neural methods, it is important to revisit whether the commonly used tools remain the best choices for syntactic analysis of biomedical texts. In this paper, we therefore investigate current state-of-the-art (SOTA) approaches to dependency parsing as applied to biomedical texts. We also present detailed results on the precursor task of POS tagging, since parsing depends heavily on POS tags. Finally, we study the impact of parser choice on biomedical event extraction, following the structure of the extrinsic parser evaluation shared task (EPE 2017) for biomedical event extraction [8]. We find that differences in overall intrinsic parser performance do not consistently explain differences in information extraction performance.We use two biomedical corpora: GENIA [2] and CRAFT [3]. GENIA includes abstracts from PubMed, while CRAFT includes full text publications. It has been observed that there are substantial linguistic differences between the abstracts and the corresponding full text publications [9]; hence it is important to consider both contexts when assessing NLP tools in biomedical domain.The GENIA corpus contains 18K sentences (\u223c486K words) from 1999 Medline abstracts, which are manually annotated following the Penn Treebank (PTB) bracketing guidelines [2]. On this treebank, we use the training, development and test split from [10]3. We then use the Stanford constituent-to-dependency conversion toolkit (v3.5.1) to generate dependency trees with basic Stanford dependencies [11].The CRAFT corpus includes 21K sentences (\u223c561K words) from 67 full-text biomedical journal articles4. These sentences are syntactically annotated using an extended PTB tag set. Given this extended set, the Stanford conversion toolkit is not suitable for generating dependency trees. Hence, a dependency treebank using the CoNLL 2008 dependencies [12] was produced from the CRAFT treebank using ClearNLP [13]; we directly use this dependency treebank in our experiments. We use sentences from the first 6 files (PubMed IDs: 11532192\u201312585968) for development and sentences from the next 6 files (PubMed IDs: 12925238\u201315005800) for testing, while the the remaining 55 files are used for training.MarMoT [14] is a well-known generic CRF framework as well as a leading POS and morphological tagger5.NLP4J\u2019s POS tagging model [15] (NLP4J-POS) is a dynamic feature induction model that automatically optimizes feature combinations6. NLP4J is the successor of ClearNLP.BiLSTM-CRF [16] is a sequence labeling model which extends a standard BiLSTM neural network [17, 18] with a CRF layer [19].BiLSTM-CRF+CNN-char extends the model BiLSTM-CRF with character-level word embeddings. For each word token, its character-level word embedding is derived by applying a CNN to the word\u2019s character sequence [20].BiLSTM-CRF+LSTM-char also extends the BiLSTM-CRF model with character-level word embeddings, which are derived by applying a BiLSTM to each word\u2019s character sequence [21].For the three BiLSTM-CRF-based sequence labeling models, we use a performance-optimized implementation from [22]7. As detailed later in the \u201cPOS tagging results\u201d section, we use NLP4J-POS to predict POS tags on development and test sets and perform 20-way jackknifing [23] to generate POS tags on the training set for dependency parsing.The Stanford neural network dependency parser [6] (Stanford-NNdep) is a greedy transition-based parsing model which concatenates word, POS tag and arc label embeddings into a single vector, and then feeds this vector into a multi-layer perceptron with one hidden layer for transition classification8.NLP4J\u2019s dependency parsing model [26] (NLP4J-dep) is a transition-based parser with a selectional branching method that uses confidence estimates to decide when employing a beam9.jPTDP v1 [27] is a joint model for POS tagging and dependency parsing,10 which uses BiLSTMs to learn feature representations shared between POS tagging and dependency parsing. jPTDP can be viewed as an extension of the graph-based dependency parser bmstparser [28], replacing POS tag embeddings with LSTM-based character-level word embeddings. For jPTDP, we train with gold standard POS tags.The Stanford \u201cBiaffine\u201d parser v1 [29] extends bmstparser with biaffine classifiers to predict dependency arcs and labels, obtaining the highest parsing result to date on the benchmark English PTB. The Stanford Biaffine parser v2 [30], further extends v1 with LSTM-based character-level word embeddings, obtaining the highest result (i.e., 1st place) at the CoNLL 2017 shared task on multilingual dependency parsing [31]. We use the Stanford Biaffine parser v2 in our experiments11.We use the training set to learn model parameters while we tune the model hyper-parameters on the development set. Then we report final evaluation results on the test set. The metric for POS tagging is the accuracy. The metrics for dependency parsing are the labeled attachment score (LAS) and unlabeled attachment score (UAS): LAS is the proportion of words which are correctly assigned both dependency arc and label while UAS is the proportion of words for which the dependency arc is assigned correctly.For the three BiLSTM-CRF-based models, Stanford-NNdep, jPTDP and Stanford-Biaffine which utilizes pre-trained word embeddings, we employ 200-dimensional pre-trained word vectors from [32]. These pre-trained vectors were obtained by training the Word2Vec skip-gram model [33] on a PubMed abstract corpus of 3 billion word tokens.For the traditional feature-based models MarMoT, NLP4J-POS and NLP4J-dep, we use their original pure Java implementations with default hyper-parameter settings.For the BiLSTM-CRF-based models, we use default hyper-parameters provided in [22] with the following exceptions: for training, we use Nadam [34] and run for 50 epochs. We perform a grid search of hyper-parameters to select the number of BiLSTM layers from {1,2} and the number of LSTM units in each layer from {100, 150, 200, 250, 300}. Early stopping is applied when no performance improvement on the development set is obtained after 10 contiguous epochs.For Stanford-NNdep, we select the wordCutOff from {1,2} and the size of the hidden layer from {100, 150, 200, 250, 300, 350, 400} and fix other hyper-parameters with their default values.For jPTDP, we use 50-dimensional character embeddings and fix the initial learning rate at 0.0005. We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from {100,150,200,250,300}. Other hyper-parameters are set at their default values.For Stanford-Biaffine, we use default hyper-parameter values [30]. These default values can be considered as optimal ones as they helped producing the highest scores for 57 test sets (including English test sets) and second highest scores for 14 test sets over total 81 test sets across 45 different languages at the CoNLL 2017 shared task [31].Table\u00a03 presents POS tagging accuracy of each model on the test set, based on retraining of the POS tagging models on each biomedical corpus. The penultimate row presents the result of the pre-trained Stanford POS tagging model english-bidirectional-distsim.tagger [35], trained on a larger corpus of sections 0\u201318 (about 38K sentences) of English PTB WSJ text; given the use of newswire training data, it is unsurprising that this model produces lower accuracy than the retrained tagging models. The final row includes published results of the GENIA POS tagger [36], when trained on 90% of the GENIA corpus (cf. our 85% training set)12. It does not support a (re)-training process.In general, we find that the six retrained models produce competitive results. BiLSTM-CRF and MarMoT obtain the lowest scores on GENIA and CRAFT, respectively. jPTDP obtains a similar score to MarMoT on GENIA and similar score to BiLSTM-CRF on CRAFT. In particular, MarMoT obtains accuracy results at 98.61% and 97.07% on GENIA and CRAFT, which are about 0.2% and 0.4% absolute lower than NLP4J-POS, respectively. NLP4J-POS uses additional features based on Brown clusters [37] and pre-trained word vectors learned from a large external corpus, providing useful extra information.BiLSTM-CRF obtains accuracies of 98.44% on GENIA and 97.25% on CRAFT. Using character-level word embeddings helps to produce about 0.5% and 0.3% absolute improvements to BiLSTM-CRF on GENIA and CRAFT, respectively, resulting in the highest accuracies on both experimental corpora. Note that for PTB, CNN-based character-level word embeddings [20] only provided a 0.1% improvement to BiLSTM-CRF [16]. The larger improvements on GENIA and CRAFT show that character-level word embeddings are specifically useful to capture rare or unseen words in biomedical text data. Character-level word embeddings are useful for morphologically rich languages [27, 38], and although English is not morphologically rich, the biomedical domain contains a wide variety of morphological variants of domain-specific terminology [39]. Words tagged incorrectly are largely associated with gold tags NN, JJ and NNS; many are abbreviations which are also out-of-vocabulary. It is typically difficult for character-level word embeddings to capture those unseen abbreviated words [40].On both GENIA and CRAFT, BiLSTM-CRF with character-level word embeddings obtains the highest accuracy scores. These are just 0.1% absolute higher than the accuracies of NLP4J-POS. Note that small variations in POS tagging performance are not a critical factor in parsing performance [41]. In addition, we find that NLP4J-POS obtains 30-time faster training and testing speed. Hence for the dependency parsing task, we use NLP4J-POS to perform 20-way jackknifing [23] to generate POS tags on training data and to predict POS tags on development and test sets.We present the LAS and UAS scores of different parsing models in Table\u00a04. The first five rows show parsing results on the GENIA test set of \u201cpre-trained\u201d parsers. The first two rows present scores of the pre-trained Stanford NNdep and Biaffine v1 models with POS tags predicted by the pre-trained Stanford tagger [35], while the next two rows 3-4 present scores of these pre-trained models with POS tags predicted by NLP4J-POS. Both pre-trained NNdep and Biaffine models were trained on a dependency treebank of 40K sentences, which was converted from the English PTB sections 2\u201321. The fifth row shows scores of BLLIP+Bio, the BLLIP reranking constituent parser [42] with an improved self-trained biomedical parsing model [10]. We use the Stanford conversion toolkit (v3.5.1) to generate dependency trees with the basic Stanford dependencies and use the data split on GENIA as used in [10], therefore parsing scores are comparable. The remaining rows show results of our retrained dependency parsing models.On GENIA, among pre-trained models, BLLIP obtains highest results. This model, unlike the other pre-trained models, was trained using GENIA, so this result is unsurprising. The pre-trained Stanford-Biaffine (v1) model produces lower scores than the pre-trained Stanford-NNdep model on GENIA. It is also unsurprising because the pre-trained Stanford-Biaffine utilizes pre-trained word vectors which were learned from newswire corpora. Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags (i.e. the pre-trained Stanford tagger at 98.37% vs. the retrained NLP4J-POS model at 98.80%).Regarding the retrained parsing models, on both GENIA and CRAFT, Stanford-Biaffine achieves the highest parsing results with LAS at 91.23% and UAS at 92.64% on GENIA, and LAS at 90.77% and UAS at 92.67% on CRAFT, computed without punctuations. Stanford-NNdep obtains the lowest scores; about 3.5% and 5% absolute lower than Stanford-Biaffine on GENIA and CRAFT, respectively. jPTDP is ranked second, obtaining about 1% and 2% lower scores than Stanford-Biaffine and 1.5% and 1% higher scores (without punctuation) than NLP4J-dep on GENIA and CRAFT, respectively. Table\u00a04 also shows that the best parsing model Stanford-Biaffine obtains about 1% absolute improvement when using gold POS tags instead of predicted POS tags.Here we present a detailed analysis of the parsing results obtained by the retrained models with predicted POS tags. For simplicity, the following more detailed analyses report LAS scores, computed without punctuation. Using UAS scores or computing with punctuation does not reveal any additional information.Those relation types with the lowest LAS scores (generally <70%) are dep on GENIA and DEP, LOC, PRN and TMP on CRAFT; dep/DEP are very general labels while LOC, PRN and TMP are among the least frequent labels. Those types also associate to the biggest variation of obtained accuracy across parsers (>8%). In addition, the coordination-related labels cc, conj/CONJ and COORD show large variation across parsers. These 9 mentioned relation labels generally correspond to long dependencies. Therefore, it is not surprising that BiLSTM-based models Stanford-Biaffine and jPTDP can produce much higher accuracies on these labels than non-LSTM models NLP4J-dep and NNdep.The remaining types are either relatively rare labels (e.g. appos, num and AMOD) or more frequent labels but with a varied distribution of dependency distances (e.g. advmod, nn, and ADV).On the remaining POS tags, we generally find similar patterns across parsers and corpora, except for IN and VB where parsers produce 8+% higher scores for IN on GENIA than on CRAFT, and vice versa producing 9+% lower scores for VB on GENIA. This is because on GENIA, IN is mostly coupled with the dependency label prep at a rate of 90% (thus their corresponding LAS scores in tables 8 and 6 are consistent), while on CRAFT IN is coupled to a more varied distribution of dependency labels such as ADV with a rate at 20%, LOC at 14%, NMOD at 40% and TMP at 5%. Regarding VB, on CRAFT it usually associates to a short dependency distance of 1 word (i.e. head and dependent words are next to each other) with a rate at 80%, and to a distance of 2 words at 15%, while on GENIA it associates with longer dependency distances with a rate at 17% for the distance of 1 word, 31% for the distance of 2 words and 34% for a distance of >5 words. So, parsers obtain much higher scores for VB on CRAFT than on GENIA.The second error type occurs on noun phrases such as \u201cthe Oct-1-responsive octamer sequence ATGCAAAT\u201d (in Table\u00a09) and \u201cthe herpes simplex virus Oct-1 coregulator VP16\u201d, commonly referred to as appositive structures, where the second to last noun (i.e. \u201csequence\u201d and \u201ccoregulator\u201d) is considered to be the phrasal head, rather than the last noun. However, such phrases are relatively rare and all parsers predict the last noun as the head.The third error type is related to the relation labels dep/DEP. We manually re-annotate every case where all parsers agree on the dependency label for a dependency arc with the same dependency label, where this label disagrees with the gold label dep/DEP (these cases are about 3.5% of the parsing errors intersected across all parsers on GENIA and 0.5% on CRAFT). Based on this manual review, we find that about 80% of these cases appear to be labelled correctly, despite not agreeing with the gold standard. In other words, the gold standard appears to be in error in these cases. This result is not completely unexpected because when converting from constituent treebanks to dependency treebanks, the general dependency label dep/DEP is usually assigned due to limitations in the automatic conversion toolkit.We present an extrinsic evaluation of the four dependency parsers for the downstream task of biomedical event extraction.Previously, Miwa et al. [45] adopted the BioNLP 2009 shared task on biomedical event extraction [46] to compare the task-oriented performance of six \u201cpre-trained\u201d parsers with 3 different types of dependency representations. However, their evaluation setup requires use of a currently unavailable event extraction system. Fortunately, the extrinsic parser evaluation (EPE 2017) shared task aimed to evaluate different dependency representations by comparing their performance on downstream tasks [47], including a biomedical event extraction task [8]. We thus follow the experimental setup used there; employing the Turku Event Extraction System (TEES, [48]) to assess the impact of parser differences on biomedical relation extraction13.EPE 2017 uses the BioNLP 2009 shared task dataset [46], which was derived from the GENIA treebank corpus (800, 150 and 260 abstract files used for BioNLP 2009 training, development and test, respectively)14. We only need to provide dependency parses of raw texts using the pre-processed tokenized and sentence-segmented data provided by the EPE 2017 shared task. For the Stanford-Biaffine, NLP4J-dep and Stanford-NNdep parsers that require predicted POS tags, we use the retrained NLP4J-POS model to generate POS tags. We then produce parses using retrained dependency parsing models.TEES is then trained for the BioNLP 2009 Task 1 using the training data, and is evaluated on the development data (gold event annotations are only available to public for training and development sets). To obtain test set performance, we use an online evaluation system. The online evaluation system for the BioNLP 2009 shared task is currently not available. Therefore, we employ the online evaluation system for the BioNLP 2011 shared task [49] with the \u201cabstracts only\u201d option15. The score is reported using the approximate span & recursive evaluation strategy [46].Table\u00a011 compares parsers with respect to the EPE 2017 biomedical event extraction task [8]. The first row presents the score of the Stanford&Paris team [50]; the highest official score obtained on the test set. Their system used the Stanford-Biaffine parser (v2) trained on a dataset combining PTB, Brown corpus, and GENIA treebank data16. The second row presents our score for the pre-trained BLLIP+Bio model; remaining rows show scores using re-trained parsing models.The results for parsers trained with the GENIA treebank (Rows 1-6, Table\u00a011) are generally higher than for parsers trained on CRAFT. This is logical because the BioNLP 2009 shared task dataset was a subset of the GENIA corpus. However, we find that the differences in intrinsic parsing results as presented in Tables\u00a04 and 10 do not consistently explain the differences in extrinsic biomedical event extraction performance, extending preliminary related observations in prior work [51, 52]. Among the four dependency parsers trained on GENIA, Stanford-Biaffine, jPTDP and NLP4J-dep produce similar event extraction scores on the development set, while on the the test set jPTDP and NLP4J-dep obtain the lowest and highest scores, respectively.Table\u00a011 also summarizes the results with the dependency structures only (i.e. results without dependency relation labels; replacing all predicted dependency labels by \u201cUNK\u201d before training TEES). In most cases, compared to using dependency labels, event extraction scores drop on the development set (except NLP4J-dep trained on CRAFT), while they increase on the test set (except NLP4J-dep trained on GENIA and Stanford-NNdep trained on CRAFT). Without dependency labels, better event extraction scores on the development set corresponds to better scores on the test set. In addition, the differences in these event extraction scores without dependency labels are more consistent with the parsing performance differences than the scores with dependency labels.These findings show that variations in dependency representations strongly affect event extraction performance. Some (predicted) dependency labels are likely to be particularly useful for extracting events, while others hurt performance. Also, investigating \u223c20 frequent dependency labels in each dataset as well as some possible combinations between them could lead to an enormous number of additional experiments. We believe a detailed analysis of the interaction between those labels in a downstream application task deserves another research paper with a more careful analysis. Here, one contribution of our paper could be seen to be that we highlight the need for further research in this direction.We have presented a detailed empirical study comparing SOTA traditional feature-based and neural network-based models for POS tagging and dependency parsing in the biomedical context. In general, the neural models outperform the feature-based models on two benchmark biomedical corpora GENIA and CRAFT. In particular, BiLSTM-CRF-based models with character-level word embeddings produce highest POS tagging accuracies which are slightly better than NLP4J-POS, while the Stanford-Biaffine parsing model obtains significantly better result than other parsing models.We also investigate the influence of parser selection for a biomedical event extraction downstream task, and show that better intrinsic parsing performance does not always imply better extrinsic event extraction performance. Whether this pattern holds for other information extraction tasks is left as future work.\nhttps://www.ncbi.nlm.nih.gov/pubmed\n\nhttps://www.ncbi.nlm.nih.gov/pmc\n\nhttps://nlp.stanford.edu/~mcclosky/biomedical.html\n\nhttp://bionlp-corpora.sourceforge.net/CRAFT\n\nhttp://cistern.cis.lmu.de/marmot\n\nhttps://emorynlp.github.io/nlp4j/components/part-of-speech-tagging.html\n\nhttps://github.com/UKPLab/emnlp2017-bilstm-cnn-crf\n\nhttps://nlp.stanford.edu/software/nndep.shtml\n\nhttps://emorynlp.github.io/nlp4j/components/dependency-parsing.html\n\nhttps://github.com/datquocnguyen/jPTDP\n\nhttps://github.com/tdozat/Parser-v2\n\nhttps://github.com/jbjorne/TEES/wiki/EPE-2017\n678 of 800 training, 132 of 150 development and 248 of 260 test files are included in the GENIA treebank training set.\nhttp://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi\nThe EPE 2017 shared task [47] focused on evaluating different dependency representations in downstream tasks, not on comparing different parsers. Therefore each participating team employed only one parser, either a dependency graph or tree parser. Only the Stanford&Paris team [50] employ GENIA data, obtaining the highest biomedical event extraction score.Bidirectional LSTMConvolutional neural networkConditional random fieldExtrinsic parser evaluationLabeled attachment scoreLong short-term memoryNatural language processingOut-of-vocabularyPart-of-speechPenn treebankState-of-the-artUnlabeled attachment scoreWall street journalThis work was supported by the ARC Discovery Project DP150101550 and ARC Linkage Project LP160101469.We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.In this section, we present our empirical approach to evaluate different POS tagging and dependency parsing models on benchmark biomedical corpora. Fig.\u00a01 illustrates our experimental flow. In particular, we compare pre-trained and retrained POS taggers, and investigate the effect of these pre-trained and retrained taggers in pre-trained parsing models (in the first five rows of Table\u00a04). We then compare the performance of retrained parsing models to the pre-trained ones (in the last ten rows of Table\u00a04). Finally, we investigate the influence of pre-trained and retrained parsing models in the biomedical event extraction task (in Table\u00a011).\n\n\nFig. 1\nDiagram outlining the design of experiments\nTable\u00a01 gives an overview of the experimental datasets, while Table\u00a02 details corpus statistics. We also include out-of-vocabulary (OOV) rate in Table\u00a01. OOV rate is relevant because if a word has not been observed in the training data at all, the tagger/parser is limited to using contextual clues to resolve the label (i.e. it has observed no prior usage of the word during training and hence has no experience with the word to draw on).\nTable 1\nThe number of files (#file), sentences (#sent), word tokens (#token) and out-of-vocabulary (OOV) percentage in each experimental dataset\n\nTable 2\nStatistics by the most frequent dependency and overlapped POS labels, sentence length (i.e. number of words in the sentence) and relative dependency distances i\u2212j from a dependent wi to its head wj\n\nIn addition, % G and % C denote the occurrence proportions in GENIA and CRAFT, respectively\nWe compare SOTA feature-based and neural network-based models for POS tagging over both GENIA and CRAFT. We consider the following: \n\nMarMoT [14] is a well-known generic CRF framework as well as a leading POS and morphological tagger5.\n\nNLP4J\u2019s POS tagging model [15] (NLP4J-POS) is a dynamic feature induction model that automatically optimizes feature combinations6. NLP4J is the successor of ClearNLP.\n\nBiLSTM-CRF [16] is a sequence labeling model which extends a standard BiLSTM neural network [17, 18] with a CRF layer [19].\n\nBiLSTM-CRF+CNN-char extends the model BiLSTM-CRF with character-level word embeddings. For each word token, its character-level word embedding is derived by applying a CNN to the word\u2019s character sequence [20].\n\nBiLSTM-CRF+LSTM-char also extends the BiLSTM-CRF model with character-level word embeddings, which are derived by applying a BiLSTM to each word\u2019s character sequence [21].\nOur second study assesses the performance of SOTA dependency parsers, as well as commonly used parsers, on biomedical texts. Prior work on the CRAFT treebank identified the domain-retrained ClearParser [24], now part of the NLP4J toolkit [25], as a top-performing system for dependency parsing over that data. It remains the best performing non-neural model for dependency parsing. In particular, we compare the following parsers: \n\nThe Stanford neural network dependency parser [6] (Stanford-NNdep) is a greedy transition-based parsing model which concatenates word, POS tag and arc label embeddings into a single vector, and then feeds this vector into a multi-layer perceptron with one hidden layer for transition classification8.\n\nNLP4J\u2019s dependency parsing model [26] (NLP4J-dep) is a transition-based parser with a selectional branching method that uses confidence estimates to decide when employing a beam9.\n\njPTDP v1 [27] is a joint model for POS tagging and dependency parsing,10 which uses BiLSTMs to learn feature representations shared between POS tagging and dependency parsing. jPTDP can be viewed as an extension of the graph-based dependency parser bmstparser [28], replacing POS tag embeddings with LSTM-based character-level word embeddings. For jPTDP, we train with gold standard POS tags.\n\nThe Stanford \u201cBiaffine\u201d parser v1 [29] extends bmstparser with biaffine classifiers to predict dependency arcs and labels, obtaining the highest parsing result to date on the benchmark English PTB. The Stanford Biaffine parser v2 [30], further extends v1 with LSTM-based character-level word embeddings, obtaining the highest result (i.e., 1st place) at the CoNLL 2017 shared task on multilingual dependency parsing [31]. We use the Stanford Biaffine parser v2 in our experiments11.\nFigure\u00a02 presents LAS scores by sentence length in bins of length 10. As expected, all parsers produce better results for shorter sentences on both corpora; longer sentences are likely to have longer dependencies which are typically harder to predict precisely. Scores drop by about 10% for sentences longer than 50 words, relative to short sentences <=10 words. Exceptionally, on GENIA we find lower scores for the shortest sentences than for the sentences from 11 to 20 words. This is probably because abstracts tend not to contain short sentences: (i) as shown in Table\u00a02, the proportion of sentences in the first bin is very low at 3.5% on GENIA (cf. 17.8% on CRAFT), and (ii) sentences in the first bin on GENIA are relatively long, with an average length of 9 words (cf. 5 words in CRAFT).\n\n\nFig. 2\nLAS scores by sentence length. Scores obtained on GENIA and CRAFT are presented in the left and right figures, respectively\nFigure\u00a03 shows LAS (F1) scores corresponding to the dependency distance i\u2212j, between a dependent wi and its head wj, where i and j are consecutive indices of words in a sentence. Short dependencies are often modifiers of nouns such as determiners or adjectives or pronouns modifying their direct neighbors, while longer dependencies typically represent modifiers of the root or the main verb [43]. All parsers obtain higher scores for left dependencies than for right dependencies. This is not completely unexpected as English is strongly head-initial. In addition, the gaps between LSTM-based models (i.e. Stanford-Biaffine and jPTDP) and non-LSTM models (i.e. NLP4J-dep and Stanford-NNdep) are larger for the long dependencies than for the shorter ones, as LSTM architectures can preserve long range information [44].\n\n\nFig. 3\nLAS (F1) scores by dependency distance. Scores obtained on GENIA and CRAFT are presented in the left and right figures, respectively\nOn both corpora, higher scores are also associated with shorter distances. There is one surprising exception: on GENIA, in distance bins of \u22124, \u22125 and <\u22125, Stanford-Biaffine and jPTDP obtain higher scores for longer distances. This may result from the structural characteristics of sentences in the GENIA corpus. Table\u00a05 details the scores of Stanford-Biaffine in terms of the most frequent dependency labels in these left-most dependency bins. We find amod and nn are the two most difficult to predict dependency relations (the same finding applied to jPTDP). They appear much more frequently in the bins \u22124 and \u22125 than in bin <\u22125, explaining the higher overall score for bin <\u22125.\nTable 5\nLAS (F1) scores of Stanford-Biaffine on GENIA, by frequent dependency labels in the left dependencies\n\n\u201cProp.\u201d denotes the occurrence proportion in each distance bin\nTables\u00a06 and 7 present LAS scores for the most frequent dependency relation types on GENIA and CRAFT, respectively. In most cases, Stanford-Biaffine obtains the highest score for each relation type on both corpora with the following exceptions: on GENIA, jPTDP gets the highest results to aux, dep and nn (as well as nsubjpass), while NLP4J-dep and NNdep obtain the highest scores for auxpass and num, respectively. On GENIA the labels associated with the highest average LAS scores (generally >90%) are amod, aux, auxpass, det, dobj, mark, nsubj, nsubjpass, pobj and root whereas on CRAFT they are NMOD, OBJ, PMOD, PRD, ROOT, SBJ, SUB and VC. These labels either correspond to short dependencies (e.g. aux, auxpass and VC), have strong lexical indications (e.g. det, pobj and PMOD), or occur very often (e.g. amod, subj, NMOD and SBJ).\nTable 6\nLAS by the basic Stanford dependency labels on GENIA\n\n\u201cAvg.\u201d denotes the averaged score of the four dependency parsers\n\nTable 7\nLAS by the CoNLL 2008 dependency labels on CRAFT\nTable\u00a08 analyzes the LAS scores by the most frequent POS tags (across two corpora) of the dependent. Stanford-Biaffine achieves the highest scores on all these tags except TO where the traditional feature-based model NLP4J-dep obtains the highest score (TO is relatively rare tag in GENIA and is the least frequent tag in CRAFT among tags listed in Table\u00a08). Among listed tags VBG is the least and second least frequent one in GENIA and CRAFT, respectively, and generally associates to longer dependency distances. So, it is reasonable that the lowest scores we obtain on both corpora are accounted for by VBG. The coordinating conjunction tag CC also often corresponds to long dependencies, thus resulting in biggest ranges across parsers on both GENIA and CRAFT. The results for CC are consistent with the results obtained for the dependency labels cc in Table\u00a06 and COORD in Table\u00a07 because they are coupled to each other.\nTable 8\nLAS by POS tag of the dependent\nWe analyze token-level parsing errors that occur consistently across all parsers (i.e. the intersection set of errors), and find that there are few common error patterns. The first one is related to incorrect POS tag prediction (8% of the intersected parsing errors on GENIA and 12% on CRAFT are coupled with incorrect predicted POS tags). For example, the word token \u201cdomains\u201d is the head of the phrase \u201cboth the POU(S) and POU(H) domains\u201d in Table\u00a09. We also have two OOV word tokens \u201cPOU(S)\u201d and \u201cPOU(H)\u201d which abbreviate \u201cPOU-specific\u201d and \u201cPOU homeodomain\u201d, respectively. NLP4J-POS (as well as all other POS taggers) produced an incorrect tag of NN rather than adjective (JJ) for \u201cPOU(S)\u201d. As \u201cPOU(S)\u201d is predicted to be a noun, all parsers make an incorrect prediction that it is the phrasal head, thus also resulting in errors to remaining dependent words in the phrase.\nTable 9\nError examples\n\n\u201cH.\u201d denotes the head index of the current word\nTable\u00a010 presents the intrinsic UAS and LAS (F1) scores on the pre-processed segmented BioNLP 2009 development sentences (i.e. scores with respect to predicted segmentation), for which these sentences contain event interactions. These scores are higher than those presented in Table\u00a04 because most part of the BioNLP 2009 dataset is extracted from the GENIA treebank training set. Although gold event annotations in the BioNLP 2009 test set are not available to public, it is likely that we would obtain the similar intrinsic UAS and LAS scores on the pre-processed segmented test sentences containing event interactions.\nTable 10\nUAS and LAS (F1) scores of re-trained models on the pre-segmented BioNLP-2009 development sentences which contain event interactions\n\nScores are computed on all tokens using the evaluation script from the CoNLL 2017 shared task [31]\n\nTable 11\nBiomedical event extraction results\n\nThe subscripts denote results for which TEES is trained without the dependency labels\nTrained on the PTB sections 0\u201318, the accuracies for the GENIA tagger, Stanford tagger, MarMoT, NLP4J-POS, BiLSTM-CRF and BiLSTM-CRF+CNN-char on the benchmark test set of PTB sections 22-24 were reported at 97.05%, 97.23%, 97.28%, 97.64%, 97.45% and 97.55%, respectively.\nTable 3\nPOS tagging accuracies on the test set with gold tokenization\n\n[ \u22c6] denotes a result with a pre-trained POS tagger. We do not provide accuracy results of the pre-trained POS taggers on CRAFT because CRAFT uses an extended PTB POS tag set (i.e. there are POS tags in CRAFT that are not defined in the original PTB POS tag set). Corpus-level accuracy differences of at least 0.17% in GENIA and 0.26% in CRAFT between two POS tagging models are significant at p\u22640.05. Here, we compute sentence-level accuracies, then use paired t-test to measure the significance level\n\nTable 4\nParsing results on the test set with predicted POS tags and gold tokenization (except [\\(\\mathcal {G}\\)] which denotes results when employing gold POS tags in both training and testing phases)\n\n\u201cWithout punctuation\u201d refers to results excluding punctuation and other symbols from evaluation. \u201cExact match\u201d denotes the percentage of sentences whose predicted trees are entirely correct [25]. [ \u2219] denotes the use of the pre-trained Stanford tagger for predicting POS tags on test set, instead of using the retrained NLP4J-POS model. Score differences between the \u201cretrained\u201d parsers on both corpora are significant at p\u22640.001 using McNemar\u2019s test (except UAS scores obtained by Stanford-Biaffine-v2 for gold and predicted POS tags on GENIA, i.e. 92.51 vs. 92.31 and 92.84 vs. 92.64, where p\u22640.05)", "s12859-018-2406-9": "Advances in medical domain has led to an increase of clinical data production which offers enhancement opportunities for clinical research sector. In this paper, we propose to expand the scope of Electronic Medical Records in the University Malaya Medical Center (UMMC) using different techniques in establishing interoperability functions between multiple clinical departments involving diagnosis, screening and treatment of breast cancer and building automatic systems for clinical audits as well as for potential data mining to enhance clinical breast cancer research in the future.Quality Implementation Framework (QIF) was adopted to develop the breast cancer module as part of the in-house EMR system used at UMMC, called i-Pesakit\u00a9. The completion of the i-Pesakit\u00a9 Breast Cancer Module requires management of clinical data electronically, integration of clinical data from multiple internal clinical departments towards setting up of a research focused patient data governance model. The 14 QIF steps were performed in four main phases involved in this study which are (i) initial considerations regarding host setting, (ii) creating structure for implementation, (iii) ongoing structure once implementation begins, and (iv) improving future applications. The architectural framework of the module incorporates both clinical and research needs that comply to the Personal Data Protection Act.The completion of the UMMC i-Pesakit\u00a9 Breast Cancer Module required populating EMR including management of clinical data access, establishing information technology and research focused governance model and integrating clinical data from multiple internal clinical departments. This multidisciplinary collaboration has enhanced the quality of data capture in clinical service, benefited hospital data monitoring, quality assurance, audit reporting and research data management, as well as a framework for implementing a responsive EMR for a clinical and research organization in a typical middle-income country setting. Future applications include establishing integration with external organization such as the National Registration Department for mortality data, reporting of institutional data for national cancer registry as well as data mining for clinical research. We believe that integration of multiple clinical visit data sources provides a more comprehensive, accurate and real-time update of clinical data to be used for epidemiological studies and audits.In 2012, Globocan reported that Malaysia had the highest breast cancer mortality in the Southeast Asian Region based on estimation from neighboring countries and regional registries in Malaysia [1]. University Malaya Medical Centre (UMMC) Surgical Breast Unit has produced the first breast cancer outcomes data in Malaysia [2\u20134]. The institutional survival rates differ tremendously with a published population based study but further details on stage at presentation and other clinical variables were not available for nationwide outcome analysis [5]. In Malaysia, data capture methods had been manual and done retrospectively by tracing notes of patients\u2019 clinical characteristics and treatment characteristics. This method is expensive with high probability of missing values and inaccuracies. Reducing manual work by automated data capture systems has been cost effective especially in the light of increasing burden of salary costs to hospitals. In a typical clinical set up, these primary data are used for surgical audits in measuring the hospital performance, while the secondary use data will be used in epidemiological analysis in breast cancer outcome research.A typical breast cancer patient\u2019s journey through diagnosis and treatment involves multiple disciplines and departments. Breast cancer diagnostics require input by surgical, radiological and pathological disciplines. In such circumstances, efficient data management and computational workflows are needed to generate meaningful clinical data, rather than having textual data and building algorithms to mine retrospective data. With the increasing use of EMR data in research, EMR has high potential in becoming a major data source for future medical research and clinical service evaluation of a practice [6\u20138]. The rapid increase in quantity of clinical information in electronic format makes secondary use of clinical data a candidate for big data solutions [9, 10]. The availability of data extraction techniques in the data repository opens up more avenues in addressing research questions [6, 11, 12]. Prospectively managed data would provide clean data and more accurate data leveraging the power of Artificial Intelligence to detect and uncover clinical relationships and knowledge [13]. Aggregating data from different sources in healthcare and research is important [14] to discover hidden knowledge from different sources in healthcare [15]. The effectiveness and data quality of records can be improved through the enhancement of the clinical research database features. Elements needed for a successful clinical research database include engagement of clinicians, utility for research and the ability to integrate with the legacy systems [16, 17].The revolutions caused by advanced computing power, advanced informatics and communication technology have changed the way clinical data are stored and used. Today almost every hospital realizes the need of storing its clinical data sets electronically in order to increase the quality of healthcare service and data. Many countries have embarked into the management of huge amount of clinical data using Electronic Medical Records systems. Examples are National Electronic Health Records (NEHR) in Singapore [18], National Programme for Information Technology (NPfIT) NHS Care Records Service in the United Kingdom [19], The Royal Children\u2019s Hospital Electronic Medical Record in Melbourne [20], Allscripts, eClinicalWorks [21], EPIC [22], McKesson, Care 360, Cerner, OPTUM Insight, NextGen, and Greenway in the USA [22\u201326]. Malaysia, although being in the forefront in providing one of the best medical care in the world, is still at a very immature stage concerning clinical data management. Electronic Medical Record (EMR) and Hospital Information Management System (HIMS) in Malaysia is still in the preliminary stage [27].In line with the Vision for Health Statement [28\u201332], Malaysia has initiated effort in providing a platform for the country\u2019s transition in promoting information technology usage in health sector during the Eighth Malaysia Plan (2000\u20132005) [28, 33]. The Ninth Malaysia Plan (2006\u20132010) [29] highlighted on strengthening the Health Information System, to improve the point-of-care service and patients\u2019 health information access. To facilitate this, a nationwide information system is introduced by focusing on enhancing digital information structure expansion. Tenth Malaysia Plan aimed to achieve integration and interoperability between various hospital information systems (HIS) [30, 34]. It has been an ongoing effort in improving the clinical data management system as being mentioned in The Eleventh Malaysia Plan [32], Malaysian Health Reference Data Model [35], and Malaysia Health Data Warehouse [36].During the implementation, it was found that there were inadequate integrated planning of HIS where different hospital uses individual stand-alone systems, lack of uniformity in operational policies of the hospitals and weak governance in securing the privacy of clinical data [31]. However till date, the success rate has been low. There is an absolute urgency in developing a reliable, integrated and interoperable Health Information Management using an implementation framework [37]. In this paper we present our Breast Cancer Module in the University Malaya Medical Center EMR which is developed using the Quality Implementation Framework (QIF) [38]. The QIF is an implementation framework used to introduce new services or workflows in a healthcare setting. This paper highlights the challenges faced especially in a developing country setting with limited resources and funds, along with the development of the system using an infrastructure that matches the hospital environment.At the initial stage, a group comprising a multidisciplinary team of expertise was formed to evaluate different areas of assessment, in terms of needs, resources and requirements. The breast cancer module is by itself a multidisciplinary clinical practice encompassing Surgery, Oncology, Radiology, Pathology and Pharmacy departments. The stakeholders and content experts are clinicians who are actively involved in day to day clinics from diagnosis to treatment and follow-ups, while the structure of system is designed by Bioinformaticians and developed by Information Technology (IT) experts. More importantly, the main stakeholders which are the Hospital Management Board, Ethics Committee and Patient Information Department responsible for policy making on clinical data in regards to patient confidentiality were engaged. Before venturing into designing a new model for the breast cancer clinical and research data reporting, an assessment was conducted to identify specific issues and concerns in the current practice at the hospital. We identified concerns on data privacy and confidentiality under the Malaysia Personal Data Protection Act (PDPA) 2010 [39]. Hence, several discussions to develop a system model and governance that is compliant to both the PDPA and research processes were done with these committees.Personal Data Protection Act 2010 (PDPA) compliance [39]; a set of regulations that provides data privacy and security provisions for protecting clinical information was discussed with the UMMC Medical Records Department which is bound by the Malaysia Health Care Act and the National Archive of Malaysia Act. Data usability for research was conducted through safe and secure use of technology to automate data transfer into the UMMC Clinical Research Knowledgebase via de-identification of primary patient records. This prototype fulfils the Health Level-7 standard (HL7), an international standard for data transfer of clinical and administrative information. The details of the prototype are illustrated in the Results section. In the case of using clinical data with identifiers, we had to obtain written permissions from the ethics committee for a given duration required for the job execution.The importance of a national cancer registry lies in the fact that they consolidate accurate and complete clinical cancer data as cancer control and epidemiological research, public health program planning, and patient care improvement. Ultimately, a complete national-level system of cancer registry can assist clinicians and researchers in understanding cancer better and maximize our resources to the best outcomes in treatment and prevention.The UMMC Breast Cancer Registry begun in 1993 with data amounting to over 6000 individual patient data. This single page proforma that was consolidated into a spreadsheet had essential data that enabled UMMC to be the first to publish breast cancer outcomes in Malaysia and had enabled collaboration internationally to establish outcomes in Southeast Asia and Asia [2\u20134]. A more complex UMMC Breast Cancer Registry Clinical Proforma was developed in 2009, which included details on diagnosis and treatments and other clinical characteristics involved in risk and prognosis of breast cancer patients. Data were collected manually through patients\u2019 visits through diagnosis and treatments prospectively and retrospectively from medical records. The work process was labor intensive and required training of non-medical personnel. Other manual workflow limitation includes the unavailability of keeping track of patients\u2019 status, including recurrence and survival status.UMMC Electronic Medical Records (EMR) i-Pesakit\u00a9 system was developed in January 2012 by the UMMC Department of Information Technology with seven main modules to cater to\u00a0patient management activities which include patient registration, outpatient, inpatient, emergency medicine visits, billing, folder tracking and reporting. The system has been operational since 1st July 2012. The system was further developed to cater to medical records requirements, which include clinical documents, orders and results. The expansion of UMMC EMR project started in September 2013 and was implemented as a pilot study in the staff health clinic a year later. The pilot project was extended to Primary Care Medicine clinic and the Breast Unit, and later Department of Surgery. The system is registered under the copyright act in July 2016 with Intellectual Property Corporation of Malaysia (MYIPO) and has already been commercialized. From time to time, various iterative improvements had been made to the system to be able to work as required by clinicians. To date 99% of UMMC clinical areas implement the i-Pesakit\u00a9 EMR.However, the current i-Pesakit\u00a9 system, only covers generic patient data which could only generate basic audit reports, which is not aligned to the bigger aspiration of data\u00a0mining for research outputs. Challenges in providing manual data transcription by salaried personnel provided avenues of building cost effective solutions for data management such as audits and for research. The primary objective was to collate accurate clinical data encompassing risk and prognostic variable and ensuring the ability to integrate with the legacy system. Hence, the assessment of institutional data management and users were aligned to build new solutions, tied up with the current hospital\u2019s EMR system which fits the environment and has been used for 6\u00a0years.Since EMR was implemented in 2012, the loss and misplacement of patient records and x-ray films, originally in physical paper folders were drastically alleviated. Ultimately, an ideal hospital information system should allow seamless connections and integration of other clinical departments to improve clinicians\u2019 work performance and produce positive healthcare institutional outcomes.Readiness for adaptation was evident as the department of surgery was slotted for complete breast cancer surgery department prototype module usage in 2016 which was designed and developed from scratch by the critical stakeholders.In filling out the research data management gaps within this research hospital, the status of EMR implementation process and responses of clinicians on its impact on their routine in patient care has been positive. This allows the establishment of ground work for next phase of breast cancer research module.In order to improve the efficiency of clinical data management system in i-Pesakit\u00a9, restructuring of information-capture process and upgrading the system flow through engagement with the doctors\u2019 clinical work processes are crucial. A mechanism to translate paper-based operations to digital data capture is introduced and entries are reflected in the hospital\u2019s EMR system, which further extends to clinical audits based on primary data obtained. Uptake of EMR use in Breast Unit was largely due to the age cohort of the users, which are mainly Master of Surgery surgical trainees and medical officers (25 to 35\u00a0years old) who are quick to adapt to migration of work routine to a digital clinical workflow.Due to poor retainment of staff in this public hospital, it is hence practical to move towards digital platforms. However, senior clinicians may perceive the system as hindrance to effective clinical work due to the inconvenience and concentration issues in multitasking between typing, paying attention to the computer screen rather than developing a rapport with the patients. The impact of the system implementation contributes to both service and clinical data quality, as well as job performance.Since the breast cancer workflow encompasses different departments in the hospital, it is ideal to design an interoperable system for sharing records between departments as well as structuring an availability of standards for integration of various clinical workflows.Engagement of critical stakeholders from these units were agreeable to a workflow of interoperable system that would be adaptable to practices in these units. The framework and development of i-Pesakit\u00a9 Breast Cancer Module, adopted from the QIF model is demonstrated in Fig.\u00a02.The source materials used in this study were obtained from the UMMC. Data sources include case report forms (CRF), manual legacy Excel sheets, in house EMR system used by the clinicians, clinical workflow requirements gathered from the users such as clinicians and nurses. The i-Pesakit\u00a9 Breast Cancer Module is based on the UMMC Breast Cancer Registry Clinical Proforma, which include details on diagnosis and treatments and other clinical characteristics involved in risk and prognosis of breast cancer patients.The flow of patient registration, diagnosis and treatment workflows was studied carefully. This was the key in making decisions on the development of the breast cancer module. Standardized data entry forms that are\u00a0compatible with the EMR system were created, by translating these\u00a0paper-format case report forms into an electronic case report form (e-CRF) before embedding the e-CRF design into EMR.A total of 42 case report forms (CRFs) were translated during the process of developing the breast cancer module; represented by 9 categories which are First Visit, Diagnostic Multidisciplinary Team (MDT) with Radiology, Results Clinic, Admission for Surgery, Treatment MDT with Oncology, Oncology Visits, Oncology Treatment Summary and Relapse.The EMR was implemented on MariaDB and open source systems for the system developmental work. Currently, the breast cancer module contains data sources from the Surgery, Oncology and Pharmacy departments. The fully automated EMR breast cancer module is being used in the clinical visits as well as for audits, while further enhancement will be made in integrating other related sources from Radiology and Pathology for a more compact system.The hospital management board, inclusive of hospital director, Patient Records Department, Hospital Informatics Department were among the crucial stakeholders with decision making power were engaged very early in the project. The vision for the hospital to apply the 4th Industrial Revolution [37] was very much aligned to this project. Hence, the support received to further this project from critical stakeholders was very important in the development process. The second step was through engagement of content experts, clinicians include surgeons, oncologists, radiologists, pathologists, and pharmacists, as key stakeholders in each of these departments. The engagement was done in stages, where surgical and oncology departments as well as e-prescription of chemotherapy with the pharmacy department were engaged for the pilot project.The collaboration between academics, graduate students and programmers was able to foster close relationships, sharing of tasks despite shortage of manpower within the service sector. The researchers played a role in providing detailed logs of changes and became the conduit between the user and the programmers. The greatest disincentive if we are not able to produce an automated system is challenges to salary personnel to continue manual data collection.Organizational capacity includes increasing more system designers and developers, and task sharing between academia, graduate students and project-based programmers. We discovered organizational policies with regards to developing IT solutions for handling of digital data in the confines of the PDPA needs improvement and proper policy and protocols in place to ensure smooth implementation.This includes processes of obtaining permission for students and researchers to work within the hospital departments where initial challenges were encountered and resolved when trust and clear boundaries were defined.The implementers of the system are the clinicians of UMMC, so training and ongoing support will be given to users to build their capacity in knowledge about the system. A breast care nurse is assigned to oversee this day-to-day system use in the clinic and holds the role as a middle person between the implementers and system designers to provide feedbacks about the system.A protocolled training session is carried out for new rotating medical officers in the unit every 3\u00a0months. This training will be conducted by the breast care nurses.In order to ensure the success of any implementation, the people involved need to have the right expertise and roles and secondly a viable plan ahead of the development. In this project, our team is multidisciplinary with distinguished roles who have been assigned with dedicated tasks and timelines. A summary of the team members with job scope is presented in Fig.\u00a05.There are five groups of crucial members in this EMR implementation; (i) project manager and critical stakeholders include (ii) hospital management and governance team, (iii) physician champions, (iv) system design and development, as well as the (v) evaluation and quality assurance teams. The project manager is the lead person in facilitating these implementation steps, connects different implementation phases and coordinate the planning, design, development, and testing phases between team members. The hospital management and governance team from the Patient Information Department provide feedback on governance and policy matters pertaining to data sharing, privacy and confidentiality. Physician champions have credibility with clinical staffs and hospital administration, to promote value of the innovation through stakeholders engagements. They are also the main point of reference from the clinical perspective, also as content experts and EMR functionalities so the digital workflow matches closely to the actual clinical workflow. The system designers (bioinformaticians) act as a liaison between physician champions and system development team (IT staffs) in connecting ideas and suitable concepts. Bioinformaticians design the digital system workflow, templates and structure through gathering EMR requirements from physician champions and put in technical form for system developers to take into development phase. IT staffs are responsible in building, customizing and deploying the breast cancer module, as well as providing maintenance service of the system to be conducted by the evaluation and quality assurance team (breast care nurse). Nurses conduct on-site system testing and performance review and coordinates training for users within the practice and system use.Workflow analysis is done in the planning stage where bioinformaticians study the existing clinical work processes, looking for opportunities for improved efficiency, assessing and designing new workflows and system structure and developing a transition plan towards a digital clinical workflow environment. Good communication is crucial between bioinformaticians and clinicians, in coming up with the best solution of improvised workflow that is time effective and user friendly for the frontline implementers.The IT experts are responsible in deploying and constructing the EMR system. Participation of clinical staff in the implementation process increases support for and acceptance of the EMR Breast Cancer Module implementation. In line with the hospital management support and participation of clinical and non-clinical staff, having an interdisciplinary implementation group involves direct stakeholders working together, where a better EMR system can be delivered faster and with less problems.We foresee difficulties in implementation and monitoring of the busy medical officers hence, qualified staff on-site to oversee and support implementation were played by breast care nurses. As a central role on the team, they understand these EMR clinical workflows, inspire clinical staff to embrace change, and drive consensus among other clinical staff. There is a close collaboration and feedback mechanism between implementers, supportive team (breast care nurses), physician champions, as well as EMR design and development team in fine tuning the system from time to time.Implementation plan involved designing the pilot system and going live with support and specific tasks starting with First Visit template (Fig.\u00a04) for all new cases, as progressively include follow-up for cancer patients. There is a mechanism in place to produce quality entries as medical officers are accountable to document EMR professionally, to ensure the clinical service as well as data are high quality.Progressively other templates were used, through the development of e-Prescription of chemotherapy from the Oncology department to Pharmacy department, as well as specific clinical templates for the departments of Radiology and Pathology.Eight months after the implementation process began, the prototype system went live on February 2016. Support for users is provided with a two-tiered approach. On-site support is available from the trained breast care nurses who understands the EMR workflow to oversee the system. If the problem still persists, an information technology services staff member is called for support by phone. This has allowed the majority of technical problems to be solved locally.In the first few months after implementation, occasional meetings between clinicians and bioinformaticians were called to address specific issues that arose. Implementing an EMR breast cancer module system is challenging. It requires good planning, strong physician leadership and supportive clinical and non-clinical staff. The most immediate benefits of the EMR breast cancer module system have been accurate diagnostics, treatment plans, legible notes and prescriptions, and lower transcription costs.The focus of the study was extended to integrating and enhancing the system interoperability according to clinicians-specific function requirements, support and maintenance (impact on technical architecture), as well as data availability and sharing amongst clinicians in providing meaningful representation of patient data electronically. In the testing stage, further enhancement effort was done to improve user friendliness, according to accurate clinical and nursing work flows.However, the current EMR system does not provide a strong basis for clinical research, as the data structure is scattered and not standardized. The mortality data from the National Registration Department will soon be linked to the EMR, which is useful in survivorship analysis research.A critical factor for successful utilization of available EMR clinical data for research is the access, management and analysis of integrated patient data, within and across different functional domains. For example, most clinical and basic research data are currently stored in disparate and separate systems, and it is often difficult for clinicians and researchers to access and share these data. Equally important is the assurance within EMR systems of security, with confidentiality, integrity and general trustworthiness to meet the requirements for high quality research data.In innovating a practical approach to develop a clinical research workflow and framework, the EMR System Mirroring was designed to provide an economical solution for rapid, reliable, robust, automatic failover between two database systems, making mirroring the ideal solution in minimizing redundant components and risk of human error transcriptions.The clinical research database use and workflow is in line with the Clinical Data Interchange Standards Consortium (CDISC) [40, 41] which supports the electronic acquisition, exchange, regulatory submission and subsequent archiving of clinical research data. Developing a new system for clinical research is not practical due to overhead cost of programming, requirements development, designing and infrastructure, hence EMR mirroring for the purpose of clinical research is the best and cost-effective solution. Successful case studies have proposed to automate data transfer from primary EMR models for clinical research [42\u201344] without using vendor specific third party clinical research databases. Hence, in this paper we propose to use the primary EMR to be mirrored and de-identified for research purposes. The mirroring could be done using a middleware to facilitate data transfer.Quality assurance mechanisms are needed to ensure that the EMR system adheres to certain quality characteristics. The governance framework and design structure of EMR fulfils the Malaysian Medical Council Confidentiality 2011 Guideline [45], which supports clinical data usage for research, clinical audit and secondary use.Training clinicians and nurses to effectively use the breast cancer module into their clinical workflow is an important step to a successful implementation in making sure data is entered in a standardized manner. Quality, safety, and integrity of data are protected, while it increases the efficiency of clinical care, especially through point of care (POC) adoption in the clinical setting. Ongoing training will be conducted from time to time when there are additional functionalities introduced to the system, including producing clinical audits and contribution to national statistics in measuring the hospital\u2019s performance.The challenging environment of staff shortage affects the time taken in updating the systems according to clinical needs. There is a need of programmers to resolve feedback quickly as not to lose momentum and affecting the digital clinical workflow. Training of medical officers by breast care nurses, briefing for each new staff to the team is important to ensure the EMR breast cancer module usage is maximized. The breast care nurses\u2019 expertise has knowledge on how daily operations work in a clinical setting, so testing and quality reviews can be performed for data security, proper functionality within the department, performance review, and to verify the system closely matches the actual clinical workflow.The common barriers in implementing this new system are users\u2019 resistance to use the digital template and wrongly using template usage. This is solved by creating an EMR workflow which matches the clinical workflow closely to ease the transition of manual to digital clinical workflow among clinicians.The information technology lead is responsible for deployment and operation of the software and hardware such as workstations, in providing IT support in servers and connection issues. Constant change and improvement of system is conducted from time to time in improving the system usability and performance.Initial adaptation of the breast cancer surgery department module was also tested with the users in the department. The pilot system has gone live since February 2016, and a usability testing survey was conducted to test the readiness of EMR adaptation in the breast cancer workflow. The system test evaluation survey material [46] was adapted from Evaluation of Electronic Medical Records Questionnaire [47].During the first year of i-Pesakit\u00a9 Breast Cancer Module implementation in the Breast Unit of Surgical Department, a system usability survey was conducted to measure clinician\u2019s perceptions of the recently implemented system in determining their satisfaction level of using the system in their daily clinical routines (Fig.\u00a06a, b, c).Positive feedbacks were given by clinicians, where 98.34% of them use the system frequently in their clinical work. The effect of using the breast cancer module system has made their workflow smoother as well as easier, while 7.58% of them experience no change in convenience and 4.55% find the system complicated. However, 83.33% of clinicians agree that the breast cancer module is worth the time and effort to be used. More user training is required, to familiarize them with the system in order to achieve maximum benefit of the system, especially in reducing time taken in documenting patients\u2019 clinical data.From time to time adjustments are made by the programmers based on feedbacks given by users. However, in order to expedite the process of correcting bugs or errors, more programmers can be recruited, or students can be engaged in the development team.In getting rapid and accurate feedbacks from first-hand users, there are three main mechanisms of communication to provide an on-going technical assistance. The first mechanism is via public talks at the hospital under the E-health initiative [48] started by the hospital. Second approach is by engaging faculty members and hospital EMR committee which includes the IT management team in the hospital and finally frequent communication with the breast care nurses who provide direct feedbacks from doctors who use the system on-site. These methods will create understanding among involved parties on how the implementation process is progressing, as well as recognizing strategies to improve the system.Through this exercise, we have a design plan which is generic to be implemented in other departments in the hospital. It is good that the foundation used in the Breast Cancer Module is the hospital\u2019s existing EMR system, hence we can reuse our upper layer design workflow to match the requirements in the other departments such as Radiology and Pathology. Continuous efforts are under way in maintaining and improving the Surgery, Oncology and Pharmacy modules using the feedback form provided to end users.The model (Fig.\u00a02) derived from the experience of design and implementation of the module has taught the importance of incorporating a platform for research that has access to both confidential data and editing capabilities, as working on cancer would need identifiers and communication with other bodies.As aforementioned, the model is in line with the standards laid out by the Clinical Data Interchange Standards Consortium [49] The CDISC standard has also been applied in prominent research on EMR [40, 41, 50].The deployment of EMR in hospitals enables electronic reporting and fosters research by establishing affiliations with other institutions and potential external collaborations. In this paper, the i-Pesakit\u00a9 Breast Cancer Module was developed following the steps in the QIF implementation framework which is a conceptual overview of implementing a healthcare innovation. Phase 1 of the QIF confirmed the need to engage clinicians and real-time workflows. Stakeholder involvement is critical to the success of these implementation efforts. Prior work on implementation of clinical information systems provides broad guidance to inform effective engagement strategies. Phase 2 focuses on the structural features on implementation. In this phase, understanding the multidisciplinary breast cancer clinical workflow is crucial in designing a practical system for users, as well as system enhancement during the testing phase. Phase 3 deals with the support strategies during implementation.In addition to that, direct engagement with stakeholder is carried out from time to time, in monitoring the system efficiency and receiving feedbacks from users to improve the system\u2019s functionalities. Finally in Phase 4, suggestions are made to improve future application through retrospective system analysis based on feedbacks and suggestions by users. The implementation framework using the QIF framework provides the team a clear path to developing a breast cancer reporting system which incorporates both clinical and research needs for clinical and research organization readiness.By adopting the QIF framework, we proposed three distinct phases in designing the innovation, which are (i) engagement of stakeholders and following the clinical workflows very closely to ensure usability and accuracy of data captured; (ii) compliance to Personal Data Protection Act (PDPA) and research needs and (iii) de identification of clinical data for research.The UMMC i-Pesakit\u00a9 Breast Cancer Module was tested rigorously by the clinicians within the department of surgery. As presented in the Results, in general, clinicians were in favor of the i-Pesakit\u00a9 Breast Cancer Module system implementation. They believed that the system has improved the quality and clarity of documentation. However, some clinicians described the system as complex and too complicated. They had difficulty at the beginning especially during the familiarization of system flow. While some feel the system, features have increased work efficiency, they also experience some drawbacks such as the point of care data entry resulted in lack of time for eye contact and less clinician-patient communication. This may reduce the effectiveness to create a therapeutic relationship with the patients as clinicians are more focused on the notes in the system as compared to patient consultation. Point of care data entry may not be enough for completeness of data for clinical research [51\u201353] hence audits to assess completeness of data need to be done. Hiring data managers to obtain complete the data through active engagement with patient and clinicians could be a possible solution Auditing the quality of data is also necessary in maintaining the clinical data integrity in ensuring a certain level of quality [54].The proposal in this paper is in line with global efforts in digitizing data and clinical workflows [42, 43, 55\u201357], by paying close attention towards the challenges faced in a middle income country setting.Many efforts attempting to improve the dire situation of poor records management are being carried out in Malaysia, however, as a developing country, there are some challenges faced which hinders the success. One of the main challenge is lack of funding in supporting medical research costs. In Malaysia, technology is available and has been incorporated into private sectors such as banking and commerce at an accelerated pace. However, as a middle-income country, funding in medical and healthcare focusing on research-based activities is restrained. According to the Malaysian National Budget 2017, budget allocated for medical research is barely USD358m [58], which is less than 10% as compared to other developed countries such as USD380m in Singapore and USD32.3b in the USA [59, 60].Another challenge faced is the issue of data confidentiality [39]. A governance framework needs to be established to comply with the Personal Data Protection Act 2010 whereby the framework must provide mechanism for data protection of personal information by ensuring secured data sharing in clinical work and research. This includes assuring the system meets the security standards, privacy protection and infrastructure readiness.Workflow factors that contribute to patient data privacy in accordance to the Personal Data Protection Act 2010 [39] and the National Archives Act 2003 [61] were incorporated early during the planning process to ensure sustainability and compliance through direct engagement with the Medical Records Department. Models of governance is managed by Clinical Investigating Centre (CIC), a committee within the UMMC hospital to address issues brought about by projects like this which is a point of integrated network between clinical workflow and research. The roles of CIC go beyond liaising with Medical Ethics Committee to ensure safe clinical workflow and ethical standards are met. Future work will also include a research module (i-Research) where primary clinical data will be de-identified and used for research-based activities. We propose mirroring of i-Pesakit\u00a9 to produce the i-Research module as presented in Fig.\u00a03. The i-Research database module provides a carefully controlled research environment for clinicians and scientists to conduct safe and high quality clinical research through the compliance of the Malaysian ICH Good Clinical Practice and is acceptable by the international regulatory authorities. The ability to produce reports on breast cancer outcomes in Malaysia to be used by stakeholders such as clinicians, researchers, and the government is essential for research, hospital performance for policymakers to track outcomes and provide direction in cancer control. This is also a time effective approach in producing new knowledge through systematic data capture design, data mining and analysis, enhancing research and development in the medical and health sciences domain. Future work in incorporating other disciplines into the i-Pesakit\u00a9 Breast Cancer Module will allow other data sources from oncology, radiology, pathology and pharmacy to be integrated to improve the completion and accuracy of the data reported into the existing audit and research forms. Outcome research in oncology requires accurate clinical and follow-up data. In view of future developments in bioinformatics, research organization readiness to produce accurate clinical and follow-up data is crucial.The highlight of the i-Pesakit\u00a9 Breast Cancer Module presented in this paper is that it was developed in-house with close supervision by clinical and research experts in the hospital using electronic forms and embedded within the EMR system as the opportunity presented during the transitioning between paper based to EMR in UMMC. This multidisciplinary collaboration between Surgery and Oncology departments enhanced clinical workflows and POC data capture, impacts clinical decision making for clinicians, for hospital performance audits and research use. The completion of the UMMC i-Pesakit\u00a9 Breast Cancer Module would require consolidating multidisciplinary data from various clinical departments. This paper documents the processes and provides a framework for practitioners in the developing world embarking on such endeavor. This framework will potentially guide healthcare entities to prepare for future clinical bioinformatics as technology application in health is still in its infancy in the developing world. Understanding the approach in secondary use of clinical data in the research domain is crucial in data mining in delivering data for meaningful oncology outcomes.Data mining, building analytical modules and machine learning techniques in prediction works, especially in breast cancer recurrence and survivorship studies are important as they guide national cancer control policy. Direct linkages with other bodies like the National Registry Department and able to report the backend directly to the Malaysian National Cancer Registry will be sought to reduce needed human resources and produce more accurate form of reporting. The accomplishment of the breast cancer module within the EMR will bridge the gap between clinical care and medical informatics research in the future. With additional UMMC biobank facilities and research questions, infrastructure for future bioinformatics research can be conducted.Clinical Data Interchange Standards ConsortiumClinical Investigating CentreCase report formElectronic case report formElectronic medical recordHospital information systemHealth Level-7Information technologyMultidisciplinary teamIntellectual Property Corporation of MalaysiaNational Electronic Health RecordsNational Programme for Information TechnologyPersonal Data Protection ActPoint of careQuality Implementation FrameworkUniversity Malaya Medical CentreThis project was supported by University of Malaya\u2019s Postgraduate Research Fund (PG130-2013A) to the first author. The Prototype Research Grant Scheme (PR001-2017A) and High Impact Research Grant (UM.C/HIR/MOHE/06) from the Ministry of Higher Education, Malaysia funded the design of the clinical proformas as well as the development of system. Publication charges for this article was partially funded by University of Malaya Page Charge Fund and Department of Surgery UMSC Fund. The remaining cost was funded by the University of Malaya\u2019s Postgraduate Research Fund (PG130-2013A).The EMR data cannot be made public due to Personal Data Protection Act policies.This article has been published as part of BMC Bioinformatics Volume 19 Supplement 13, 2018: 17th International Conference on Bioinformatics (InCoB 2018): bioinformatics. The full contents of the supplement are available online at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-19-supplement-13.This study obtained ethics approval from Medical Research Ethics Committee, of the University Malaya Medical Centre Kuala Lumpur (MREC ID NO: #733.22).Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.The Quality Implementation Framework (QIF) is adopted because it synthesizes existing models and research support to provide a conceptual overview of the critical steps that comprise quality implementation [38]. The QIF contains four temporal phases and 14 distinct steps as described in Fig.\u00a01.\n\nFig. 1\nEstablishing i-Pesakit\u00a9 Breast Cancer Module at the University Malaya Medical Centre using the Quality Implementation Framework\nThese steps comprise four QIF phases: (i) initial considerations regarding the host setting, (ii) creating a structure for implementation, (iii) ongoing structure once implementation begins, and (iv) improving future applications. In our study, we have completed to Phase 4, providing steps to improve future applications. Summary of the four implementation phases and 14 critical steps in the Quality Implementation Framework [38] that are associated with quality implementation is presented in Fig.\u00a02.\n\nFig. 2\nFramework and development of i-Pesakit\u00a9 Breast Cancer Module adopted from Quality Implementation Framework (QIF) Model\nThe system workflow is illustrated in Fig.\u00a03, where the design of breast cancer module in EMR consolidates data input from the Surgery and Oncology departments and is merged with the existing patient details in EMR. To date, we have successfully developed a prototype module which includes Surgical and Oncology, as well as the Pharmacy departments. Future works will be done in integrating other clinical departments such as Radiology and Pathology to enhance the system interoperability and increases data accuracy. The i-Pesakit\u00a9 Breast Cancer Module is used widely in clinics, wards, as well as during diagnostic and treatment plan multidisciplinary meetings with Radiology and Oncology departments in discussing diagnosis and treatment plans.\n\nFig. 3\nArchitecture system of the UMMC i-Pesakit\u00a9 Breast Cancer Module\nThe construction of the breast cancer module involves studying the requirements of the organization. Based on the workflow, a prototype was built by translating the CRF into computerized web forms, linking with the existing rudimentary patient details available in the EMR. The translation of the CRF into a web-based i-Pesakit\u00a9 Breast Cancer Module is shown in Fig.\u00a04.\n\nFig. 4\nTranslation of breast cancer proforma into web-based UMMC i-Pesakit\u00a9 Breast Cancer Module\nFigure\u00a05 describes the team members involved directly or indirectly in the project from permissions to execution and testing. There are different categories of roles in the implementation team involving the project manager who is the quarterback of the EMR implementation group, project team members include critical key stakeholders; the hospital management, governance team, physician champions, bioinformaticians and IT staffs in designing and building the EMR module, as well as nurse leads for evaluation and quality assurance team in doing on-site testing and user trainings.\n\nFig. 5\nCrucial members of i-Pesakit\u00a9 Breast Cancer Module implementation team\nFollowing the system usability amongst the department of surgery users, it was found that the clinicians welcomed migration to a new routine from paper-based clinical notes into a fully digitized environment. Overall, the response was good in terms of using the EMR which was piloted in the Surgery department (Fig.\u00a06), hence the climate for up taking EMR for breast cancer was positive.\n\nFig. 6\nUsability survey on clinicians\u2019 perceptions of i-Pesakit\u00a9 BCM (a) Usage of system in clinical work (b) Clinicians\u2019 acceptance in finding this system worth the time and effort to be used (c) Effect on clinical workflow by using the system\nThe user web interfaces of the i-Pesakit\u00a9 Breast Cancer Module is demonstrated in Fig.\u00a07a, b. It supports basic and advanced functions that allow multidisciplinary users to experience the interoperability system between clinical departments. Some interesting features of i-Pesakit\u00a9 Breast Cancer Module include the ability to browse for clinical notes, lab reports, and treatment plans with tabs in current window, specific data editing roles for clinician, nurses and researchers, and data sharing between related clinical departments. More importantly, it implements the point of care data collection method which increases the efficiency of clinical workflow.\n\nFig. 7\na User web interface of the i-Pesakit\u00a9 Breast Cancer Module in Surgical Unit (b) User web interface of the i-Pesakit\u00a9 Breast Cancer Module in Oncology Department for Chemotherapy e-prescription\nClinical audit is important in measuring the hospital\u2019s quality measurement. One of the main output of the i-Pesakit\u00a9 Breast Cancer Module is its ability to track recurrence. Over the course of 17\u00a0months of i-Pesakit\u00a9 Breast Cancer Module implementation since February 2016, there are a total of 6974 follow-up of cancer cases and 10 recurrences tended by clinicians as shown in Fig.\u00a08a, b. In measuring the hospital performance, it is found that the average duration of a typical breast cancer diagnosis takes 16.8\u00a0days. The Treatment MDT meetings of 360 visits would describe the number of new breast cancer patients diagnosed in UMMC. Further work will refine meaningful data capture that will be reported in future works.\n\nFig. 8\nUMMC i-Pesakit\u00a9 Breast Cancer Module system test and evaluation results (Feb \u201816 - June \u201917) (a) Summary of UMMC i-Pesakit\u00a9 Breast Cancer Module usability (b) Monthly usage breakdown of UMMC i-Pesakit\u00a9 Breast Cancer Module in Surgery Department", "s12859-018-2396-7": "Molecular Recognition Features (MoRFs) are short protein regions present in intrinsically disordered protein (IDPs) sequences. MoRFs interact with structured partner protein and upon interaction, they undergo a disorder-to-order transition to perform various biological functions. Analyses of MoRFs are important towards understanding their function.Performance is reported using the MoRF dataset that has been previously used to compare the other existing MoRF predictors. The performance obtained in this study is equivalent to the benchmarked OPAL predictor, i.e., OPAL achieved AUC of 0.815, whereas the model in this study achieved AUC of 0.819 using TEST set.Achieving comparable performance, the proposed method can be used as an alternative approach for MoRF prediction.In the traditional view, the function of protein critically depends on the well-defined three-dimensional structure. This concept implies that protein sequence defines the structure, which in turn outlines the protein function. However, recent studies have revealed that many proteins do not form a defined three-dimensional structure but they are functional [1\u20134]. These proteins are called intrinsically disordered proteins (IDPs) or intrinsically disordered regions (IDRs). IDPs and IDRs lack the hydrophobic cores which makeup the structured domain. Thus, the functionality of these proteins arises in a different manner compared to the protein structure-function paradigm.IDPs consist of functional sites that are associated with important cellular functions, such as transcriptional regulation and signal transduction [2, 3]. Molecular recognition features (MoRFs) are one of the important functional sites that reside in IDPs and they permit interaction with structured partner proteins [2, 5, 6]. Upon interaction, they undergo a disorder-to-order transition and adopt conformations such as \u03b1-helix (\u03b1-MoRFs), \u03b2-strand (\u03b2-MoRFs), and \u03b3-coil (\u03b3-MoRFs) or mixtures of these complex-MoRFs. For a deeper understanding of disordered proteins and MoRFs, several studies have been done and databases have been introduced [5\u201310].Analyses of MoRFs can be done using experimental methods, however, these experiments are time-consuming and expensive to perform. Therefore, it is prudent to computationally identify MoRFs in disordered protein sequences. Many machine learning methods for predicting MoRFs have been studied [8, 9, 11\u201315] in this respect. A detailed literature review of the available state-of-the-art methods has been thoroughly done in our previous work [15].Analyzing the structural properties of MoRFs, their conformational behavior, and their interaction mechanism with various binding region helps in the understanding of MoRF properties. The disordered regions may fluctuate between several states including coil-like states, localized secondary structure and more compact states. The structural characteristics and the individual states of conformation are determined by the nature of amino acids in the disordered sequences. Thus, to this end, we predicted the structural properties of the disordered region using the structural predictor [16] and utilized it to identify the MoRFs.To predict amino acid residues of the protein sequence as MoRF and non-MoRF, a learning algorithm requires information of the residue itself and the information of the neighboring residues. However, to predict the terminal residues of the disordered protein sequence, complete neighboring information is not available and this adds complexity to the learning algorithm if a single model is trained to predict all the amino acids of the protein sequence. Therefore, we believe that if separate models are trained to predict the middle and the terminal regions, the performance is thought to improve as the neighboring information of the residues is appropriately incorporated for prediction.In this paper, we present a MoRF prediction scheme which involves support vector machine (SVM) models to predict MoRFs in protein sequences. In the proposed scheme, separate SVM models are used to predict the terminal and middle regions of a protein sequence. To do this, we have constructed two SVM models, the first one is trained using the terminal regions of training sequences and the second SVM model is trained using the middle region of training sequences. The presented scheme is different from the design approach of other state-of-the-art methods as here separate models are used to predict terminal and middle regions. To complement information present in the protein regions, we followed a similar approach as presented in Malhis et al., [12, 13] and Sharma et al., [15] where scores of many MoRF prediction models are combined. Therefore, we selected the following predictors MoRFpred-plus [14], PROMIS [15] and MoRFchibi [11], and combined their scores with the scores of the proposed model. The main aim of this amalgamation is the use of different sources of information encoded in the protein regions, as this has been proved to improve the MoRF prediction accuracies. The proposed model uses structural information, MoRFpred-plus uses evolutionary profiles and physicochemical properties, MoRFchibi uses physicochemical properties, PROMIS uses structural information and all are developed using a different learning algorithm. The reported performance of the combined model in this study is closer to the benchmarked predictor.There exist many tools to obtain structural information of a protein sequence. In this study, we utilized SPIDER2 predictor [16] to predict the structural attributes such as SS, ASA, HSE and backbone torsion angles of the protein sequences. SS represents the structural description of the protein sequence in a number of discrete states, such as helix, coil, and sheet. SS output is a three-dimensional vector containing the transition probabilities to three secondary structures. ASA represents the exposure level of the amino acids to solvent in a protein sequence and the output is a one\u2013dimensional vector representing the structural property. Backbone angles contain the backbone dihedral angles of the amino acids in the protein sequence. These angles are Phi, Psi, Theta (\u03b8) and Tau (\u03c4). HSE provides the number of C alpha atoms in the upper and lower spheres of the amino acids. We used the measures including HSE alpha and HSE beta along with the contact numbers for the amino acids.An SVM classifier with radial basis function (RBF) is used for MoRF prediction. We have used the same values of C and gamma (1000 and 0.0038) as in our previous study [15] to evaluate the proposed method. We have selected these values because the datasets used and features computed in both studies are similar and also these values provided good results in our previous study [15].where Aj is the j-th amino acid in the sequence, T is the total number of protein sequences in the training set and ni is the length of protein sequence\u00a0Pi. Before we define the positive and negative segments representing MoRFs and non-MoRFs, it is essential to select a suitable flank size (the length of neighboring residues), as this size will determine the length of the terminal regions. We selected the flank size as 20 from our previous study [15] because this flank size provided good performance for MoRF prediction. Using flank size as 20, the segments were extracted in the following way: suppose for a protein\u00a0Pi if the j-th amino acid is part of MoRF region for 1\u2009\u2264\u2009j\u2009\u2264\u200920 and ni\u2009\u2212\u200920\u2009<\u2009j\u2009\u2264\u2009ni, we extract the MoRF region plus flank regions of 20 amino acids upstream and downstream (if exist) of MoRF region as a positive segment for STENMoRF; and, if j-th amino acid is a part of MoRF region for 20\u2009<\u2009j\u2009\u2264\u2009ni\u2009\u2212\u200920, we extract the MoRF region plus flank of 20 amino acids upstream and downstream of MoRF region as a positive segment for MIDMoRF. Besides, a negative segment (same size as a positive segment) is extracted from a non-MoRF region in a similar way for STENMoRF and MIDMoRF, respectively.We extract an equal number of positive and negative samples using the steps of the StructMoRF method described in Sharma et al., [15], i.e., positive sample is extracted from a positive segment and negative sample is extracted from a negative segment, and to compute the feature vector for the samples, we used structural attributes. Suppose if the u-th number of the attribute is considered, the structural matrix M for a sample S of length l will be given as:where Mi, j is the element of a matrix M for 1\u2009\u2264\u2009i\u2009\u2264\u2009l\u00a0and\u00a01\u2009\u2264\u2009j\u2009\u2264\u2009u. To extract features from matrix M, we use auto-covariance based features for STENMoRF. Auto-covariance feature is computed from matrix M as follows:where DF is the distance factor. The computed feature matrix ACk, j will be of size DF \u00d7 u and can be rearranged in a vector form by reshaping it into a vector of length DF\u2009\u00d7\u2009u. Observing the performance, the effective value of DF was obtained as 10. Moreover, to extract features for MIDMoRF, we use feature extraction procedure of structMoRF method described in Sharma et al., [15].To score each residue in the query protein sequence, we extract a sample for each query residue using the window of size 41 (flank size\u00d7\u20092\u2009+\u20091). Except for the terminal region residues, the sample length will be of 41 amino acids. For a query residue, sample Sj is defined aswhere Aj is the query residue in the query sequence, j=1,2,...L and L is the length of the query protein sequence. Samples for a query sequence of length L can be is interpreted using eq. (4) as:We use the performance measures AUC, true positive rate (TPR) and false positive rate (FPR) to evaluate the models in this study, where AUC is defined as the area under the receiver characteristics curve.The performance in this study is reported using the same datasets that were used to analyses MoRF predictors such as MoRFchibi, MoRFpred, MoRFpred-plus, MoRFchibi-web, and OPAL. In this section, we present the model tuning scheme followed by the performance comparison.Feature selection techniques are very crucial for machine learning algorithms, as it reduces the computational complexity of the algorithm by reducing the feature dimension and it also selects best features to represent the data. In this study, we used successive feature selection scheme in the forward direction [17] to choose structural attributes for each of the model. Evaluating the scheme using structural attributes, the proposed models provided good performance (AUCs) with attributes from half-sphere exposure (HSE) \u03b1 and \u03b2 group. HSE is a measure of solvent exposure of a residue and it gives the number of C alpha atoms in the upper and lower spheres [18]. As more structural attributes are concatenated using the scheme, the performance deteriorates. Therefore, we used the attribute HSEu from the HSE \u03b1 group to extract features for the proposed models.We further evaluated the performance of the proposed model against the benchmarked OPAL predictor. For comparison, we plotted the propensity score of proteins P15337, P26645, P02686, P42768 and Q99967 from the EXP53 set. (Additional\u00a0file\u00a01: Figures S1 to S5) shows the propensity scores for each of the protein. We particularly observe that where OPAL performs poorly, the proposed model upgrades the scores of the verified MoRF regions. The analysis also showed that for some non-MoRF residues, the propensity scores of the proposed model are lower compared with that of OPAL.In detail, comparing the proposed method with MoRFchibi-web and OPAL, we obtained performance improvement (in terms of AUCs) of 1.9% and 0.4% using TEST set, 1.3% and 0.2% using TEST464 set, 1.2% and 0.2% using TEST266 set, and 4.1% and 0.2% using EXP53\u00a0ALL set, respectively. Furthermore, we observe that OPAL performed better in predicting long MoRFs, whereas MoRFchibi-web obtained good performance in scoring short MoRFs. Thus, on an average scale, the proposed method has boosted the performance of scoring short MoRFs by 1.1% compared to OPAL.The sequences in the TRAIN set contain MoRFs of variable size from 5 to 25 residues, and a single MoRF is present per sequence. Thus, this brings the issue of unbiased data, as the number of non-MoRF residues is more significant compared to the number of MoRF residues. To overcome this issue, during training step we have selected positive samples from MoRFs and we have extracted the same number of negative samples from non-MoRFs.use of different sources of information of disordered regions such as structural attributes; evolutionary profiles, and physicochemical attributes.use of different learning algorithms obtained by combining scores of the proposed model with the scores of MoRFpred-plus, PROMIS and MoRFchibi.selecting an equal number of positive and negative training samples from unbiased MoRF and non-MoRF regions.processing output scores, this processing provided extra information to see if the neighboring residues have high scores to form a MoRF region or not.In this study, disordered protein sequences are trisected into the terminal and middle regions for MoRF prediction. Incorporating structural, evolutionary and physicochemical information of disordered proteins, a comparable performance is achieved compared with the performance of the state-of-the-art MoRF predictors. Thus, the proposed method can be used as an alternative approach for MoRF prediction.Accessible surface areaArea under the curvefalse positive rateHalf-sphere exposureIntrinsically disordered proteinsIntrinsically disordered regionsMolecular recognition featuresRadial basis functionSecondary structureSupport vector machineTrue positive ratePublication charge for of this article is funded by RIKEN, Center for Integrative Medical Sciences, Japan and CREST, JST, Yokohama 230\u20130045, Japan.The data and materials are available at https://github.com/roneshsharma/BMC_Models2018/wiki.This article has been published as part of BMC Bioinformatics Volume 19 Supplement 13, 2018: 17th International Conference on Bioinformatics (InCoB 2018): bioinformatics. The full contents of the supplement are available online at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-19-supplement-13.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.To gauge MoRF predictors, in recent studies [8, 11\u201315], MoRF datasets have been introduced to train and test a model. Table\u00a01 shows the details of these datasets. The datasets TRAIN, TEST, and NEW were collected and assembled by Disfani et al., [8]. To assemble these sets, they collected and filtered the sequence from PDB depositions made before April 2008. The sequences were from different species. They filtered these sets such that each sequence in the set contains MoRF of size between 5 and 25 residues, and sequences in the TEST and NEW sets share less than 30% identity to the sequences in the TRAIN set. The TRAIN set is used to train the proposed model, the TEST set is used to evaluate the model, and we further combine TEST and NEW (as done in previous studies) sets and referred as TEST464 set to compare the MoRF predictors. We found that 42% of the sequences in the TEST464 set share 30% or more sequence identity to one another sequences in the same set. To address this, in our previous work [15] we have filtered the TEST464 set and obtained a resulting set as TEST266 containing 266 sequences. This set is also used for comparison. To validate MoRF predictors, it is important to have test sequences with MoRFs that are verified to be disordered in isolation. However, according to the sequences selection procedure described in Disfani et al., [8], it is not verified that the identified MoRFs in the sequences are disordered in isolation. Therefore, to address the aforementioned issue, we use the dataset EXP53 introduced in Malhis et al., [13] to report the performance. EXP53 contains 53 non-redundant protein sequences that have MoRFs experimentally validated to be disordered in isolation.Table 1\nDatasets used to train and test a MoRF predictor\nTo predict residues of intrinsically disordered protein sequences as MoRF or non-MoRF, a machine learning algorithm requires information of the residue itself and the information of the neighboring residues. However, to predict terminal residues of the disordered protein sequence, complete neighboring information is not available and this adds complexity to the learning algorithm to correctly predict MoRFs. To overcome this problem, in this study, we trisect the disordered protein sequence into the terminals and middle regions and we train two different models to predict these regions. Figure\u00a01 shows the overview of the proposed method. The first model (STENMoRF) is used to predict the terminal regions of the protein sequences and the second model (MIDMoRF) is used to predict the middle region of the protein sequences. To incorporate structural information, we computed features using backbone torsion angles, secondary structure (SS), half-sphere exposure (HSE) and accessible surface area (ASA) of the disordered protein sequence.\n\nFig. 1\nOverview of the proposed method. Fuse score means that the model scores are combined to provide the whole sequence scores\nIn the training step, we extract features from MoRFs and non-MoRFs. Suppose a protein sequence Pi is given as: (1)\n (2)\n\n (3)\n\n (4)\n\n (5)\nFigure\u00a02 shows the schematic illustration of extracting query samples from a query protein sequence. First 20 and last 20 samples representing terminal region residues are scored using STENMoRF and the remaining samples are scored using MIDMoRF.\n\nFig. 2\nSchematic illustration of extracting samples to score a query sequence. Aj is the j-th amino acid in the query sequence and L refers to the length of the query protein sequence\nThe proposed model predicts the terminal and middle regions of the disordered protein sequence by incorporating structural information. According to the previous studies [12, 13, 15], combining different learning algorithms with different sources of information is supposed to provide more information for MoRF prediction. Thus, we selected the recently published MoRF predictors (MoRFpred-plus [14], PROMIS [15] and MoRFchibi [11]) and combined their output scores with the scores of the proposed model. Figure\u00a03 shows the details of the combined scheme. To combine the output scores, we apply the common averaging principle, where scores of all the models are averaged.\n\nFig. 3\nCombined model. MoRFpred-plus and PROMIS are our predictors while we download MoRFchibi predictor and integrate it with our proposed model\nFurthermore, MoRFs considered in this study are of a size greater than 5 residues. Therefore, a query residue predicted as MoRF should be a part of MoRF region. To incorporate this criterion into the proposed scheme, we used the score calculation technique from our previous study [15] to process and compute the output scores of each model used in the combined scheme of Fig.\u00a03. The procedure of processing the scores involved the following steps: (1) take the window of scores for each residue, i.e., residue score plus region of flank scores on both sides; (2) compute the final score as the maximum of the window scores plus the median of the window scores divided by two. Thus, for each of the model, we varied the window flank size values from 1 to 30 to process the output scores, and we selected the best window flank size value for each model by observing the AUC performance measure. From Fig.\u00a04, we note that the proposed model performs well at window flank size value of 12 and to get average performance from MoRFpred-plus, MoRFchibi and combined proposed model, we processed their output scores with window flank size values of 4, 15 and 8, respectively. To show the increase in performance using separate models, instead of a single model used to predict the entire sequence, (Additional\u00a0file\u00a01: Table S1) describes the performance.\n\nFig. 4\nAUCs for the proposed model with varying window flank size values to process the output scores\nWe reported AUCs using the datasets TEST, TEST464, TEST266, and EXP53. The datasets TEST, TEST464 and TEST266 contain sequences with MoRFs of length 5 to 25 amino acids. However, sequences in the EXP53 dataset include MoRFs of length greater than 30 amino acids. Therefore, we report the performance of EXP53 as EXP53 ALL (contains all MoRFs), EXP53 SHORT (contains MoRFs up to the length of 30 amino acids) and EXP53 LONG (contains MoRF greater than 30 amino acids in length). Table\u00a02 shows the performance of the proposed and combined models. Although the models are trained to predict short MoRFs, we also reported performance for long MoRFs to see how the models perform while predicting long MoRFs. As observed in Table\u00a02, the proposed combined model performed similar to the benchmarked OPAL predictor. Hence, the novelty in this study is that we have presented a new alternative method of MoRF prediction and have also obtained close results compared with the state-of-the-art predictors.Table 2\nAUCs using the test sets\nIn this study, we presented the method of identifying MoRFs in disordered protein sequences. The method involves the construction of two SVM models, the first model is used to predict the terminal regions and the second model is used to predict the middle region of the disordered protein sequences. We decided to construct separate models for the two following reasons. First, since the residues in the middle region contain full neighboring information whereas the residues in the terminal regions do not contain full neighboring information, therefore, if a single model is to be used to predict both the regions, then complexity is added in identifying the MoRF residues. Second, MoRF regions in the datasets are distributed on the entire protein sequences, i.e., we note that in the TEST464 set, there are 296,362 residues and out of this 18,560 residues in this study are considered as terminal regions with 30% of which are MoRF residues. Therefore, it is necessary to score such a large number of terminal residues using a separate model to avoid fault detection of MoRFs. Figure\u00a05 shows the percentage of MoRF residues present in the terminal and middle regions of the TRAIN, TEST464 and EXP53 sets.\n\nFig. 5\nPercentage of MoRFs present in terminal and middle regions\nTo perform analyses on the average length of MoRFs used for training and evaluation, we plotted the number of MoRFs available for each length. Figure\u00a06 shows the analyses of MoRFs for the TRAIN, TEST464 and EXP53 sets. For the TRAIN and TEST464 sets, a larger number of the MoRFs are of length 7 to 11 residues while an equal number of MoRFs are present for the other lengths. The EXP53 set contains short and long MoRFs, and thus in Fig.\u00a06, it is observed that more MoRFs are present for length 10 to 28 residues while less number of MoRFs are present for length 29 to 110 residues. Since the models are trained using short MoRFs, to evaluate EXP53 set, we report the performance for EXP53 short MoRFs up to 30 residues and in addition to see how the models perform in predicting long MoRFs greater than 30 residues in length, we reported performance for EXP53 long MoRFs separately. The models show good results for predicting short MoRFs, and even though the models were trained to predict short MoRFs, they performed well in scoring long MoRFs. This was achievable because the models use residue information and its upstream/downstream neighboring residue information for prediction.\n\nFig. 6\nPercentage of MoRFs per respective length for the TRAIN, TEST464 and EXP53 sets\nThe comparable performance obtained by the proposed combined model in comparison with the benchmarked state-of-the-art predictors achieved due to the following implementation:(1)\nuse of different sources of information of disordered regions such as structural attributes; evolutionary profiles, and physicochemical attributes.\n\u00a0(2)\nuse of different learning algorithms obtained by combining scores of the proposed model with the scores of MoRFpred-plus, PROMIS and MoRFchibi.\n\u00a0(3)\nselecting an equal number of positive and negative training samples from unbiased MoRF and non-MoRF regions.\n\u00a0(4)\nprocessing output scores, this processing provided extra information to see if the neighboring residues have high scores to form a MoRF region or not.\n\u00a0Incorporating each of the mentioned implementation, the complementary information residing in the protein regions were extracted and combined for MoRF prediction. To compare the combined model with the benchmarked OPAL predictor, Table\u00a03 shows the FPR values for a range of TPR values. Thus, similar performance is noted.Table 3\nFPR for a given TPR value for the combined model and OPAL using EXP53 SHORT\n\n\nAdditional file 1:\nSupplementary text for Discovering MoRFs by trisecting intrinsically disordered protein sequence into terminals and middle regions. (PDF 446 kb)", "s12859-018-2567-6": "Consideration of tissue-specific gene expression in reconstruction and analysis of molecular genetic networks is necessary for a proper description of the processes occurring in a specified tissue. Currently, there are a number of computer systems that allow the user to reconstruct molecular-genetic networks using the data automatically extracted from the texts of scientific publications. Examples of such systems are STRING, Pathway Commons, MetaCore and Ingenuity. The MetaCore and Ingenuity systems permit taking into account tissue-specific gene expression during the reconstruction of gene networks. Previously, we developed the ANDSystem tool, which also provides an automated extraction of knowledge from scientific texts and allows the reconstruction of gene networks. The main difference between our system and other tools is in the different types of interactions between objects, which makes the ANDSystem complementary to existing well-known systems. However, previous versions of the ANDSystem did not contain any information on tissue-specific expression.A new version of the ANDSystem has been developed. It offers the reconstruction of associative gene networks while taking into account the tissue-specific gene expression. The ANDSystem knowledge base features information on tissue-specific expression for 272 tissues. The system allows the reconstruction of combined gene networks, as well as performing the filtering of genes from such networks using the information on their tissue-specific expression. As an example of the application of such filtering, the gene network of the extrinsic apoptotic signaling pathway was analyzed. It was shown that considering different tissues can lead to changes in gene network structure, including changes in such indicators as betweenness centrality of vertices, clustering coefficient, network centralization, network density, etc.The consideration of tissue specificity can play an important role in the analysis of gene networks, in particular solving the problem of finding the most significant central genes. Thus, the new version of ANDSystem can be employed for a wide range of tasks related to biomedical studies of individual tissues. It is available at http://www-bionet.sscc.ru/and/cell/.The rapid growth of data in the field of biology and biomedicine makes almost impossible for the researcher a complete analysis of this information without the use of automated text-mining tools. It should be especially noted that scientific publications are the main source of information for the vast majority of molecular genetic databases, which together contain only a part of the verified facts about the molecular interactions presented in publications. At the moment, the number of scientific publications (according to the PubMed database) and the number of international patents exceeds 28 million and 40 million, respectively, which allows regarding them as large text data. In fact, today, owing to the information explosion, we have a paradoxical situation in the biological sciences, where part of new knowledge, presented in publications, does not have a real opportunity to reach most researchers. One of the solutions to this is the development of computer tools that implement a full cycle of knowledge engineering and knowledge management, including an automated extraction of knowledge from scientific publications, patents and databases, formalization and accumulation of extracted data, and providing such data to end-users to deal with fundamental and practical tasks [1].In particular, the integration of information in knowledge bases with its further analysis can allow the generation of new hypotheses and obtaining novel knowledge about complex processes or mechanisms based on information about the individual subsystems that are part of such processes. This allows the reconstruction of the molecular genetic mechanisms of the functioning of living systems, which is a necessary condition for study of almost any important task in modern biology and biomedicine, including the search for drug targets, an assessment of the potential efficacy and toxicity of new drugs in pre-clinical trials, identification of biomarker molecules to create effective diagnostic systems, the search for candidate genes for genotyping, etc. [2].Thus, the methods of automated retrieval of information from unstructured textual data (text-mining) have been actively developed, and, in particular, such methods are popular in the fields of biology, biomedicine and various medical applications, including support for clinical decisions [3, 4], systems and integrative biology [5], curation of biological/biomedical databases [6], and pharmacovigilance inspections [7]. Modern text-mining techniques allow performance of an automated analysis of a wide range of information sources, including full-texts of scientific publications, patents and abstracts of articles [8, 9], as well as electronic health records of patients [10] and health-related data from social networks [7].An automated recognition of the names of biological entities in natural language texts is one of the primary tasks of any text-mining-based research [11]. The aim of this task is to identify the names of objects of a specific type (proteins, genes, drugs, etc.) within the raw text. The solution of the name-recognition task is associated with a number of issues, primarily caused by the incompleteness of existing dictionaries of object names, as well as with the lack of universal rules for the naming of newly discovered objects. Additionally, challenges emanate from the high synonymy of biological objects along with different stylistic features often used by authors, including anaphora, epiphora, coreferent mentions, etc. The existing name-recognition approaches can be classified into three main categories: methods based on the use of dictionaries, rules-based methods, and machine-learning methods [11]. The ANDSystem tool combines rules-based and dictionaries-based methods [12].Methods for the retrieval of information on the interactions between biological objects can also be divided into three main groups: (1) methods based on the statistically significant values of co-occurrence of object names in texts; (2) methods based on rules; and (3) methods based on machine-learning approaches. The main advantage of the first approach is its ease of implementation and robust completeness of the search, but at the same time, its accuracy is not high [13]. Moreover, such an approach does not allow detection of recently discovered interactions, information about which is not reflected in a large number of articles; also, this approach cannot be employed to determine specific parameters of interactions, such as the type of interaction or its direction.Rules-based methods allow achievement of a high level of accuracy of information retrieval but, at the same time, have relatively low completeness values [14]. An alternative approach to automated information retrieval that does not require the use of manually created rules is machine-learning methods, which have been widely utilized in recent years. Examples of such methods are the naive Bayesian classifier, decision trees, conditional random fields [15], and structured support vector machines [16], as well as deep-learning algorithms based on neural networks [17]. In turn, these methods can be divided into three large categories [14]: 1) learning with the use of labelled data (supervised learning); (2) learning with the use of unlabeled data (semi-supervised learning); and (3) hybrid methods based on integrated training schemes. The \u201csupervised learning\u201d methods commonly require large corpora of textual data with mapped interactions. At the present time, a second group of machine-learning methods is actively developing, and, among them, special attention is being paid to those based on the use of external knowledge bases for training, the so-called methods with remote training. Such methods combine the advantages of both \u201csupervised learning\u201d and \u201cunsupervised learning\u201d [18]. An approach of this kind assumes that any sentence where a pair of objects from the base of knowledge is mentioned most likely describes the interaction between these objects. To reduce errors associated with the fact that certain sentences from a positive training set may feature pairs of objects, but do not describe their interactions, various approaches have been proposed, including multivariate training [19\u201321]. These approaches have proven themselves to perform really well with respect to tasks of identifying protein-protein interactions, gene-disease associations, and analysis of catalytic reactions [13, 22\u201326].The most convenient form of representing extracted knowledge in the field of biology and biomedicine is semantic networks, where nodes correspond to biological objects, and edges correspond to interactions between them. The biological objects are molecular-genetic entities (genes, proteins, metabolites, drugs, microRNAs), biological processes, diseases, etc. The interactions between objects are determined by molecular, physical, chemical (protein-protein interactions, catalytic reactions), regulatory (activation, suppression, etc.), and associative (co-occurrence of object names in one sentence or document) relationships. The types of objects used and the relationships between them are specified by the domain ontology, specific for each computer system. On the basis of such ontologies, the molecular, cellular, and physiological mechanisms of functioning of living organisms under normal and pathological conditions can be described and visualized.Among the computer programs that use text-mining methods for automated extraction of knowledge, special attention should be paid to software and information systems implementing a so-called full cycle of knowledge engineering, including knowledge extraction, integration, and presentation of data to the end user. The well-known examples of such systems are STRING (http://string-db.com), Coremine (https://www.coremine.com/), Pathway Commons (https://www.pathwaycommons.org/), MetaCore (https://clarivate.com/products/metacore/), and Ingenuity (https://www.qiagenbioinformatics.com/products/ingenuity-pathway-analysis/). All the extracted information is automatically stored in special knowledge bases of such systems, while the data representation takes place through the execution of user queries. Moreover, it is often the case that such systems include various statistical methods, for example, the prioritization of genes, the identification of overrepresented processes or diseases, etc., permitting the user to perform an analysis and interpretation of experimental data. It should be especially noted that the depth and detail of the knowledge representation in such systems is determined by the terms and concepts of the used ontology. In particular, the Ingenuity system makes use of seven object types (proteins, genes, complexes, cells, tissues, drugs, and diseases), the relationships between which are described by the following types of interactions: transcriptional regulation, miRNA-mRNA target, phosphorylation cascades, protein-protein, or protein-DNA interactions.The use of such ontologies facilitates dealing with a wide range of problems related to analysis and visualization of disease mechanisms, gene expression, as well as proteomics and metabolomics data analysis. However, their narrow focus leads to the loss of information owing to the limitations in the descriptions of such ontologies and provides modern systems with the ability to extract just a small part of the knowledge presented in the texts of scientific publications [27]. For example, with Ingenuity and other computer systems, diseases are presented by a kind of generalized object that do not take into account various pathological conditions and dysfunctions, which cannot be considered independent diseases or their symptoms. At the same time, the number of references to such terms in the scientific literature, according to our estimates, can be in the hundreds of thousands. In particular, scientific articles contain a vast amount of information on the characterization of such dysfunctions as biomarkers, predictors, and risk factors for diseases. Huge amounts of information remain unused by the existing systems, including, for example, environmental factors which also are risk factors for many diseases.Earlier, we developed the ANDSystem tool, a software and information system based on the syntactic-semantic rules and designed for automated extraction of medical and biological knowledge from scientific publications [12, 28, 29]. The original ontology of the ANDSystem provides a highly detailed description of the subject area. In particular, the distinctive characteristic of the ANDSystem is the fact that all interactions are subdivided into organisms and cell lines. Unlike most existing systems, proteins and genes are treated by the ANDSystem as separate entities, and catalytic reactions can be represented by an enzyme, a substrate, and a product, potentially including many participants.The knowledge base of ANDSystem, created on the basis of the automated analysis of more than 25 million of texts from PubMed abstracts and dozens of biological and biomedical factual databases, is an unique resource featuring formalized information on more than 20 million interactions of various types (more than 25 types) between molecular genetic objects (proteins, genes, metabolites, microRNAs), biological processes, phenotypic signs, drugs and their side effects, and diseases including: physical interactions with forming of molecular complexes (protein/protein, protein/DNA, metabolite/protein); catalytic reactions and proteolytic events involving the substrate/enzyme/product, as well as substrate/product transformation events in the case of complex reactions where there is no description of the involved enzymes; regulatory interactions, divided into positive and negative regulation of gene expression, function/activity, transport, and protein stability, involving proteins, metabolites and drugs, regulation of protein translation involving miRNAs, regulation of biological processes and phenotypic traits involving proteins, metabolites and drugs; associative interactions of genes, proteins, metabolites, biological processes, phenotypic traits with diseases, etc. Each interaction in ANDSystem is characterized by its participants, the type and direction, and also by the organism and cell in which it occurred according to literature sources and external databases. It should be noted that, unlike the similar tools, genes and proteins within ANDSystem are treated as separate objects linked by a directed type of interaction (gene-\u2009>\u2009protein) - expression, and are classified by individual organisms. Furthermore, every interaction has a cell type attribute in which events related to this interaction were observed. The developed system surpasses its well-known analogues by the number of types of interactions and object types.Regarding the use of ANDSystem, a number of scientific studies have been carried out, in particular an analysis of the data of high-throughput proteomic experiments related to the study of Helicobacter pylori and their contribution to the development of gastritis and gastric tumors [30]; the study of the proteomic profile of the urine of a healthy person in normal conditions and under the influence of space-flight factors [31, 32]; an analysis of the tissue-specific knockout effect of genes and a search for potential drug targets [33]; and the identification of new regulatory molecular genetic mechanisms in the life cycle of the hepatitis C virus [34]. The application of ANDSystem to the analysis of comorbid diseases has allowed predicting new molecular genetic mechanisms of comorbidity of asthma and hypertension [35], as well as dystropy (reverse comorbidity) that determined the relationship between asthma and tuberculosis [36]; to study the molecular mechanisms of the comorbid relationship between pre-eclampsia, diabetes (diabetes mellitus and gestational diabetes), and obesity [37]; to investigate the molecular interactions of glaucoma with more than 20 comorbid diseases [38], etc. Based on the ANDSystem knowledge base, a number of software tools for analysis of experimental gene sets have been developed, in particular, a web-based FunGeneNet program that performs automated reconstruction of the associative gene network for the user-specified gene set and identifies genes involved in a greater number of interactions than can be expected according to random reasons [39]. In addition, the NACE tool employs the ANDSystem knowledge base to evaluate the effectiveness of potential signaling in gene networks [40]. Using the ANDSystem technology, we developed the Solanum TUBEROSUM knowledge base [41, 42], which is a computer platform for the complex intellectual processing of big data in the field of potato growing providing: 1) automated analysis of texts of scientific publications and factual databases with extraction of knowledge about genetics, markers, breeding, seed production, diagnostics of pathogens, protective equipment, and potato-storage technologies; 2) formalized representation of the extracted information in the knowledge base; 3) user access to this data; and 4) analysis and visualization of user query results. The ontology of the Solanum TUBEROSUM knowledge base contains dictionaries of molecular genetic objects (proteins, genes, metabolites, microRNAs, biomarkers, etc.), potato varieties and their phenotypic traits, potato diseases and pests, biotic and abiotic environmental factors, agrobiotechnology of cultivation, and technologies of potato processing and storage.Consideration of tissue-specific gene expression in the reconstruction of gene networks is a necessary condition for describing the processes taking place in the cells of this tissue. The ability to filter genes in gene networks by the level of their expression is realized in Ingenuity and some other systems, but absent in ANDSystem. As gene networks reconstructed with ANDSystem differ from similar networks created with other systems, the inclusion of information regarding tissue-specific gene expression in ANDSystem becomes a timely task. Thus, our paper is dedicated to the new version of ANDSystem, providing the reconstruction of tissue-specific gene networks. For this, we expanded ANDSystem with the data describing the expression of human genes in 272 different tissues, extracted from the Bgee database [43]. With regards to the example of the associative gene network of extrinsic apoptotic signaling pathway, it was shown that it was possible to filter the genes by their tissue-specific expression effects, such as network characteristics like the distribution of centrality of the vertices, the clustering coefficient, network centralization, the number of shortest paths, etc. Such effects can determine the difference in the functioning of the same processes in varying tissues. A new version of ANDSystem is available at http://www-bionet.sscc.ru/and/cell/.Data extracted using the templates are automatically stored in the ANDCell knowledge base, which is a module of ANDSystem. The analysis of publication texts and the filling of the ANDCell knowledge base are carried out on the server side of the system. Also, ANDSystem includes the ANDVisio program, which is a graphical user interface for the ANDCell database. It allows users to conduct reconstruction, expansion, graphic visualization, data filtering, as well as laying out of the gene network graph. The filtering in ANDVisio can be completed by object type, interaction type, data source, and organism [28].Number of nodes [44].Clustering coefficient [45]. The clustering coefficient of a given node is the probability that the two nearest neighbors of this node are also the nearest neighbors of each other. The clustering coefficient is the degree that determines how much the nodes are tending to cluster.Number of connected components [46] is a number of subgraphs in which any two vertices are connected to each other by paths, and which are connected to no additional vertices in the graph.Network centralization [47] that is a network-level, macrostructural measure that quantifies how \u2018dispersed\u2019 the centralities of the nodes are. When the measure is large, it means that few nodes are highly central, and the remaining occupy much fewer central positions in the network. Conversely, if network centralization is low, this means that the network is populated by nodes which occupy similarly central positions.Number of shortest paths [46] \u2013 the total number of shortest paths between all pairs of vertices in the graph. The shortest path is a path between two vertices in a graph such that the sum of the weights of its constituent edges is minimized. If the weights of edges are not specified, they are considered equal to 1.Average number of neighbors [44, 46].Network density [46] that describes the portion of the potential connections in a network that are actual connections. A \u201cpotential connection\u201d is a connection that could possibly exist between two \u201cnodes\u201d, regardless of whether or not it actually does, e.g., this person could know that person; this computer could connect to that one. Whether or not they do connect is irrelevant when one is referring to a potential connection. In contrast, an \u201cactual connection\u201d is one that actually exists, e.g., this person does know that person; this computer is connected to that one.Network heterogeneity [48] that reflects the tendency of a network to contain hub nodes.Bgee database (https://bgee.org/) contained information on gene expression patterns in multiple animal species obtained from different types of experiments, including RNA sequencing, Affymetrix microarray analysis, in situ hybridization, and expressed sequence tag (EST) surveys [43]. Bgee provides information on the presence/absence of gene expression under normal conditions (e.g., no gene knockout, no treatment, no disease). The data on tissue-specific gene expression were extracted from the Bgee database and used in ANDSystem \u201cas is\u201d.The evaluation of the statistical significance of differences in the structural characteristics of associative gene networks was carried out using the generation of pseudo-random gene networks. For each associative gene network, including a combined and five tissue-specific networks, 1000 pseudo-random networks were reconstructed. For generating of pseudo-random networks, the number of vertices was set equal to the number of vertices in the original network. Vertices were randomly selected from all human genes presented in ANDSystem, the interactions between the selected vertices were automatically reconstructed using ANDSystem as well. For each generated pseudo-random network, seven indicators of structural characteristics were calculated. Then all possible pairs of a combined network with tissue-specific networks were considered. For each such pair, seven indicators were calculated, representing the difference between the values of structural characteristics of the networks in this pair. For each indicator, a distribution by 1000 values was built, according to the number of the generated pseudo-random networks. Additionally, five pairs were considered, each including an original combined and a tissue-specific network. The statistical significance of the indicator difference for pairs of original networks and pairs of pseudo-random networks was estimated based on the indicator values for the original pairs falling below the 5% or above 95% quantiles on the distribution density graph for pseudo-random pairs.Apoptosis is a form of programmed cell death. Disruptions of apoptosis are associated with many diseases, including cancer, neurodegenerative, autoimmune, inflammatory diseases, and others [49\u201353]. There are two main apoptosis pathways: the extrinsic apoptotic pathway mediated by cell death receptors and the mitochondrial pathway [54, 55]. The study of tissue specificity of apoptotic signal transduction, in particular, was performed by Yoshinori Otsuki, 2004 [56]. Of particular interest to researchers is the evaluation of tissue-specificity for the extrinsic apoptotic pathway [57, 58]. Using the new functionality of ANDSystem, it was interesting for us to perform the filtering of genes according to their tissue-specific expression and analyze the structural features of the filtered associative gene network of the extrinsic apoptotic signaling pathway. The associative gene network of extrinsic apoptosis was reconstructed automatically with ANDSystem. A set of human genes from the \u201cextrinsic apoptotic signaling pathway\u201d Gene Ontology biological process (GO: 0097191) was used as input data. In total, 220 genes were involved in this process. A list of these genes was obtained using AmiGO 2 (http://amigo.geneontology.org/amigo).Lists of genes involved in the extrinsic apoptotic signaling pathway, which are expressed in various tissues, are available in Additional file 1: Table S1. For the analysis, five tissues were selected, including the lymph node, endometrium, embryo, retina, and substantia nigra. It is known that apoptosis actively occurs in the lymph node, endometrium, and embryo tissues in the norm [56]. At the same time, the activation of apoptosis in the retina and substantia nigra often leads to pathologies [59\u201361]. In this regard, it was noteworthy evaluating the differences in the structural organization of associative gene networks of apoptosis.All five tissue-specific associative gene networks were statistically significantly different from the combined network by values of Number of shortest paths and Average number of neighbors. The Clustering coefficient was the least informative among all the other structural characteristics, because, according to this indicator, only one retina gene network had statistically significant differences from the combined network.Reconstruction of gene networks that describe the biological and pathological processes occurring in a given tissue requires the consideration of tissue-specific gene expression. A new version of ANDSystem, containing information on tissue-specific expression for 272 tissues, provides such a feature. Analysis of the gene network of the extrinsic apoptotic signaling pathway using the new version of ANDSystem has shown that a change in the structure of the gene network can take place when considering different tissues. Regarding the example of the reconstructed gene networks for the five selected tissues herein, it was demonstrated that applying tissue-specificity filtering to their nodes affects such network characteristics as betweenness of centrality of vertices, clustering coefficient, network centralization, network density, etc. Thus, the newly developed version of ANDSystem, which provides an automated reconstruction of gene networks taking into account tissue-specific expression of genes, can find application in a wide range of medical and biological studies aimed at analysis of various biological processes within individual tissues.The publication cost was covered by the project \u201cInvestigation, analysis and complex independent expertize of projects of the National technological initiatives, including the accompanying of projects of \u201croad map\u201d \u201cNeuroNet\u201d\u201d, which is executed in the framework of the state assignment \u211628.12487.2018/12.1 of the Ministry of Science and Higher Education of Russian Federation.Results are shared in the additional files.This article has been published as part of BMC Bioinformatics Volume 20 Supplement 1, 2019: Selected articles from BGRS\\SB-2018: bioinformatics. The full contents of the supplement are available online at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-1.Not applicable.Not applicable.The authors declare that they have no competing interest.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.The text-mining module of ANDSystem performs all the necessary preprocessing of textual data, including conversion of input text into the ANDSystem format, text segmentation, normalization, morphological and syntactic analysis, mapping of the named entities (object names), etc. The mapping is carried out by ANDSystem utilizing a complex dictionary-based algorithm. Then, the semantic-linguistic templates are applied to such preprocessed texts for the automated retrieval of information regarding interactions between the mapped objects. Next, all retrieved interactions are classified by organisms and cell lines using the special patterns, which perform identification of cells and species within the analyzed text. Semantic-linguistic templates of the ANDSystem have a complex structure that can be distinguished into two separate parts, where a first part is presented by syntactic relationships between the names of objects and keywords in the sentence, while a second is the semantic links between the objects (Fig.\u00a01).\n\nFig. 1\nAn example of semantic-linguistic template from ANDSystem and result of its application for analysis of a sentence from Genestier et al., 1999 [62]\nThe following structural characteristics of associative gene networks are calculated in ANDVisio:\nNumber of nodes [44].\n\nClustering coefficient [45]. The clustering coefficient of a given node is the probability that the two nearest neighbors of this node are also the nearest neighbors of each other. The clustering coefficient is the degree that determines how much the nodes are tending to cluster.\n\nNumber of connected components [46] is a number of subgraphs in which any two vertices are connected to each other by paths, and which are connected to no additional vertices in the graph.\n\nNetwork centralization [47] that is a network-level, macrostructural measure that quantifies how \u2018dispersed\u2019 the centralities of the nodes are. When the measure is large, it means that few nodes are highly central, and the remaining occupy much fewer central positions in the network. Conversely, if network centralization is low, this means that the network is populated by nodes which occupy similarly central positions.\n\nNumber of shortest paths [46] \u2013 the total number of shortest paths between all pairs of vertices in the graph. The shortest path is a path between two vertices in a graph such that the sum of the weights of its constituent edges is minimized. If the weights of edges are not specified, they are considered equal to 1.\n\nAverage number of neighbors [44, 46].\n\nNetwork density [46] that describes the portion of the potential connections in a network that are actual connections. A \u201cpotential connection\u201d is a connection that could possibly exist between two \u201cnodes\u201d, regardless of whether or not it actually does, e.g., this person could know that person; this computer could connect to that one. Whether or not they do connect is irrelevant when one is referring to a potential connection. In contrast, an \u201cactual connection\u201d is one that actually exists, e.g., this person does know that person; this computer is connected to that one.\n\nNetwork heterogeneity [48] that reflects the tendency of a network to contain hub nodes.\nData on the expression of human genes in 272 tissues were loaded from the Bgee database [43] into ANDSystem. Figure\u00a02 depicts the dependence of the number of tissues on the number of genes expressed in them. From the figure, it can be seen that roughly 11,000 to 12,000 genes are expressed in the majority of tissues. The highest number of genes (26,363) was expressed in the testis. More than 20,000 genes were expressed in the female gonads and prostate gland. It should be noted that for the 12 tissues, information on expression was presented only for less than 100 genes. Meanwhile, for four tissues (corpus striatum, osseus labyrinth vestibule, parathyroid gland and seminiferous tubule of testis), the number of expressed genes appeared to be less than 10.\n\nFig. 2\nThe distribution of the number of tissues depending on the number of genes expressed in them\nFor filtering genes by their expression in a given tissue, a new filter was added to ANDVisio (Fig.\u00a03). To filter, in the \u201cFilter\u201d window of the ANDVisio visualizer, the user must click on the \u201cAttribute filter\u201d tab, then select \u201cExpressed in tissue\u201d from the top drop-down list entitled \u201cAttribute\u201d, and then type the name of the tissue of interest in the \u201cPattern\u201d field. Moreover, ANDVisio allows searching for the names of tissues by their partial matching, and for this, the user needs to add an asterisk character to the end of the name.\n\nFig. 3\nAn example of tissue selection in the ANDVisio program for the filtering of genes by their tissue-specific expression\nFor each of these tissues, a tissue-specific associative gene network of extrinsic apoptotic signaling pathway was reconstructed, which included only genes expressed in a given tissue, then the structural characteristics of all the reconstructed networks were calculated (Table\u00a01). Further, the combined network was generated, which is an associative gene network of an extrinsic apoptotic signaling pathway reconstructed without applying tissue-specific expression filters. From Table 1, it can be observed that the largest number of vertices among the five analyzed tissue networks was in the lymph node network, which had only 38 vertices less than in the combined network. The smallest network was that reconstructed for the retina. It was interesting to note that while the size of the embryo network was not the largest, it had the maximum clustering coefficient. The maximum value of the number of connected components was for the combined network, the largest network. At the same time, even though the retina network was the smallest, the value of this indicator for it exceeded the values for the endometrium and substantia nigra networks. It can also be highlighted that the network reconstructed for the substantia nigra had the highest network density value, while the lowest value of this parameter was for the embryo network. The network centralization, number of shortest paths, average number of neighbors, and network heterogeneity parameters correlated with the number of vertices in the networks.Table 1\nThe structural characteristics of the reconstructed associative gene networks of the extrinsic apoptotic signaling pathway\n\na statistically significant differences from the combined network with p-value<\u20090.05\nAdditional file 2: Table S2 features information regarding the centrality of the gene network vertices for the analyzed tissues. It appeared that the highest value for betweenness centrality in the gene networks reconstructed for the endometrium, embryo and substantia nigra had the BCL2 protein, in the combined and lymph node networks, the TNF protein, while in the retina network, it was Caspase-3. As such, in the considered networks, the highest centrality vertices presented with different genes/proteins, and it was intriguing to assess the connectivity of these nodes with one another. For this purpose, a new gene network was reconstructed with ANDSystem, which included all three central vertices, i.e., BCL2, TNF and Caspase-3 (Fig.\u00a04). Additionally, vertices intermediaries were included in the network, i.e., nodes interacting with at least two of the three central vertices. It is clearly evident from Fig. 4 that the obtained network is strongly connected. An analysis of this gene network using the FunGeneNet system (http://www-bionet.sscc.ru/fungenenet/) showed that the p-value for the significance of the difference of this network from random networks by the number of connections was less than 10\u2212\u200911. In this regard, it can be assumed that the three central vertices of the gene network are represented in a certain functional module and the transition from one central node to another, depending on the tissue, does not lead to a disruption of the apoptosis regulation function.\n\nFig. 4\nAssociative gene network describing interactions between three central genes/proteins of the extrinsic apoptotic signaling pathway biological process. The label of each vertex has the name of the gene or protein as well as the code of the tissue in which this gene/protein is expressed (L - lymph node, En - endometrium, Em - embryo, R - retina, Sn - substantia nigra)\nTable\u00a02 lists the paired correlations between the values of the betweenness centrality for the same genes/proteins, obtained from the networks reconstructed for the analyzed tissues and from the combined network. In cases when the vertex was absent in at least one of the networks, its betweenness centrality value was set to zero. The highest correlation was observed for the pair, lymph node and combined network (R\u2009=\u20090.985), while the smallest was for retina and combined network (R\u2009=\u20090.398). As is observed in Table 2, for some pairs of tissues, the values of the betweenness centrality varied widely. Thus, considering tissue specificity can be important in the analysis of gene networks, in particular, solving the problem of finding the most significant central genes.Table 2\nPairwise correlation between the values of the betweenness centrality of vertices from the reconstructed networks (combined network and tissue networks)\n\n\nAdditional file 1:\nTable S1. Lists of genes involved in extrinsic apoptotic signaling pathway expressed in different tissues. (XLSX 187 kb)\n\n\n\nAdditional file 2:\nTable S2. Values of betweenness centrality for gene network of extrinsic apoptotic signaling pathway filtered according to tissue-specific expression of genes. (XLSX 45 kb)", "s12859-019-2675-y": "Long non-coding RNAs play an important role in human complex diseases. Identification of lncRNA-disease associations will gain insight into disease-related lncRNAs and benefit disease diagnoses and treatment. However, using experiments to explore the lncRNA-disease associations is expensive and time consuming.In this study, we developed a novel method to identify potential lncRNA-disease associations by Integrating Diverse Heterogeneous Information sources with positive pointwise Mutual Information and Random Walk with restart algorithm (namely IDHI-MIRW). IDHI-MIRW first constructs multiple lncRNA similarity networks and disease similarity networks from diverse lncRNA-related and disease-related datasets, then implements the random walk with restart algorithm on these similarity networks for extracting the topological similarities which are fused with positive pointwise mutual information to build a large-scale lncRNA-disease heterogeneous network. Finally, IDHI-MIRW implemented random walk with restart algorithm on the lncRNA-disease heterogeneous network to infer potential lncRNA-disease associations.Compared with other state-of-the-art methods, IDHI-MIRW achieves the best prediction performance. In case studies of breast cancer, stomach cancer, and colorectal cancer, 36/45 (80%) novel lncRNA-disease associations predicted by IDHI-MIRW are supported by recent literatures. Furthermore, we found lncRNA LINC01816 is associated with the survival of colorectal cancer patients. IDHI-MIRW is freely available at https://github.com/NWPU-903PR/IDHI-MIRW.Long non-coding RNAs (lncRNAs) are the biggest part of non-coding RNAs with at least 200 nucleotides and no observed potential to encode proteins [1, 2]. To date, 15,778 lncRNA genes and 27,908 lncRNA transcripts have been annotated in human genome by the GENCODE v27. Increasing evidences have revealed that lncRNAs have key roles in gene regulations, affecting cellular proliferation, survival, migration and genomic stability [3\u20137]. Therefore, there is no surprise that mutation and dysregulation of lncRNAs could contribute to the development of various human complex diseases [8\u201310], such as HOTAIR in breast cancer [11] and MALAT1 in early-stage non-small cell lung cancer [12]. On the other hand, lncRNAs can drive many important cancer phenotypes through their interactions with other cellular macromolecules including DNA, protein, and RNA [4]. For example, lncRNA PCGEM1 and PRNCR1 are associated with androgen receptor in prostate cancer cells [6]. And lncRNA PTCSC3 could be a tumor suppressor in thyroid cancer cells by interacting with miR-574-5p [13].In recent years, the number of experimentally verified lncRNA-disease associations is gradually increasing. Several databases for lncRNA functions and disease associations have been published, such as LncRNAdb [14], LncRNADisease [15], Lnc2Cancer [16] and NONCODE [17]. However, known lncRNA-disease associations still involve a small part of lncRNAs and diseases. Computational methods have been developed to predict the potential lncRNA-disease associations that can be used as candidates for biological experiment verifications, which would greatly reduce the experiment cost and save time for finding new lncRNA-disease associations. Existing computational methods can mainly be categorized into machine learning-based methods [18\u201329] and network-based methods [30\u201341]. The machine learning-based methods, such as LRLSLDA [18], LDAP [26], and MFLDA [27], have been developed to predict the potential lncRNA-disease associations. LRLSLDA [18] combined optimal classifiers in lncRNA space and disease space into a single classifier to predict lncRNA-disease associations based on lncRNA expression profiles and known lncRNA-disease associations. But how to combine the classifiers reasonably needs to further study. LDAP [26] employed two lncRNA similarity measures and five disease similarity measures to calculate lncRNA similarities and disease similarities, respectively, then used the bagging SVM to predict lncRNA-disease associations. However, this method suffered from fusing multiple similarities effectively. Fu et al. [27] developed a lncRNA-disease associations prediction model (MFLDA) with matrix factorization by integrating seven relational data sources between six object types (e.g. lncRNAs, miRNAs, genes, Gene Ontology, Disease Ontology, and drugs). Yet, MFLDA can only predict the potential lncRNA-disease associations which share both lncRNAs and diseases with known associations in training set.The network-based methods, such as RWRlncD [30], RWRHLD [32], KATZLDA [33] and GrwLDA [40], use lncRNA-disease association, disease similarity, lncRNA similarity, and other molecular similarity to construct the lncRNA similarity networks, or lncRNA-disease heterogeneous network, then implement global network models (such as random walk and various propagation algorithms) to predict potential lncRNA-disease associations [10]. RWRlncD [30] constructed a lncRNA similarity network based on known lncRNA-disease associations, i.e., each lncRNA in their network has at least one known lncRNA-disease association, for predicting potential lncRNA-disease associations. So, the major limitation of RWRlncD is that it cannot predict lncRNA-disease associations for lncRNAs and diseases without any known lncRNA-disease associations. RWRHLD [32] calculated lncRNA similarities and disease similarities based on crosstalk between lncRNAs and miRNAs and directed acyclic graph in the disease ontology, respectively. One weakness of RWRHLD is that lncRNAs interacting with similar miRNAs do not always mean related with similar diseases, and only a small fraction of lncRNA-miRNA interactions is used [25]. KATZLDA [33] integrated lncRNA expression similarity, lncRNA functional similarity, Gaussian interaction profile kernel similarity for diseases and lncRNAs, disease semantic similarity, and known lncRNA-disease associations to build a lncRNA-disease heterogeneous network, then used KATZ algorithm to calculate potential association probability of each lncRNA-disease pair. GrwLDA [40] introduced a global network random walk method to predict potential lncRNA-diseases association by integrating disease semantic similarity, lncRNA functional similarity and known lncRNA-disease associations. Overall, the results of existing network-based methods show that integrating diverse lncRNA-related and disease-related information can boost the prediction accuracy of the lncRNA-disease association. However, most existing methods are limited to a small number of lncRNAs and diseases. For example, the network built in RWRHLD involves 697 lncRNAs and 126 diseases, while the network built in GrwLDA just involves 78 lncRNAs and 113 diseases. In addition, most existing methods calculate the lncRNA/disease similarities only on those that have at least one known lncRNA-disease association.To address the aforementioned issues (or limitations) and further improve the prediction accuracy, we proposed a novel network-based method, namely IDHI-MIRW, to predict the potential lncRNA-disease associations by constructing a large-scale lncRNA-disease heterogeneous network with Random Walk with Restart (RWR) algorithm and the positive pointwise mutual information (PPMI). Instead of constraining lncRNA and disease on those with at least one known lncRNA-disease association, IDHI-MIRW calculates the lncRNA similarities for all the lncRNAs involved in lncRNA expression profiles, lncRNA-miRNA interactions, and lncRNA-protein interactions, and also calculates the diseases similarities for all the diseases involved in disease ontology, disease-miRNA associations, and disease-gene associations. Then, IDHI-MIRW uses the RWR algorithm on each similarity network to capture network topological structural features for measuring the lncRNA/disease topological similarity through the PPMI. By integrating the lncRNA/disease topological similarity, and introducing the known lncRNA-disease association information, a large-scale lncRNA-disease heterogeneous network is built. Finally, the random walk with restart on heterogeneous network (RWRH) algorithm [42] is applied on the lncRNA-disease heterogeneous network to predict the potential lncRNA-disease associations. The computational results show that IDHI-MIRW cannot only better predict the known lncRNA-disease associations, but also can effectively predict the potential lncRNA-disease associations, providing more candidates for experimental verification. Most of the new predicted lncRNA-disease associations are supported by recent literatures. By analyzing nine unvalidated lncRNAs, we found that six lncRNAs were differentially expressed in corresponding cancers. We also found that lncRNA LINC01816 is associated with the survival of colorectal cancer patients, which provides evidence that this lncRNA is disease-related.In this section, we first introduced the evaluation method and metrices for evaluating the performance of the IDHI-MIRW method. Then, we compared our IDHI-MIRW method with other existing state-of-the art methods on a small-scale lncRNA-disease heterogeneous network, explored the predictive power of IDHI-MIRW on a large-scale lncRNA-disease heterogeneous network, and discussed the effect of different parameters. In the end, we analyzed several predicted potential lncRNA-disease associations with our IDHI-MIRW.The leave-one-out cross validation (LOOCV) test method was used to evaluate the performance of the IDHI-MIRW method. In LOOCV test method, each known lncRNA-disease association in the dataset is singled out in turn as a test sample, and the remaining lncRNA-disease associations are used as training samples. That is, for a given disease di, each known lncRNA associated with di is left out in turn as a test sample, and corresponding association edge between test lncRNA and di is removed, and the remaining lncRNAs associated with di are considered as training samples.The area under the receiver operating characteristic (ROC) curve (AUC) and the area under the precision-recall (PR) curve (AUPR) were used as evaluation metrices in our experiments. The ROC curve is the plot of the true-positive rate (TPR, or Recall) versus the false-positive rate (FPR) at different rank cutoffs. The PR curve is the plot of the ratio of true positives among all positive predictions for each given recall rate.There are four main parameters in our method, which are the restart probability \u03b1 in RWR, and the restart probability \u03b2, jumping probability \u03b3, parameter \u03b7 in RWRH. \u03b7 is used to weight the importance of lncRNA topological similarity subnetwork and disease topological similarity subnetwork. To evaluate the effect of parameters, we implemented our IDHI-MIRW on HNetL heterogeneous network in LOOCV test with different \u03b1,\u00a0\u03b2, \u03b3, and \u03b7 values (varying from 0.1 to 0.9 with scale 0.1). Additional\u00a0file\u00a03 shows the AUC and AUPR values of IDHI-MIRW with different parameters. We can see that the performance of IDHI-MIRW is robust to the value of these four parameters. Additional\u00a0file\u00a04 presents the AUC and AUPR values of IDHI-MIRW on HNetS heterogeneous network in LOOCV test. In this work, we selected \u03b1\u2009=\u20090.9, \u03b3\u2009=\u20090.9, \u03b7\u2009=\u20090.2, and \u03b2\u2009=\u20090.6.We used breast cancer, stomach cancer, and colorectal cancer as the cases to predict their potential associated lncRNAs with our IDHI-MIRW. For a given disease, all known lncRNAs associated with this given disease were considered as the seed nodes, and other remaining lncRNAs (i.e., without known association with the given disease) were considered as the candidates associated with the given disease. By implementing our IDHI-MIRW algorithm on the large-scale lncRNA-disease heterogeneous network, and according to the lncRNA-disease associations ranking scores from large to small, we extract top 15 potential association lncRNAs for each cancer. These top potential association lncRNAs are listed in Additional\u00a0files\u00a05, 6, and 7.For breast cancer which is one of most common cancers and the second leading cause of cancer death [49], 13 out of 15 potential association lncRNAs are supported by recent literatures. For example, Diego Chacon-Cortes et al. [50] investigated six SNPs (i.e. rs1888138, rs7336610, rs9589207, rs17735387, rs4248505, rs1428) in the lncRNA MIR17HG, and identified significant association between rs4248505 at the allele level and rs4248505/ rs7336610 at the haplotype level susceptibility to breast cancer, which means that lncRNA MIR17HG plays the main role in the pathophysiology of breast cancer. Fu et al. [51] found lncRNA SNHG1, SNORD28 and sno-miR-28 are all significantly upregulated in breast tumors. LncRNA can be used as the biomarkers and therapeutic targets in combatting breast cancer [52].For stomach cancer (or gastric cancer) which is the third leading cause of cancer mortality in the world [53, 54], 11 out of 15 potential association lncRNAs can be supported by recent literatures. For example, Hu et al. [55] discovered that lncRNA CRNDE increases gastric cancer cell viability and promotes proliferation by targeting miR-145. Pan et al. [56] found that lncRNA DANCR is activated by SALL4 and promotes the proliferation and invasion of gastric cancer cells. Specially, lncRNA LINC01816 (also known as LOC100133985) associated with stomach cancer has been confirmed by Tian et al. [57]. LncRNA LINC01816 is down-regulated and might be protective factor in gastric cancer.For colorectal cancer which is the third most commonly diagnosed cancer in males and the second in females [58], 12 out of 15 potential association lncRNAs can be supported by recent literatures. For example, Zhao et al. [59] found that lncRNA SNHG1 promotes cell proliferation by affecting P53 in colorectal cancer. Zhang et al. [60] found that lncRNA CYTOR (also known as LINC00152) down-regulated by miR-376c-3p restricts viability and promotes apoptosis of colorectal cancer cells.Additional\u00a0files\u00a08 and 9. 5/6 unvalidated lncRNAs are significantly differentially expressed in corresponding cancers.In summary, 36 (13 for breast cancer, 11 for stomach cancer, 12 for colorectal cancer) out of 45 potential association lncRNAs have been supported by recent literatures. By analyzing the nine unvalidated potential association lncRNAs, we found that six lncRNAs are differentially expressed in corresponding cancers, and lncRNA LINC01816 is associated with the survival of patients with colorectal cancer. Results of these three case studies show that IDHI-MIRW can effectively predict the new association lncRNAs for a disease.LncRNAs play important roles in the development of human complex diseases. More and more attentions have been paid to discover the lncRNA functions related with human complex disease. Most previous computational methods only focus on the small-scale lncRNA-disease heterogeneous network (i.e., involving small numbers of lncRNAs and diseases) to predict the lncRNA-disease associations. To address this issue, IDHI-MIRW was developed to predict the potential lncRNA-disease associations based on a large-scale lncRNA-disease heterogeneous network (containing 7637 lncRNAs and 6453 diseases). Instead of calculating similarities of lncRNAs and diseases only involving in known lncRNA-disease associations, IDHI-MIRW used three lncRNA-related information (i.e., lncRNA expression profiles, lncRNA-miRNA interactions, and lncRNA-protein interactions) to form three lncRNA similarity networks, and three disease-related information (i.e., disease semantic similarity, disease-miRNA associations, and disease-gene associations) to form three disease similarity networks. Furthermore, instead of directly fusing those similarity networks, IDHI-MIRW applied the RWR algorithm on each lncRNA/disease similarity network to capture the topological similarity, and the PPMI to generate lncRNA/disease topological similarity network. The large-scale lncRNA-disease heterogeneous network was constructed by combing the lncRNA topological similarity network, disease topological similarity network, and the known lncRNA-disease bipartite graph. Then, the RWRH algorithm was used to prioritize candidate lncRNAs for each query disease. Our experiment results show that IDHI-MIRW achieves a better performance than other existing methods. We evaluated the effectiveness of introducing multiple information sources and capturing topological similarities, Tables\u00a02 and 3 show that those strategies are effective for improving the performance of predicting lncRNA-disease associations. In addition, more novel lncRNA-disease associations predicted by IDHI-MIRW are supported by recent literatures, which means that IDHI-MIRW can effectively predict the novel association lncRNAs for a query disease. All the predicted lncRNA-disease associations are provided in Additional\u00a0file\u00a010.Although IDHI-MIRW can effectively predict potential lncRNA-disease associations, there are still several issues need to be further addressed in the future. First, IDHI-MIRW used three lncRNA-related and three disease-related information to generate similarity matrices, we still expect to integrate more information (e.g., lncRNA GO annotations and disease MeSH annotation) to better predict lncRNA-disease association. Second, the averaging strategy was used to integrate the lncRNA/disease topological similarity matrices, we expect to design better integration approaches in future work to measure the different contributions of multiple lncRNA/disease similarities.In this study, we proposed a novel network-based method (namely IDHI-MIRW) for identifying potential lncRNA-disease associations. We built a large-scale lncRNA-disease heterogeneous network by integrating multiple lncRNA-related information (i.e. lncRNA expression profiles, lncRNA-miRNA interactions, and lncRNA-protein interactions), multiple disease-related information (i.e. disease semantic similarity, disease-miRNA associations, and disease-gene associations), and known lncRNA-disease association information using RWR and PPMI. Our experimental results show that IDHI-MIRW can achieve higher performance than other state-of-the-art methods, and we found lncRNA LINC01816 is associated with the survival of colorectal cancer patients. These results indicate that IDHI-MIRW will contribute to the identification of potential lncRNA-disease associations.We collected lncRNA expression profile, lncRNA-miRNA interaction, and lncRNA-protein interaction data for constructing the lncRNA similarity networks, and Diseases Ontology (DO) information, disease-miRNA association, and disease-protein association data for constructing the disease similarity networks. All lncRNAs are annotated by ensembl gene ID, and all diseases are annotated by Disease Ontology ID.LncRNA expression profiles were downloaded from EMBL-EBI (E-MTAB-5214), which includes the expression profiles in 53 human tissue samples. LncRNA-miRNA interactions and lncRNA-protein interactions were collected from starBase v2.0 [43], NPInter v3.0 [44], and RAID v2.0 [45] databases. Diseases ontology terms were collected from the Disease ontology [46]. Diseases-miRNAs associations were collected from HMDD v2.0 [47]. Disease-gene associations were collected from DisGeNet [48]. Known lncRNA-disease associations were collected from lncRNAdisease [15], lnc2Cancer [16], and GeneRIF [62]. Details and statistics of these data are shown in Additional\u00a0file\u00a011.When the L1 norm of \u0394S\u2009=\u2009St\u2009+\u20091\u2009\u2212\u2009Stis less than a small positive \u03b5 (we set \u03b5\u2009=\u200910\u221210), we can obtain a stationary distribution matrix S, which was referred as the diffusion state of each node [70]. The element S(i,\u2009j) in diffusion state matrix S represents the probability of RWR starting node i and ending up at node j in equilibrium. When the diffusion states of two nodes are close, which suggests that they may have similar positions with respect to other nodes in the network and they probably share similar functions.The matrix MI is a non-symmetric matrix, thus we use the average of MI(i,\u2009j) and MI(j,\u2009i) to represent the topological similarity of node i and node j. After obtaining three lncRNA topological similarity matrices \\( {X}_L^1 \\), \\( {X}_L^2 \\), \\( {X}_L^3 \\) of LncNet1, LncNet2, LncNet3, and three disease topological similarity matrices \\( {X}_D^1 \\), \\( {X}_D^2 \\), \\( {X}_D^3 \\) of DisNet1, DisNet2, DisNet3, we can form the integration lncRNA topological similarity matrix \\( {X}_L^{\\prime } \\) by averaging three lncRNA topological similarity matrices, and the disease topological similarity matrix \\( {X}_D^{\\prime } \\) by averaging three disease topological similarity matrices, that is, \\( {X}_L^{\\prime }=\\left({X}_L^1+{X}_L^2+{X}_L^3\\right)/3 \\), \\( {X}_D^{\\prime }=\\left({X}_D^1+{X}_D^2+{X}_D^3\\right)/3 \\). Thus, we generated the lncRNA topological similarity network LncTSNet, and disease topological similarity network DisTSNet.After some steps, the steady state probability vector p\u2217\u2009=\u2009p\u221e can be obtained by performing the iteration until the difference between pt and pt\u2009+\u20091 (measured by the L1 norm) fall below 10\u221210. p\u2217 gives the ranking score of every lncRNA for a query disease. The lncRNAs with maximum in p\u2217 are considered as the most probable associated lncRNAs of the query disease.The area under the receiver operating characteristic curveThe area under the precision-recall curveDirected acyclic graphDisease ontologyFalse-positive rateLong noncoding RNAsLeave-one-out cross validation; ROC: receiver operating characteristicPositive pointwise mutual informationPrecision-recallRandom walk with restartRandom walk with restart on heterogeneous networkTrue-positive rateThis work was supported by the National Natural Science Foundation of China under Grant No. 61873202, No. 61473232 and No. 91430111; and the National Library of Medicine grants of United States under Grant No. R00LM011673. The funding bodies did not play any roles in the design of the study, in the collection, analysis, or interpretation of data, or in writing the manuscript.IDHI-MIRW is available at https://github.com/NWPU-903PR/IDHI-MIRW, and the datasets used and/or analyzed during the current study are available from the corresponding references.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.We compared our IDHI-MIRW method with other six state-of-the-art methods of LRLSLDA [18], LNCSIM [19], RWRlncD [30], IRWRLDA [34], KATZLDA [33] and GrwLDA [40] on the small-scale lncRNA-disease heterogeneous network (HNetS) which contains 362 lncRNAs, 370 diseases, and 2169 known lncRNA-disease associations. Most existing methods often built this small-scale lncRNA-disease heterogeneous network in which each lncRNA (or disease) has at least an associated disease (or lncRNA) to predict the potential lncRNA-disease associations. LRLSLDA [18] and LNCSIM [19] adopt the semi-supervised learning frameworks with Laplacian regularized least squares. RWRlncD [30], IRWRLDA [34], KATZLDA [33] and GrwLDA [40] are the network-based methods. All methods were executed on a win10 system pc with i7\u20136700 CPU and 16.0G memory. Figure\u00a01 shows the AUC and AUPR values of IDHI-MIRW and other six methods. IDHI-MIRW achieved a better performance than other six methods in terms of AUC and AUPR. The AUC of IDHI-MIRW is 0.866, which is 0.337, 0.108, 0.350, 0.245, 0.197 and 0.061 higher than that of LRLSLDA, LNCSIM, RWRlncD, IRWRLDA, KATZLDA and GrwLDA, respectively. The AUCPR of IDHI-MIRW is 0.318, which is 0.143, 0.213, 0.296, 0.172, 0.194 and 0.166 higher than that of LRLSLDA, LNCSIM, RWRlncD, IRWRLDA, KATZLDA and GrwLDA, respectively. The recall values of seven methods at different rank cutoffs are listed in Table\u00a01, from which we can see that the recall value of IDHI-MIRW is higher than that of other six existing methods at 10, 20, 50, and 100 ran cutoff. These results show that our IDHI-MIRW can effectively predict the lncRNA-disease associations.\n\nFig. 1\nResults of IDHI-MIRW, LRLSLDA, LNCSIM, RWRlncD, IRWRLDA, KATZLDA and GrwLDA on a small-scale lncRNA-disease heterogeneous network in LOOCV test. a AUC values. b AUPR values\nTable 1\nRecalls of seven methods at different cutoffs on a small-scale lncRNA-disease heterogeneous network in LOOCV test\nTo further evaluate the performance of IDHI-MIRW for predicting the associated lncRNAs for new diseases without any known lncRNA association information, we removed all the known lncRNA associations for the query disease in the small-scale lncRNA-disease heterogeneous network. Due to RWRlncD implemented the RWR algorithm on an lncRNA similarity network, we just compared our IDHI-MIRW method with other five methods of LRLSLDA, LNCSIM, IRWRLDA, KATZLDA and GrwLDA for predicting the associated lncRNAs of the query diseases. The comparison results are shown in Fig.\u00a02, which shows that our IDHI-MIRW method can better predict the associated lncRNAs for the new disease than other existing prediction methods.\n\nFig. 2\nPrediction results for diseases without any known disease association information. a AUC values. b AUPR values\nIn order to illustrate the effectiveness of introducing multiple information sources, we collected 7637 lncRNAs and 6453 diseases from EMBL-EBI (E-MTAB-5214), starBase v2.0 [43], NPInter v3.0 [44], RAID v2.0 [45], Diseases ontology [46], HMDD v2.0 [47], and DisGeNet [48] to construct a large-scale lncRNA-disease heterogeneous network (HNetL) by introducing 2169 known lncRNA-disease associations, then implemented our IDHI-MIRW method on HNetL. Additional\u00a0files\u00a01 and 2 provided the data processing procedure for lncRNAs and diseases. The results of IDHI-MIRW on HNetS and HNetL heterogeneous networks in LOOCV test are listed in Table\u00a02, from which we can see that introducing more lncRNAs and diseases can effectively improve the predictive performance of IDHI-MIRW and can predict the potential lncRNAs/diseases for new disease/lncRNA without any known disease/lncRNA association information. All these results show that IDHI-MIRW can obtain a more reliable performance for predicting lncRNA-disease associations.Table 2\nResults of IDHI-MIRW on the small-scale lncRNA-disease heterogeneous network and large-scale lncRNA-disease heterogeneous network in LOOCV test\nIn order to evaluate the effectiveness of using the topological similarity network to construct the lncRNA-disease heterogeneous network for improving the predictive performance, we designed another method of IDHI-AVG by adopting the strategy of averaging three lncRNA similarity matrices of LncNet1, LncNet2 and LncNet3 to form the lncRNA integration network (i.e., LncINet), averaging of three disease similarity matrices of DisNet1, DisNet2, and DisNet3 to form the disease integration network (i.e., DisINet). IDHI-AVG combines these two integration similarity networks of LncINet and DisINet with known lncRNA-disease bipartite network to construct the lncRNA-disease heterogeneous network on which RWRH algorithm is implemented to predict the potential lncRNA-disease associations. The compared results of IDHI-AVG and IDHI-MIRW on the small-scale lncRNA-disease heterogeneous network (HNetS) and large-scale ncRNA-disease heterogeneous network (HNetL) in LOOCV test are shown in Table\u00a03. We can see the AUC and AUPR values of IDHI-MIRW are higher than that of IHDI-AVG. These results demonstrate that the strategy of using RWR and PPMI to form lncRNA/disease topological similarity networks and further constructing the lncRNA-disease heterogeneous network is effective. It can improve the performance of predicting lncRNA-disease associations.Table 3\nCompared results of IDHI-MIRW and IDHI-AVG on the small-scale lncRNA-disease heterogeneous network and large-scale lncRNA-disease heterogeneous network in LOOCV test\nTo further discover the evidences for the predicted lncRNAs associated with cancers, we analyzed the RNAseq and clinical data from TCGA for breast cancer, stomach cancer and colorectal cancer. For colorectal cancer, the RNASeq data including 19,676 protein coding genes, 15,513 lncRNA genes in 41 normal samples and 474 tumor samples were downloaded from TCGA. Using DESeq2 [61] algorithm, we found 1230 significantly upregulated lncRNAs and 568 downregulated lncRNAs by setting log2FC\u2009>\u20091 (or\u2009<\u2009\u2212\u20091), FDR\u2009<\u20090.001. Among three unvalidated lncRNA, lncRNA SNHG7 (14th) is significantly upregulated in tumor samples (Fig.\u00a03a). Meanwhile, we downloaded the clinical data of 448 tumor samples, and Kaplan-Meier survival analysis shows that lncRNA LINC01816 (10th) can divided the 448 colorectal cancer patients into high and low-risk groups with different survival times (Fig. 3b). The results of RNAseq and clinical data analysis for breast cancer and stomach cancer are shown in.\n\nFig. 3\nResults of RNASeq and clinical data analysis for colorectal cancer. a boxplot of lncRNA SNHG7 expression in normal and tumor samples. b survival curve for lncRNA LINC01816\nOur IDHI-MIRW algorithm consists of the following four steps. Step 1, build three lncRNA similarity networks (i.e., LncNet1, LncNet2, LncNet3) based on lncRNA expression profiles, lncRNA-miRNA interactions, and lncRNA-protein interactions, and also build three disease similarity networks (i.e., DisNet1, DisNet2, DisNet3) based on disease ontology, disease-miRNA associations, and disease-gene associations. Step 2, form the lncRNA topological similarity network (LncTSNet) and disease topological similarity network (DisTSNet) by fusing lncRNA and disease multiple topological similarities obtained through implementing RWR on lncRNA similarity network (LncNet1, LncNet2, LncNet3) and disease similarity network (DisNet1, DisNet2, DisNet3), respectively. Step 3, construct a large-scale lncRNA-disease heterogeneous network by integrating lncRNA topological similarity network (LncTSNet), disease topological similarity network (DisTSNet), and known lncRNA-disease associations. Step 4, implement RWRH on the lncRNA-disease heterogeneous network for predicting the potential lncRNA-disease associations. The flowchart of IDHI-MIRW is shown in Fig.\u00a04.\n\nFig. 4\nFlowchart of the IDHI-MIRW. a building three lncRNA similarity networks and three disease similarity networks by calculating the Pearson correlation coefficient and Gaussian interaction profile kernel similarity. b forming the lncRNA/disease topological similarity networks with RWR and positive pointwise mutual information. c constructing the large-scale lncRNA-disease heterogeneous network by integrating lncRNA/disease topological similarities and known lncRNA-disease associations. d predicting the potential lncRNA-disease associations by implementing RWRH\nBy calculating the Pearson correlation coefficient of any lncRNA pair with expression profiles and fixing the P-value threshold (<\u20090.01), we built the LncNet1 lncRNA similarity weighted network. Based on Gaussian interaction profile kernel similarity [18, 63] of lncRNA-miRNA and lncRNA-protein interactions, we computed the Gaussian interaction profile kernel similarity between any pair of lncRNA li and lncRNA lj, then built the LncNet2 and LncNet3 lncRNA similarity weighted networks, respectively. Gaussian interaction profile kernel similarity between lncRNA li and lncRNA lj is calculated. (1) (2)where, the interaction profile IP(li) is the binary vector of lncRNA-miRNA (or lncRNA-protein) interactions encoding the presence or absence of interactions between lncRNA li and miRNA (or protein) in the lncRNA-miRNA (or lncRNA-protein) interaction dataset, \u03bal controls the kernel bandwidth, and Nl is the total number of lncRNAs.Based on the structure of a directed acyclic graph (DAG) in Disease Ontology, we used the function \u201cdoSim\u201d form R package \u201cDOSE\u201d [64] to obtain the similarity between any disease pair, then built the DisNet1 disease similarity weighted network. Based on Gaussian interaction profile kernel similarity of disease-miRNA and disease-gene associations, we computed the Gaussian interaction profile kernel similarity between any pair of disease di and dj, then built the DisNet2 and DisNet3 disease similarity weighted networks, respectively. (3) (4)where, the interaction profile IP(di) is the binary vector of disease-miRNA (or disease-gene) associations encoding the presence or absence of associations between di and miRNA (or gene) in the disease-miRNA (or disease-gene) association dataset. \u03bad controls the kernel bandwidth, and Nd is the total number of diseases.Instead of directly fusing six similarity networks (i.e., LncNet1, LncNet2, LncNet3, DisNet1, DisNet2, and DisNet3), we captured the network topological structural features by implementing the RWR algorithm on each similarity network. The RWR algorithm is a network diffusion algorithm, which has been extensively applied to analyze the complex biological network [65\u201369]. By considering both local and global topological connectivity patterns within network, the RWR algorithm can fully exploit the direct or indirect relation between nodes [65]. The RWR algorithm can be formulated as: (5) (6)where, St is the distribution matrix in which the (i,\u2009j)-th element denotes the distribution probability of node j being visited from node i after t iterations in the random walk process and S0 is the initial distribution matrix in which S0(i,\u2009i)\u2009=\u20091, S0(i,\u2009j)\u2009=\u20090, \u2200j\u2009\u2260\u2009i.\u00a0\u03b1 is restart probability controlling the relative influence of local and global topological information. B is the weighted adjacency matrix of lncRNA (or disease).Motivated by Gligorijevic et.al. [69], we then calculated the topological similarity of each node pair by using PPMI, which is defined as: (7)By integrating the LncTSNet and DisTSNet networks with known lncRNA-disease bipartite network, we can construct the lncRNA-disease heterogeneous network whose adjacency matrix can be defined as: (8)where, AL and AD represent the weighted adjacency matrices of LncTSNet and DisTSNet, respectively; ALD is the adjacency matrix of the lncRNA-disease bipartite graph; ADL represents the transpose of ALD. If there is association between lncRNA i and disease j in known lncRNA-disease associations, ALD(i,\u2009j)\u2009=\u20091, otherwise, ALD(i,\u2009j)\u2009=\u20090.To predict the association between lncRNA and disease, we adopted the RWRH (random walk with restart on heterogeneous network) algorithm [42] to prioritize candidate lncRNAs associated with a given disease. The RWRH algorithm is well-known heterogeneous network-based algorithm to infer the gene-phenotype relationship. It can effectively capture the complementarity of two kinds of node within heterogeneous network, which is widely used to predict the association problem [42, 71, 72]. The RWRH algorithm on the lncRNA-disease heterogeneous network can be formulated as: (9)where, pt is a probability vector in which the i-th element holds the probability of finding the random walker at node i at step t; \u03b2\u2009\u2208\u2009(0,\u20091) is restart probability; p0 is the initial probability vector for lncRNA-disease heterogeneous network which is defined as \\( {p}^0=\\left[\\begin{array}{c}\\eta \\ast {u}_0\\\\ {}\\left(1-\\eta \\right)\\ast {v}_0\\end{array}\\right] \\). u0 and v0 represent the initial probability of LncTSNet and DisTSNet, respectively. The initial probability u0 of LncTSNet network is set such that all the seed nodes are assigned to the equal probabilities with the sum of probabilities equal to 1. Similarity, the initial probability v0 of DisTSNet network is given. The parameter \u03b7\u2009\u2208\u2009(0,\u20091) is used to weight the importance of each subnetwork.\\( M=\\left[\\begin{array}{cc}{M}_L& {M}_{LD}\\\\ {}{M}_{DL}& {M}_D\\end{array}\\right] \\) is the transition matrix of the lncRNA-disease heterogenous network, where ML and MD are the intra-subnetwork transition matrices, MLD and MDL are the inter-subnetwork transition matrices. Let \u03b3 be the jumping probability, that is, the probability of random walker jumping from lncRNA network to disease network or vice versa. Thus, the transition probability ML(i,\u2009j) from lncRNA li to lncRNA lj and the transition probability MD\u2009(i,\u2009j) from disease di to disease dj are defined as (10) (11)The transition probability from lncRNA li to disease dj and the transition probability from disease di to lncRNA lj are described as: (12) (13)\n\nAdditional file 1:\nLncRNA data processing procedure. (TIF 1447 kb)\n\n\n\nAdditional file 2:\nDisease data processing procedure. (TIF 1340 kb)\n\n\n\nAdditional file 3:\nAUPR values of IDHI-MIRW on the large-scale lncRNA-disease heterogeneous with different parameters in LOOCV test. (A) AUC values with different \u03b1. (B) AUC values with different \u03b3. (C) AUC values with different \u03b7. (D) AUC values with different \u03b2. (E) AUPR values with different \u03b1. (F) AUPR values with different \u03b3. (G) AUPR values with different \u03b7. (H) AUPR values with different \u03b2. (TIF 3520 kb)\n\n\n\nAdditional file 4:\nAUC and AUPR values of IDHI-MIRW on the small-scale lncRNA-disease heterogeneous with different parameters in LOOCV test. (A) AUC values with different \u03b1. (B) AUC values with different \u03b3. (C) AUC values with different \u03b7. (D) AUC values with different \u03b2. (E) AUPR values with different \u03b1. (F) AUPR values with different \u03b3. (G) AUPR values with different \u03b7. (H) AUPR values with different \u03b2. (TIF 3705 kb)\n\n\n\nAdditional file 5:\nThe top 15 predicted associated lncRNAs for breast cancer. (XLSX 9 kb)\n\n\n\nAdditional file 6:\nThe top 15 predicted associated lncRNAs for stomach cancer. (XLSX 9 kb)\n\n\n\nAdditional file 7:\nThe top 15 predicted associated lncRNAs for colorectal cancer. (XLSX 9 kb)\n\n\n\nAdditional file 8:\nThe results of RNASeq data analysis for breast cancer. (A) heatmap of top 200 most significantly dysregulated lncRNA expression values. (B) heatmap of lncRNA AL157395.1 expression values. (C) boxplot of lncRNA AL157395.1 expression in normal and tumor samples. (D) heatmap of lncRNA AP001528.1 expression values. (E) boxplot of lncRNA AP001528.1 expression in normal and tumor samples. (TIF 9850 kb)\n\n\n\nAdditional file 9\nThe results of RNASeq data analysis for stomach cancer. (A) heatmap of top 200 most significantly dysregulated lncRNA expression values. (B) heatmap of lncRNA KCNQ1OT1 expression values. (C) boxplot of lncRNA KCNQ1OT1 expression in normal and tumor samples. (D) heatmap of lncRNA DLEU2 expression values. (E) boxplot of lncRNA DLEU2 expression in normal and tumor samples. (F) heatmap of lncRNA LINC00299 expression values. (G) boxplot of lncRNA LINC00299 expression in normal and tumor samples. (TIF 9211 kb)\n\n\n\nAdditional file 10:\nThe predicted lncRNA-disease associations. (TXT 180 kb)\n\n\n\nAdditional file 11:\nDetails and statistics of collected data. (DOCX 34 kb)", "s12859-019-2656-1": "Group structures among genes encoded in functional relationships or biological pathways are valuable and unique features in large-scale molecular data for survival analysis. However, most of previous approaches for molecular data analysis ignore such group structures. It is desirable to develop powerful analytic methods for incorporating valuable pathway information for predicting disease survival outcomes and detecting associated genes.We here propose a Bayesian hierarchical Cox survival model, called the group spike-and-slab lasso Cox (gsslasso Cox), for predicting disease survival outcomes and detecting associated genes by incorporating group structures of biological pathways. Our hierarchical model employs a novel prior on the coefficients of genes, i.e., the group spike-and-slab double-exponential distribution, to integrate group structures and to adaptively shrink the effects of genes. We have developed a fast and stable deterministic algorithm to fit the proposed models. We performed extensive simulation studies to assess the model fitting properties and the prognostic performance of the proposed method, and also applied our method to analyze three cancer data sets.Both the theoretical and empirical studies show that the proposed method can induce weaker shrinkage on predictors in an active pathway, thereby incorporating the biological similarity of genes within a same pathway into the hierarchical modeling. Compared with several existing methods, the proposed method can more accurately estimate gene effects and can better predict survival outcomes. For the three cancer data sets, the results show that the proposed method generates more powerful models for survival prediction and detecting associated genes. The method has been implemented in a freely available R package BhGLM at https://github.com/nyiuab/BhGLM.Survival prediction from high-dimensional molecular data is an active topic in the fields of genomics and precision medicine, especially for various cancer studies. Large-scale omics data provide extraordinary opportunities for detecting biomarkers and building accurate prognostic and predictive models. However, such high-dimensional data also introduce statistical and computational challenges. Tibshirani [1, 2] has proposed a novel penalized method, lasso, for variable selection in high-dimensional data, which has attracted considerable attention in modern statistical research. Thereafter, several penalized methods were developed, like minimax concave penalty (MCP) method by Zhang [3, 4], smoothly clipped absolute deviation (SCAD) penalty method by Fan and Li [5]. These penalization approaches have been widely applied for disease prediction and prognosis using large-scale molecular data [6\u201311].Furthermore, the group structures among molecular variables was noticed in analysis. For example, genes can be grouped into known biological pathways or functionally similar sets. Genes within a same biological pathway may be biologically related and statistically correlated. Incorporating such biological grouping information into statistical modeling can improve the interpretability and efficiency of the models. Several penalization methods have been proposed had been proposed to utilize the grouping information, such as group Lasso method [12], sparse group lasso (SGL) [13, 14]. group bridge [15], composite MCP [16], composite absolute penalty method [17], group exponential Lasso [18], group variable selection via convex log-exp-sum penalty method [19], and doubly sparse approach for group variable selection [20]. Some of these methods perform group level selection, including or excluding an entire group of variables. Others can perform bi-level selection, achieving sparsity within each group. Huang et al. [21] and Ogutu et al. [22] reviewed these penalization methods in prediction and highlighted some issues for further study.Ro\u010dkov\u00e1 and George [23, 24] recently proposed a new Bayesian approach, called the spike-and-slab lasso, for high-dimensional normal linear models using the spike-and-slab mixture double-exponential prior distribution. Based on the Bayesian framework, we have recently incorporated the spike-and-slab mixture double-exponential prior into generalized linear models (GLMs) and Cox survival models, and developed the spike-and-slab lasso GLMs and Cox models for predicting disease outcomes and detecting associated genes [25, 26]. More recently, we have developed the group spike-and-slab lasso GLMs [27] to incorporate biological pathways.In this article, we aim to develop the group spike-and-slab lasso Cox model (gsslasso Cox) for predicting disease survival outcomes and detecting associated genes by incorporating biological pathway information. An efficient algorithm was proposed to fit the group spike-and-slab lasso Cox model by integrating Expectation-Maximization (EM) steps into the extremely fast cyclic coordinate descent algorithm. The novelty is incorporating group or biological pathway information into the spike-and-slab lasso Cox model for predicting disease survival outcomes and detecting associated genes. The performance of the proposed method was evaluated via extensive simulations and comparing with several commonly used methods. The proposed procedure was also applied to three cancer data sets with thousands of gene expression values and their pathways information. Our results show that the proposed method not only generates powerful prognostic models for survival prediction, but also excels at detecting associated genes.In Cox survival model, variables yi\u2009=\u2009(ti, di) for each individual is the survival outcome. The censoring indicator di takes 1 if the observed survival time ti for individual i is uncensored. The di takes 0 if it is censored. For individual i, the true survival time is assumed by Ti. Therefore, when Ti\u2009=\u2009ti, di\u2009=\u20091, whereas when Ti\u2009>\u2009ti, di\u2009=\u20090. The predictor variables include numerous molecular predictors (e.g., gene expression) and some relevant demographic/clinical covariates. Assume that the predictors can be organized into G groups (e.g., biological pathways) based on existing biological knowledge. It should be indicated that the group could overlap each other. For example, one or some genes can belong to two or more biological pathway. Following the idea of overlap group lasso [28\u201331], we performed a restructure step by replicating a variable in whatever group it appears to expand the vector of predictors.If group g includes important predictors, the parameter \u03b8g will be estimated to be relatively large, implying other predictors in the group more likely to be important. Therefore, the group-specific Berllouli prior plays a role on incorporating the biological similarity of genes within a same pathway into the hierarchical model. For the probability parameters, we adopt a beta prior, \u03b8g~beta(a,\u2009b), setting a\u2009=\u2009b\u2009=\u20091 yielding the uniform hyper prior \u03b8g~U(0,\u20091) that will be used in later sections to illustrate our method. Hereafter, the above hierarchical Cox models are referred to as the group spike-and-slab lasso Cox model.From Eqs. (7) and (8), we can see that the estimates of pj and Sj are larger for larger coefficients\u03b2j, leading to different shrinkage for different coefficients.Choose a starting value for \u03b20, and \\( {\\theta}_g^0 \\). For example, we can initialize \u03b20\u2009=\u20090, and \\( {\\theta}_g^0=0.5 \\).For t\u2009=\u20091, 2, 3, \u2026,E-step: Update \u03b3j and \\( {S}_j^{-1} \\) by their conditional posterior expectations.Update \u03b2 using the cyclic coordinate decent algorithm;Update (\u03b81, \u22ef, \u03b8G) by Eq. (11).We assess convergence by the criterion: \u2223d(t)\u2009\u2212\u2009d(t\u2009\u2212\u20091)\u2009\u2223\u2009/(0.1\u2212|\u2009d(t)|\u2009)\u2009<\u2009\u03b5, where d(t)\u2009=\u2009\u2009\u2212\u20092pl(\u03b2(t)) is the estimate of deviance at the tth iteration, and \u03b5 is a small value (say 10\u2212\u20095).We can use several ways to measure the performance of a fitted group lasso Cox model, including the partial log-likelihood (PL), the concordance index (C-index), the survival curves, and the survival prediction error [37]. The partial log-likelihood function measures the overall quality of a fitted Cox model, and thus is usually used to choose an optimal model [37, 41, 42]. The standard way to evaluate the performance of a model is to fit the model using a data set and then calculate the above measures with independent data. A variant of cross-validation [31, 43], called pre-validation method was used in the present study to evaluate the performance. The data was randomly split to K subsets of roughly the same size. The (K \u2013 1) subsets was used to fit a hierarchical Cox model. The estimate of coefficients denoted as \\( {\\widehat{\\beta}}^{\\left(-k\\right)} \\) from the data excluding the k-th subset. The prognostic indices \\( {\\widehat{\\eta}}_{(k)}={X}_{(k)}{\\widehat{\\beta}}^{\\left(-k\\right)} \\), called the cross-validated or pre-validated prognostic index, were calculated for all individuals in the k-th subset of the data. Cross-validated prognostic indices \\( {\\widehat{\\eta}}_i \\) for all individuals can be calculated by cycling through all the K parts. Then, (\\( {t}_i,{d}_i,{\\widehat{\\eta}}_i \\)) was used to compute the several measures described above. We can see that the cross-validated prognostic value for each patient is derived independently of the observed response of the patient. Therefore, the \u2018pre-validated\u2019 dataset (\\( {t}_i,{d}_i,{\\widehat{\\eta}}_i \\)) can essentially be treated as a \u2018new dataset\u2019. This procedure provides valid assessment of the predictive performance of the model [31, 43].The spike-and-slab double-exponential prior requires two preset scale parameters (s0, s1). Following the previous studies [24\u201326], we set the slab scale s1 to be relatively large (e.g., 1), and consider a sequence of L decreasing values {\\( {s}_0^l \\)}: \\( {s}_1>{s}_0^1>{s}_0^2>\\cdots >{s}_0^L>0 \\), for the spike scale s0. We then fit L models with scales {\\( \\left({s}_0^l,{s}_1\\right);l=1,\\cdots, L \\)} and select an optimal model using the method described above. This procedure is similar to the lasso implemented in the widely-used R package glmnet, which quickly fits the lasso Cox models over a grid of values of \u03bb covering its entire range, giving a sequence of models for users to choose from [31, 41].We have incorporated the method proposed in this study into the function bmlasso() in our R package BhGLM [44]. The package BhGLM also includes several other functions for summarizing and evaluating the predictive performance, like summary.bh, cv.bh predict.bh. The function in the package is very fast, usually taking several minutes for fitting and evaluating a model with thousands of variables. The package BhGLM is freely available from https://github.com/nyiuab/BhGLM.Each simulated dataset included n\u2009=\u2009500 observations, with a censored survival response yi and a vector of m\u2009=\u20091000 continuous predictorsXi\u2009=\u2009(xi1,\u2009\u2026,\u2009xim). We assumed 20 groups. Each group included about 50 predictors. For example, group 1 and 2 included variables (x1,\u2009\u2026,\u2009x50) and (x51,\u2009\u2026,\u2009x100), respectively. The vector Xi was randomly sampled from multivariate normal distributionN1000(0,\u2009\u03a3), where the covariance matrix \u03a3 was set to account for varied grouped correlation and overlapped structures under different simulation scenarios. We simulated several scenarios. The predictors were assumed to be correlated each other with in group and those predictors in different groups were assumed to be independent. The correlation coefficient r was generally set to be 0.5. To simulate the censored survival response, following the method of Simon [41], we generated the \u201ctrue\u201d survival time Ti for each individual from the exponential distribution: \\( {T}_i\\sim \\mathrm{Expon}\\left(\\exp \\left({\\sum}_{j=1}^m{x}_{ij}{\\beta}_j\\right)\\right) \\) and the censoring time Ci for each individual from the exponential distribution: Ci~Expon(exp(ri)), where ri were randomly sampled from a standard normal distribution. The observed censored survival time ti was set to be the minimum of the \u201ctrue\u201d survival and censoring times, ti\u2009=\u2009min(Ti,\u2009Ci), and the censoring indicator di was set to be 1 if Ci\u2009>\u2009Ti and 0 otherwise. Our simulation scenarios resulted in different censoring ratios, but generally below 50%. For all the scenarios, we set eight coefficients to be non-zero and the others to be zero.only four non-zero predictors included in group 1 and 11:20 predictors included in group 1 and 11:50 predictors included in group 1 and 11:There are 8 non-null groups including non-zero coefficients: {x5}, {x55}, {x305}, {x355}, {x505}, {x555}, {x905}, and\u00a0{x955};There are 3 non-null groups including non-zero coefficients: {x5,\u2009x15,\u2009x25}, {x355,\u2009x365,\u2009x375}, and\u00a0{x905,\u2009x915};There is only 1 non-null group including non-zero coefficients: {x5,\u2009x10,\u2009x15,\u2009x20,\u2009x25,\u2009x30,\u2009x35,\u2009x40}. The overlap settings were the same with scenario 2. The group number and effect sizes of these non-zero coefficients are shown in Table 1.To evaluate the effect of correlation within group, we set different correlation coefficients within a group: r\u2009=\u20090.0, 0.5, and 0.7. Other settings were the same with scenario 2.The significant feature of the proposed spike-and-slab prior is the self-adaptive shrinkage. To show this property, we performed additional simulation study based on Scenario 1. We fixed the prior scale (s0, s1)\u2009=\u2009(0.02, 1) and varied the effect size of the first simulated non-zero predictor (x5) from (\u2212\u20092, 2). We recorded the scale parameters for this non-zero predictor (x5) and nearby zero effect predictor (x6), and non-zero predictor (x20) with the simulated effect size \u2212\u20090.7. These three predictors belong to the same group.We applied the proposed gsslasso Cox model to analyze three real datasets, ovarian cancer (OV), lung adenocarcinoma (LUAD), and breast cancer. The whole genome expression data were downloaded from The Cancer Genome Atlas (TCGA, http://cancergenome.nih.gov/) (updated at June 2017). We firstly clean the data to get the clear survival information and potential genes involved in further analysis. The details of the three datasets and clean procedure are described below paragraphs. Secondly, several genome annotation tools were used to construct the pathways information. All the genes were mapped to KEGG pathways by using R/bioconductor packages: mygene, clusterProfiler and AnnotationDbi [46]. The R/Bioconductor mygene package was used to convert gene names to gene ENTREZ ID. The clusterProfiler package was used to get pathway/group information for genes, by loading the gene ENTREZ ID. AnnotationDbi was used primarily to create mapping objects that allow easy access from R to underlying annotation databases, like KEGG in the present study. By using these packages, we mapped the genes into pathways, and got group structure information for further analysis. Only the gene included in pathways were used in further analysis. Thirdly, the proposed method and several penalization approaches used in above simulation study were applied to analyze the survival data with thousands of genes and pathway/group information. We performed 10-fold cross-validation with 10 replicates to evaluate the predictive values of the several models. After model fitting, the non-zero parameters were the detected genes.This dataset contains mRNA expression data and relevant clinical outcome for ovarian cancer (OV) from TCGA. The raw dataset includes 304 patients and 20,503 genes after removing the duplication and unknown gene names. The raw clinical data included 586 patients. We cleaned the clinical survival data from several clinical files, and obtained 582 patients with clear survival information. We merged the individuals both with gene expression data and survival information, and obtained 304 patients with 20,503 genes for further analysis. First, we filtered the genes with expressions values less than 10. Then, genes with more than 30% of zero expression values in the dataset were removed. Furthermore, we calculated the coefficient of variance (CV) of expression values for each gene, and kept the genes with CV of larger than 20% quantile. After these steps, 304 patients with 14,265 genes were included in our analysis. The censoring ratio was 39.5%.We mapped these genes to 271 pathways including 4260 genes.The raw expression data contains 578 patients and 20,530 genes. After removing the duplication and unknown gene names, there are 516 patients with 20,501 used for further analysis. The raw clinical data included 521 patients. We cleaned the clinical data with clear survival records, and included 497 patients in our analysis. We then merged the clinical data and expression data, and obtained 491 patients for with 20,501 genes for quality control. Similar with the steps for ovarian cancer dataset, we filtered the genes with expressions values less than 10. Then, we removed genes with more than 30% of zero expression values in the dataset. Furthermore, we calculated the coefficient of variance (CV) of expression values for each gene, and kept the genes with CV of larger than 20% quantile. After these steps, 491 patients with 14,143 genes were included in our analysis. The censoring ratio was 68.4%. We mapped these genes to 274 pathways including 4266 genes.The raw expression data contains 1220 patients and 20,530 genes. After removing the duplication and unknown gene names, there are 1097 patients with 20,503 used for further analysis. The raw clinical data included 1097 patients. We cleaned the clinical data with clear survival records, and included 1084 patients in our analysis. We then merged the clinical data and expression data, and obtained 1082 patients for with 20,503 genes for quality control. The same steps used here for breast cancer dataset, we filtered the genes with expressions values less than 10, and removed genes with more than 30% of zero expression values in the dataset. Furthermore, we calculated the coefficient of variance (CV) of expression values for each gene, and kept the genes with CV of larger than 20% quantile. After these steps, 1082 patients with 14,077 genes were included in our analysis. The censoring ratio was 86.0%. We mapped these genes to 275 pathways including 4385 genes.There were about one third genes were mapped to pathways for the above three real datasets. The rest genes were put together as an additional group. The detailed information of genes shared by different pathways is listed in Additional file 8: S1, S2, and S3, for ovarian cancer, lung cancer and breast cancer, respectively.Real data analysis is to build a survival model for predicting the overall survival outcome by integrating gene expression data and pathway information. We standardized all the predictors to use a common scale for all predictors, prior to fitting the models, using the function covariate() function in BhGLM package. In our prior distribution, there were to preset parameters, (s0, s1). In our real data analysis, we fixed the slab scale s1 to 1, and varied the spike scale s0 values by: {k\u2009\u00d7\u20090.01;\u2009k\u2009=\u20091,\u2009\u2026,\u20099}, leading to 9 models. The optimal spike scale s0 was select by the 10-fold 10-time cross-validation according to the CVPL. Using the optimal s0, we performed further real data analysis. For comparison, we also analyzed the data using the several existing methods as described in the simulation studies.The pathway enrichment analyses for these detected genes were summarized in Additional file 10: S4-S6. Additional file 11: S7 presents the genes detected by the proposed and existed methods. Their standardized effects size were also plotted for the three real data sets. There were many common gene among different methods. For ovarian cancer dataset, the genes CYP2R1 and HLA-DOB detected by the proposed gsslasso method, were also detected by both lasso and cMCP methods. For Lung cancer dataset, several genes detected by the proposed gsslasso method, such as VDAC1, EHHADH, ACAT2, KIT, CCND1, PIK3R1, NRAS, GNPNAT1, and KYNU, were also detected by other method. For, breast cancer dataset, two genes HSPA1A and ABCB5 detected by the proposed gsslasso method were also detected by other method.We found that most of the genes detected by the proposed method were associated with cancers in previous studies. HABP2, detected in ovarian cancer, was associated with familial nonmedullary thyroid cancer [47]. CYP24A1, the main enzyme responsible for the degradation of active vitamin D, plays an important role in many cancer related cellular processes. The associations between CYP24A1 polymorphisms and cancer susceptibility had been evaluated by many studies [48]. Keratin 8 (KRT8) plays an essential role in the development and metastasis of multiple human cancers. A recent study suggested that in clear cell renal cell carcinoma and gastric cancer, KRT8 upregulation promotes tumor metastasis and associated with poor prognosis [49, 50]. E2F7, detected in lung cancer, involved in several cancer studies, which might act as an independent prognostic factor for breast cancer, and Squamous Cell Carcinoma, and gliomas [51\u201353]. Most of the genes detected by the proposed method in the three real datasets had been found associated with different cancers. These results may provide some interesting information for further studies.The group structures among various features arise naturally in many biological and medical researches, especially in large-scale omics data. Such grouping information is biologically meaningful and intrinsically encoded in the biological data. Thus it is desirable to incorporate the grouping information into data analysis. Various penalization methods have been designed for such situations [13, 14, 30, 54, 55]. Recently, we have developed a novel hierarchical modeling approach, the group spike-and-slab lasso GLMs, to integrate the variable group information for gene detection and prognostic prediction [27]. In this study, we extended the method to Cox proportional hazards model for analyzing censored survival data.Similar to the group spike-and-slab lasso GLMs, the key to our group spike-and-slab lasso Cox is the group spike-and-slab double-exponential prior. This prior has significant advantage in variable selection and parameter estimation. It induces weak shrinkage on larger coefficients and strong shrinkage on irrelevant coefficients. In contrast, other methods usually gave a strong shrinkage amount on all the coefficients and resulted in the solutions that non-zero coefficients were shrunk and underestimated. The proposed group spike-and-slab prior allows the model to incorporate the biological similarity of genes within a same pathway into the analysis.The spike-and-slab prior depends on the spike and slab scale parameters. Our previous study suggested that slab scale s1 had little influence on model fitting, while the spike scale s0 strongly affected model performance [25, 26]. A slab scale s1 value introducing weak shrinkage amount would be helpful to include relevant variables into the model. Therefore, we set s1\u2009=\u20091 in our analysis. We evaluated the performance of the proposed model on a grid of values of spike scale s0 from a reasonable range, e.g., (0, 0.1), and then selecting an optimal value using cross-validation. This is a path-following strategy for fast dynamic posterior exploration of the proposed models, which is similar to the approach of Ro\u010dkov\u00e1 and George [24, 56]. Additional file 12: Figure S8 a and b show the solution paths under Scenario 2, for the proposed model and the lasso Cox model. Additional file 12: Figure S8 c and d show the profiles of cross-validated palatial log-likelihood by 10-fold cross-validation for the proposed model. These profiles would help to choose optimal tuning parameters. It could be found that, similar to the lasso, the spike-and-slab lasso Cox is a path-following strategy for fast dynamic posterior exploration. However, the solution path is essentially different from that of the lasso model. For the lasso cox model, the number of non-zero coefficient could be a few, even zero if a strong penalty is adopted. However, in the spike-and-slab lasso Cox model, larger coefficients will be always included in the model with weak shrinkage, while irrelevant coefficients are removed (grey path in Additional file 12: Figure S8 a).Another feature of the proposed spike-and-slab prior is bi-level selection, which is capable of selecting important groups as well as important individual variables within those groups. Several methods perform bi-level selection, including cMCP method [16], SGL [14], and group exponential lasso [18]. The underlying assumption is that the model is sparse at both the group and individual variable levels. The proposed group spike-and-slab lasso Cox model can efficiently perform bi-level selection. In group level, the importance of a group is controlled by the group-specific probability \u03b8g. Within a group, the spike-and-slab prior allows to perform variable selection by shrinking irrelevant or small effect coefficients exactly to zero, without affecting the prediction performance.The extensive simulation studies show that the prediction performance of the proposed method is always slightly better than cMCP method, and significantly better than all other methods under different scenarios. In the real data analysis, the prediction accuracy of the proposed method incorporating pathway information was slightly improved compared with the existing methods. This might be mainly due to the complex genetic components involved in the expression data, like haplotype blocks, subnetworks, and interaction among the genes. The present model under the linear assumption may not capture these complexities. More sophisticated strategies could potentially enhance prediction accuracy and further improve the models, by defining more precise biological grouping information.There are several further extensions of the proposed method. For example, it can also be extended to incorporate multiple level group structure, like three-level group structure, i.e. SNP-gene-pathway. In addition, the proposed model takes the spike-and-slab mixture double-exponential prior. Other priors with a spike at zero and includes heavier tails could be investigated, like Cauchy distribution, a special case of Student-t distribution. The theoretical and empirical properties of other priors are different, which may introduce more interesting results.Incorporating biological group structure in high-dimensional molecular data analysis can improve the accuracy of disease prediction and power of gene detection. We propose a new hierarchical Cox model, gsslasso Cox, for incorporating biological pathway information for predicting disease survival outcomes and detecting associated genes. We develop a fast and stable deterministic algorithm to fit the proposed models. Extensive simulation studies and real applications show that compared with several existing methods, the proposed approach provides more accurate parameter estimation and survival prediction. The proposed method has been implemented in a freely available R package BhGLM.Composite minimax concave penaltyCross-validated partial likelihoodGroup lassoGroup minimax concave penaltyGroup smoothly clipped absolute deviationGroup spike-and-slab lasso cox modelLeast absolute shrinkage and selection operatorThis work was supported in part by the National Natural Science Foundation of China (81773541 and 81573253), funds from the Priority Academic Program Development of Jiangsu Higher Education Institutions at Soochow University, grants from China Scholarship Council, Jiangsu Provincial Key Project in Research and Development of Advanced Clinical Technique (BL2018657) to ZT, USA National Institutes of Health (R03-DE024198, R03-DE025646) to NY. The funding body did not played any roles in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.All data for the manuscript will be available without restriction at website: https://cancergenome.nih.gov/. The package BhGLM is freely available from https://github.com/nyiuab/BhGLM.Not applicable.Not Applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.In Cox proportional hazards model, it usually assumes that the hazard function of survival time T takes the form [32, 33]: (1)where the baseline hazard function h0(t) is unspecified, X and \u03b2 are the vectors of explanatory variables and coefficients, respectively, and X\u03b2 is the linear predictor or called the prognostic index.Fitting classical Cox models is to estimate \u03b2 by maximizing the partial log-likelihood [34]: (2)where R(ti) is the risk set at time ti. In the presence of ties, the partial log-likelihood can be approximated by the Breslow or the Efron methods [35, 36]. The standard algorithm for maximizing the partial log-likelihood is the Newton-Raphson algorithm [32, 37].For high dimensional and/or correlated data, the classical model fitting is often unreliable. The problem can be solved by using Bayesian hierarchical modeling or penalization approaches [31, 38, 39]. We here propose a Bayesian hierarchical modeling approach, which allows us to simultaneously analyze numerous predictors and more importantly provides an efficient way to incorporate group information. Our hierarchical Cox models employ the spike-and-slab mixture double-exponential (de) prior on the coefficients: (3)where, s0 and s1 are the preset scale parameters, which are small and relatively large (0\u2009<\u2009s0\u2009<\u2009s1), inducing strong or weak shrinkage on \u03b2j, respectively. \u03b3j is the indicator variable: \u03b3j = 1 or 0. Equivalently, this prior can be expressed as (1 - \u03b3j) de(0, s0)\u2009+\u2009\u03b3j de(0, s1), a mixture of the shrinkage prior de(0, s0) and the weakly informative prior de(0, s1), which are the spike and slab components of the prior distribution, respectively.We incorporate the group structure by proposing a group-specific Berllouli distribution for the indicator variables. For predictors in group g, the indicator variables are assumed to follow the Berllouli distribution with the group-specific probability \u03b8g: (4)We have developed a fast deterministic algorithm, called the EM coordinate descent algorithm to fit the spike-and-slab lasso Cox models by estimating the posterior modes of the parameters [26]. The EM coordinate descent algorithm incorporates EM steps into the cyclic coordinate descent procedure for fitting the penalized lasso Cox models, and has been shown to be fast and efficient for analyzing high-dimensional survival data [26]. We here extend the EM coordinate descent algorithm to fit the group spike-and-slab lasso Cox models. We derive the algorithm based on the log joint posterior density of the parameters\u03d1\u2009=\u2009(\u03b2,\u2009\u03b3,\u2009\u03b8): (5)The log-likelihood function, logp(t,\u2009d|\u2009\u03b2,\u2009h0), is proportional to the partial log-likelihood pl(\u03b2) defined in Eq. (2) or the Breslow or the Efron approximation in the presence of ties [35, 36], if the baseline hazard function h0 is replaced by the Breslow estimator [37, 40]. Therefore, the log joint posterior density can be expressed as (6)where pl(\u03b2) is the partial likelihood described in (2), and Sj\u2009=\u2009(1\u2009\u2212\u2009\u03b3j)s0\u2009+\u2009\u03b3js1.In EM coordinate decent algorithm, the indicator variables \u03b3j were treated the as \u2018missing values\u2019. The parameters (\u03b2, \u03b8) were estimated by averaging the missing values over their posterior distributions. For the E-step, the expectation of the log joint posterior density was calculated with respect to the conditional posterior distributions of the missing data. For predictors in group g, the conditional posterior expectation of the indicator variable \u03b3j can be derived as (7)where p(\u03b3j\u2009=\u20091|\u2009\u03b8g)\u2009=\u2009\u03b8g, p(\u03b3j\u2009=\u20090|\u2009\u03b8g)\u2009=\u20091\u2009\u2212\u2009\u03b8g, p(\u03b2j|\u2009\u03b3j\u2009=\u20091,\u2009s1)\u2009=\u2009de(\u03b2j|\u20090,\u2009s1) and p(\u03b2j|\u2009\u03b3j\u2009=\u20090,\u2009s0)\u2009=\u2009de(\u03b2j|\u20090,\u2009s0). Therefore, the conditional posterior expectation of \\( {S}_j^{-1} \\) can be obtained by (8)For the M-step, parameters (\u03b2, \u03b8) were updated by maximizing the posterior expectation of the log joint posterior density with \u03b3j and \\( {S}_j^{-1} \\) replaced by their conditional posterior expectations. From the log joint posterior density, we can see that \u03b2 and \u03b8 can be updated separately, because the coefficients \u03b2 are only involved in\\( pl\\left(\\beta \\right)-{\\sum}_{j=1}^J{S}_j^{-1}\\left|{\\beta}_j\\right| \\) and the probability parameter \u03b8 is only in \\( {\\sum}_{j=1}^J\\left({\\gamma}_j\\log {\\theta}_g+\\left(1-{\\gamma}_j\\right)\\log \\left(1-{\\theta}_g\\right)\\right)+{\\sum}_{g=1}^G\\left(\\left(a-1\\right)\\log {\\theta}_g+\\left(b-1\\right)\\log \\left(1-{\\theta}_g\\right)\\right) \\). Therefore, the coefficients \u03b2 are updated by maximizing the expression: (9)where \\( {\\widehat{S}}_j^{-1} \\) is the conditional posterior expectation of \\( {S}_j^{-1} \\) as derived above. Given the scale parameters Sj, the term \\( {\\sum}_{j=1}^J{\\widehat{S}}_j^{-1}\\left|{\\beta}_j\\right| \\) serves as the L1 lasso penalty with \\( {\\widehat{S}}_j^{-1} \\) as the penalty factors, and thus the coefficients can be updated by maximizing Q1(\u03b2) using the cyclic coordinate decent algorithm, which is extremely fast and can estimate some coefficients exactly to zero [31, 41]. The probability parameters {\u03b8g} are updated by maximizing the expression: (10)We can easily obtain: (11)where Jg is the number of predictors belonging to group g.Totally, the framework of the proposed EM coordinate decent algorithm was summarized as follows:1)\nChoose a starting value for \u03b20, and \\( {\\theta}_g^0 \\). For example, we can initialize \u03b20\u2009=\u20090, and \\( {\\theta}_g^0=0.5 \\).\n\u00a02)\nFor t\u2009=\u20091, 2, 3, \u2026,\n\u00a0M-step:a)\nUpdate \u03b2 using the cyclic coordinate decent algorithm;\n\u00a0b)\nUpdate (\u03b81, \u22ef, \u03b8G) by Eq. (11).\n\u00a0Moreover, we also use an alternative way to evaluate the partial log-likelihood, i.e., the so-called cross-validated partial likelihood (CVPL), defined as [37, 41, 42]. (12)where \\( {\\widehat{\\beta}}_{\\left(-k\\right)} \\) is the estimate of \u03b2 from all the data except the k-th part, \\( pl\\left({\\widehat{\\beta}}_{\\left(-k\\right)}\\right) \\) is the partial likelihood of all the data points and \\( {pl}_{\\left(-k\\right)}\\left({\\widehat{\\beta}}_{\\left(-k\\right)}\\right) \\) is the partial likelihood excluding part k of the data. By subtracting the log-partial likelihood evaluated on the non-left out data from that evaluated on the full data, we can make efficient use of the death times of the left out data in relation to the death times of all the data.We assessed the proposed approach by extensive simulations, and compared with the lasso implemented in the R package glmnet and several penalization methods that can incorporate group information, including sparse group lasso (SGL) in the R package SGL, overlap group lasso (grlasso), overlap group MCP (grMCP), overlap group SCAD (grSCAD), and overlap group composite MCP (cMCP) in the R package grpregOverlap [45]. Our simulation method was similar to our previous work [26, 27]. We considered five simulation scenarios with different complexities, including non-overlap or overlap groups, group sizes, number of non-null groups, and correlation coefficients (r) (Table\u00a01). In simulation scenario 2\u20135, overlap structures were considered. To handle the overlap structures, we duplicated overlapping predictors into groups that predictors belong to [28, 30]. In each scenario, we simulated two data sets, and used the first one as the training data to fit the models and the second one as the test data to evaluate the predictive values. We replicated the simulation 100 times and summarized the results over these replicates. In simulation scenario 6, we vary the effect size of the non-zero coefficient \u03b25, from \u2212\u20092 to 2. Other simulation setting are the same with scenario 2. The purpose of this simulation is to see the profile of prior scale along with varying effect size.Table 1\nThe preset non-zero predictors and their assumed effect values of the different simulation scenarios\n\nNote:{} quotes the predictors within a group.\nIn this scenario, each group is independent. There was no any overlap among groups. Eight non-zero predictors{x5,\u2009x20,\u2009x40}, \u2002{x210,\u2009\u2002x220,\u2009\u2002x240}, \u2002{x975,\u2009\u2002x995} were simulated to be included into three groups, group 1, 5, and 20 (Table 1). The group sizes is 50, including 50 predictors, presented as below:In this scenario, overlapped grouping structure was considered. Only the last group is independent. For example, for group 1 and group 2, there were five predictors (x46, x47, x48, x49, x50) belong to two groups. The setting for eight non-zero predictors and their effect sizes are the same with scenario 1. The group sizes is still 50. The overlap structure are presented below:Group size means the number of predictors included in a group. A big group size means the group included relative more predictors. The group size may affect the model fitting. In this scenario, we assumed two groups, group 1 and 11, including non-zero predictors, {x1,\u2009x2,\u2009x3,\u2009x4} and {x501,\u2009x502,\u2009x503,\u2009x504}, respectively. Other simulation setting are similar with scenario 2. To investigate the group size effect on model fitting, we simulated different group size as below:(1).\nonly four non-zero predictors included in group 1 and 11:\n\u00a0\n(2).\n20 predictors included in group 1 and 11:\n\u00a0\n\n\n(3).\n50 predictors included in group 1 and 11:\n\u00a0\n\nThe true non-zero predictors may be included in some groups. Other zero predictors belong to other groups. These groups included non-zero predictors called non-null group. The number of non-null group may also affect the model fitting. To evaluate the group number effect, we varied the number of non-null groups, as following:(1).\nThere are 8 non-null groups including non-zero coefficients: {x5}, {x55}, {x305}, {x355}, {x505}, {x555}, {x905}, and\u00a0{x955};\n\u00a0(2).\nThere are 3 non-null groups including non-zero coefficients: {x5,\u2009x15,\u2009x25}, {x355,\u2009x365,\u2009x375}, and\u00a0{x905,\u2009x915};\n\u00a0(3).\nThere is only 1 non-null group including non-zero coefficients: {x5,\u2009x10,\u2009x15,\u2009x20,\u2009x25,\u2009x30,\u2009x35,\u2009x40}. The overlap settings were the same with scenario 2. The group number and effect sizes of these non-zero coefficients are shown in Table 1.\n\u00a0Tables\u00a02 and 3 summarizes the CVPL (cross-validated partial likelihood) and C-index in the testing data over 100 replicates for Scenarios 1\u20135. We observed that the group spike-and-slab lasso Cox model performed similarly with cMCP and outperformed other methods, under different simulation scenarios. These results suggested that, with complex group structures, the proposed method could perform well.Table 2\nEstimates of two measures over 100 replicates under simulation scenario 1 and 2\n\nNote: Values in the parentheses are standard deviations. \u201cgsslasso\u201d represents the proposed group spike-and-slab lasso cox. The slab scales, s1, are 1 in the analyses. The optimal s0\u2009=\u20090.02 and s0\u2009=\u20090.03 for gsslasso cox methods under scenario 1 and 2, respectively. For scenarios with overlap structures, SGL method was not used for comparison since it cannot handle overlap situation directly\nTable 3\nEstimates of two measures over 100 replicates for varying group size and varying number of non-null group under simulation scenario 3,4 and 5, respectively\n\nNotes: in scenario 3, group size \u201c4/50\u201d denotes that there are four none-zero coefficients embedded in a group with 50 predictors. The group size is 50. This is true for \u201c4/20\u201d and \u201c4/4\u201d. The optimal s0\u2009=\u20090.02 for different group size settings. In scenario 4, \u201c8/20\u201d denotes that there are 8 non-null groups among 20 groups. Each non-null group includes at least one non-zero coefficients. The optimal s0\u2009=\u20090.02 for the three settings. In scenario 5, the optimal s0 are 0.02, 0.03, and 0.04 for different correlation coefficients, 0.0, 0.5, and 0.7 within group, respectively. The slab scales, s1, are 1 in this scenario 3 4, and 5. Values in the parentheses are standard errors. \u201cgsslasso\u201d represents the proposed group spike-and-slab lasso cox\nTo evaluate the accuracy of parameters estimation, we summarized the average numbers of non-zero coefficients and the mean absolute errors (MAE) of coefficient estimates, defined as MAE = \\( \\sum \\left|{\\widehat{\\beta}}_j-{\\beta}_j\\right|/m \\), in Tables\u00a04 and 5 for different scenarios. It was found that the dected number of null-zero coefficients were very close preset number 8, and the values of MAE were very small for the proposed method under different scenarios. The performances of the group spike-and-slab lasso Cox and cMCP were consistently better than the other methods for all the five scenarios, and the proposed method was slightly better than cMCP. These results suggested that the proposed method can generate lowest false positive and unbiased estimation.Table 4\nAverage number of non-zero coefficients and mean absolute error (MAE) of coefficient estimates over 100 simulations for scenario 1 and 2\n\n*: the optimal s0\u2009=\u20090.02 and s0\u2009=\u20090.03 for gsslasso method under scenario 1 and 2, respectively. For scenarios with overlap structures, SGL method was not used for comparison since it cannot handle overlap situation directly\nTable 5\nAverage number of non-zero coefficients and mean absolute error (MAE) of coefficient estimates over 100 simulations for scenario 3, 4, and 5\n\nNotes: in scenario 3, group size \u201c4/50\u201d denotes that there are four none-zero coefficients embedded in a group with 50 predictors. The group size is 50. This is true for \u201c4/20\u201d and \u201c4/4\u201d. The optimal s0\u2009=\u20090.02 for different group size settings. The slab scales, s1, are 1 in this scenario. In scenario 4 \u201c8/20\u201d denotes that there are 8 non-null groups among 20 groups. Each non-null group includs at least one non-zero coefficients. The optimal s0\u2009=\u20090.02 for the three settings. In scenario 5, the optimal s0 are 0.02, 0.03, and 0.04 for different correlation coefficients, 0.0, 0.5, and 0.7 within group, respectively. The slab scales, s1, are 1 in this scenario 3, 4 and 5. Values in the parentheses are standard errors. \u201cgsslasso\u201d represents the proposed group spike-and-slab lasso cox\nThe estimates of coefficients from the group spike-and-slab lasso Cox and the other methods over 100 replicates are shown in Fig.\u00a01 and Additional file 1: Figure S1, Additional file 2: Figure S2, Additional file 3: Figure S3, Additional file 4: Figure S4, Additional file 5: Figure S5, Additional file 6: Figure S6 and Additional file 7: Figure S7 for different scenarios. It can be seen that the group spike-and-slab lasso Cox method produced estimates close to the simulated values for all the coefficients. This is expected, because the spike-and-slab prior can induce weak shrinkage on larger coefficients and strong shrinkage on zero coefficients. In contrast, other methods except for cMCP, gave a strong shrinkage amount on all the coefficients and resulted in the solutions that non-zero coefficients were shrunk and underestimated compared to true values. In addition, higher false positives (grey bars) were observed, except for the group spike-and-slab lasso Cox and cMCP methods.\n\nFig. 1\nThe parameter estimation averaged over 100 replicates for the group spike-and-slab lasso Cox (gsslasso), the lasso, grlasso, grMCP, grSCAD, SGL and cMCP methods for Scenario 1. Blue cycles are the simulated non-zero values. Black points and lines represent the estimated values and the interval estimates of coefficients\nTo show the self-adaptive shrinkage feature, we performed simulation 6. Figure\u00a02 shows the adaptive shrinkage amount on non-zero coefficients x5, along with the varying effect size. It clearly shows that the proposed spike-and-slab lasso Cox model approach has self-adaptive and flexible characteristics, without affecting the nearby zero coefficient (x6) and non-zero variable (x20) belong to the same group.\n\nFig. 2\nThe adaptive shrinkage amount, along with the varying effect size for x5. The x20 is the none-zero coefficients with the simulated effect size \u2212\u20090.7, while x6 is the nearby coefficient with simuated zero effect size\nWe performed 10-fold cross-validation with 10 replicates to evaluate the predictive values of the several models. Table\u00a06 summarizes the measures of the prognostic performance on these three data sets, by only using the genes included in pathway. For all the data sets the proposed group spike-and-slab lasso Cox model performed better than the other methods. The above results used only genes mapped in pathways. Additional file 9 shows the measures of the performance on these three data sets, by using the all genes. The genes which were not mapped into any pathway were put together as an additional group. We can see that the prediction performance of the proposed method were still better than the other methods.Table 6\nThe measures of optimal group spike-and-slab lasso (gsslasso) cox and the lasso cox models for TCGA ovarian cancer, lung adenocarcinoma (LUAD) and breast cancer dataset with pathway genes by 10 times 10-fold cross validation\n\nNote: Values in the parentheses are standard errors. For group spike-and-slab lasso model, the optimal s0\u2009=\u20090.03 for three data sets. In TCGA ovarian cancer, we mapped 4260 genes into 271 pathways. The analyses was performed on these genes including in these pathways. The same is true for other two datasets\n\n\nAdditional file 1:\nFigure S1. The parameter estimation averaged over 100 replicates for the group spike-and-slab lasso Cox (gsslasso), the lasso, grlasso, grMCP, grSCAD, and cMCP methods for Scenario 2. Blue cycles denote the simulated non-zero values. Black points and lines represent the estimated values and the interval estimates of coefficients. (PDF 523 kb)\n\n\n\nAdditional file 2:\nFigure S2. The parameter estimation averaged over 100 replicates for the group spike-and-slab lasso Cox (gsslasso), the lasso and grlasso methods for Scenario 3. Blue cycles denote the simulated non-zero values. Black points and lines represent the estimated values and the interval estimates of coefficients. The main title of each plot denotes the varying group size for scenario 3. (PDF 778 kb)\n\n\n\nAdditional file 3:\nFigure S3. The parameter estimation averaged over 100 replicates for grMCP, grSCAD, and cMCP methods for Scenario 3. Blue cycles denote the simulated non-zero values. Black points and lines represent the estimated values and the interval estimates of coefficients. The main title of each plot denotes the varying group size for Scenario 3. (PDF 767 kb)\n\n\n\nAdditional file 4:\nFigure S4. The parameter estimation averaged over 100 replicates for the group spike-and-slab lasso Cox (gsslasso), the lasso and grlasso methods for Scenario 4. Blue cycles denote the simulated non-zero values. Black points and lines represent the estimated values and the interval estimates of coefficients. The main title of each plot denotes the varying the number of non-null group for Scenario 4. (PDF 791 kb)\n\n\n\nAdditional file 5:\nFigure S5. The parameter estimation averaged over 100 replicates for grMCP, grSCAD, and cMCP for Scenario 4. Blue cycles denote the simulated non-zero values. Black points and lines represent the estimated values and the interval estimates of coefficients. The main title of each plot denotes the varying the number of non-null group for Scenario 4. (PDF 783 kb)\n\n\n\nAdditional file 6:\nFigure S6. The parameter estimation averaged over 100 replicates for the group spike-and-slab lasso Cox (gsslasso), the lasso and grlasso methods for Scenario 5. Blue cycles denote the simulated non-zero values. Black points and lines represent the estimated values and the interval estimates of coefficients. The main title of each plot denotes the varying the number of non-null group for Scenario 5. (PDF 1054 kb)\n\n\n\nAdditional file 7:\nFigure S7. The parameter estimation averaged over 100 replicates for grMCP, grSCAD, and cMCP for Scenario 5. Blue cycles denote the simulated non-zero values. Black points and lines represent the estimated values and the interval estimates of coefficients. The main title of each plot denotes the varying the number of non-null group for Scenario 5. (PDF 778 kb)\n\n\n\nAdditional file 8:\nS1, S2, and S3. The detailed information of genes shared by different pathways for ovarian cancer, lung cancer and breast cancer, respectively. (ZIP 213 kb)\n\n\n\nAdditional file 9:\nTable S1. The measures of optimal group spike-and-slab lasso (gsslasso) cox and the lasso cox models for TCGA ovarian cancer, lung adenocarcinoma (LUAD) and breast cancer dataset with all genes by 10 times 10-fold cross validation. (DOCX 20 kb)\n\n\n\nAdditional file 10:\nS4, S5 and S6. The pathway enrichment analyses for these detected genes for ovarian cancer, lung cancer and breast cancer, respectively. (ZIP 15 kb)\n\n\n\nAdditional file 11:\nS7. The detected genes and their standardized effect sizes estimated by the group spike-and-slab lasso Cox model and five existed methods for TCGA real datasets. (PDF 1340 kb)\n\n\n\nAdditional file 12:\nFigure S8. The solution path and cross-validated partial loglikelihood profiles of the group spike-and-slab lasso Cox (a, c) and the lasso (b, d) based on the Scenario 2. The colored points on the solution path represent the estimated values of assumed eight non-zero coefficients, and the circles represent true non-zero coefficients. Vertical lines correspond to the optimal models. (PDF 1052 kb)", "s12859-019-2672-1": "Ligand-binding proteins play key roles in many biological processes. Identification of protein-ligand binding residues is important in understanding the biological functions of proteins. Existing computational methods can be roughly categorized as sequence-based or 3D-structure-based methods. All these methods are based on traditional machine learning. In a series of binding residue prediction tasks, 3D-structure-based methods are widely superior to sequence-based methods. However, due to the great number of proteins with known amino acid sequences, sequence-based methods have considerable room for improvement with the development of deep learning. Therefore, prediction of protein-ligand binding residues with deep learning requires study.In this study, we propose a new sequence-based approach called DeepCSeqSite for ab initio protein-ligand binding residue prediction. DeepCSeqSite includes a standard edition and an enhanced edition. The classifier of DeepCSeqSite is based on a deep convolutional neural network. Several convolutional layers are stacked on top of each other to extract hierarchical features. The size of the effective context scope is expanded as the number of convolutional layers increases. The long-distance dependencies between residues can be captured by the large effective context scope, and stacking several layers enables the maximum length of dependencies to be precisely controlled. The extracted features are ultimately combined through one-by-one convolution kernels and softmax to predict whether the residues are binding residues. The state-of-the-art ligand-binding method COACH and some of its submethods are selected as baselines. The methods are tested on a set of 151 nonredundant proteins and three extended test sets. Experiments show that the improvement of the Matthews correlation coefficient (MCC) is no less than 0.05. In addition, a training data augmentation method that slightly improves the performance is discussed in this study.Without using any templates that include 3D-structure data, DeepCSeqSite significantlyoutperforms existing sequence-based and 3D-structure-based methods, including COACH. Augmentation of the training sets slightly improves the performance. The model, code and datasets are available at https://github.com/yfCuiFaith/DeepCSeqSite.Benefiting from the development of massive signature sequencing, protein sequencing is becoming faster and less expensive. By contrast, owing to the technical difficulties and high cost of experimental determination, the structural details of only small parts of proteins are known in terms of protein-ligand interaction. Both biological and therapeutic studies require accurate computational methods for predicting protein-ligand binding residues [1].The primary structure of a protein directly determines the tertiary structure, and the binding residues of proteins are closely bound with the tertiary structure. These properties of proteins ensure the feasibility of predicting binding residues from amino acid sequences (primary structures) or 3D structures. However, the complex relationship between binding residues and structures is not completely clear. Thus, we have motivation for using machine learning in binding residue prediction, which is based on the unknown complex mappings from structures to binding residues.The existing methods for computational prediction of protein-ligand binding residues can be roughly categorized as sequence-based [2\u20135] or 3D-structure-based methods [1, 6\u201311]. The fundamental difference between the two types of methods is whether 3D-structure data are used. Some consensus approaches comprehensively consider the results of several methods. These methods can be seen as 3D-structure-based methods if any submethod uses 3D-structure data. Up to now, 3D-structure-based methods have been shown to be widely superior to sequence-based methods in a series of binding residue prediction tasks [1, 11]. However, 3D-structure-based methods depend on a large number of 3D-structure templates for matching. The time cost of template matching for a protein can reach several hours in a distributed environment. Furthermore, the number of proteins with known amino acid sequence is three orders of magnitude higher than that of proteins with known 3D structures. The enormous disparity in these quantities leads to difficulties in effectively utilizing 3D-structure information and massive sequence information together, which limits further progress in binding residue prediction.A series of traditional machine learning methods have been used in binding residue prediction. Many computational methods based on support vector machines (SVM) have been proposed for specific types of binding residue prediction [12\u201315]. A traditional BP neural network has been used in protein-metal binding residue prediction, but the network has considerable room for improvement [16]. Differing in interpretability from the mentioned methods, a robust method based on a Bayesian classifier has been developed for zinc-binding residue prediction [17]. Many methods based on template matching achieve considerable success at the expense of massive computational complexity [1, 10, 11]. A representative consensus approach, COACH, combines the prediction results of TM-SITE, S-SITE, COFACTOR, FINDSITE and ConCavity, some of which are 3D-structure-based methods [1, 6, 7, 10, 11]. This robust approach to protein-ligand binding residue recognition substantially improves the Matthews correlation coefficient (MCC). These methods have achieved successful results on small datasets. However, the methods would achieve even higher accuracy if massive data could be further utilized. One crucial factor for the available utilization of massive data is the representation capability of classifiers, which has a dominant impact on generalization.Deep neural networks have achieved a series of breakthroughs in image classification, natural language processing and many other fields [18\u201321]. In bioinformatics, deep neural networks have been applied in many tasks, including RNA-protein binding residue prediction, protein secondary structure prediction, compound-protein interaction prediction and protein contact map prediction [22\u201325]. Various recurrent networks are commonly used in sequence modeling [26, 27]. Context dependencies universally existing in sequences can be captured effectively by recurrent networks, and these networks are naturally suitable for variable-length sequences. Nevertheless, recurrent networks depend on the computations of the previous time step, which blocks parallel computing within a sequence. To solve this problem, convolutional neural networks are introduced into neural machine translation (NMT) [28, 29]. These architectures are called temporal convolution networks (TCN). In contrast to recurrent networks, the computation within a convolutional layer does not depend on the computation of the previous time step, so the calculation of each part is independent and can be parallelized. Convolutional sequence-to-sequence models outperform mature recurrent models on very large benchmark datasets by an order of magnitude in terms of speed and have achieved the state-of-the-art results on several public benchmark datasets [29]. Many similarities exist between NMT and binding residue prediction. The performance of binding residue prediction can be improved with progress in NMT.In this study, we propose a new approach, DeepCSeqSite (DCS-SI), for protein-ligand binding residue prediction. The architecture of DCS-SI is inspired by a series of sequence-to-sequence models including ConvS2SNet [29]. DCS-SI includes two editions: stdDCS-SI and enDCS-SI. The encoders of the two editions are the same. The decoder of enDCS-SI evolves from the decoder of stdDCS-SI. The former executes forward propagation twice and takes the previous output into consideration to produce more accurate predictions. In DCS-SI, the fully convolutional architecture contributes to improving parallelism and processing variable-length inputs. Several convolutional layers are stacked on top of each other to extract hierarchical features. The low-level features reflect local information over residues near the target while the high-level features reflect global information over a long range of an amino acid sequence. Correspondingly, the size of the effective context scope is expanded as the number of layers increases. The long-distance dependencies between the residues can be captured by an effective context scope that is sufficiently large. A simple gating mechanism is adopted to select relevant residues. Templates are not used in DCS-SI. The network in DCS-SI is trained only on sequence information. The state-of-the-art ligand-binding method COACH and some of its submethods are selected as baselines. Experiments show that stdDCS-SI and enDCS-SI significantly outperform the baselines.The datasets used in this study are collected from the BioLip database and the previous benchmarks [1, 11]. Our training sets contain binding residues of fourteen ligands (ADP, ATP, Ca 2+, Fe 3+, FMN, GDP, HEM, Mg 2+, Mn 2+, Na +, NAD, PO\\(_{4}^{3-}\\), SO\\(_{4}^{2-}\\), Zn 2+)1. A total of 151 proteins are selected from the previous benchmarks with the fourteen ligands as the benchmark testing set, called SITA. Every protein in the training sets has a sequence identity to the proteins in the validation sets and testing sets of less than 40% [13]. To obtain as much data as possible for training, the pairwise sequence identity is allowed to be 100% in the training sets. We speculate that the augmented training sets (Aug-Train) can drive networks to achieve better generalization performance.Considerable data skew generally exists in protein-ligand binding residue prediction. ADP, ATP, FMN, GDP, HEM and NAD have more binding residues than do metal ions and acid radical ions, which means that the substantial data skew is attributed more to metal ions and acid radical ions. The computational binding residue prediction of metal ions and acid radical ions is still difficult because of the small size and high versatility. To demonstrate the ability of the models to predict the binding residues of metal ions and acid radical ions, we extend SITA with metal ions and acid radical ions. Every protein in the testing sets has a sequence identity to the proteins in the training sets and the other testing sets of less than 40%. Furthermore, the extended testing sets (SITA-EX1, SITA-EX2 and SITA-EX3) reduce the variance in the tests.Each residue in an amino acid sequence plays a specific role in the structure and function of a protein. For a target residue, nearby residues in the tertiary structure plausibly affect whether the target residue is a binding residue for some ligand. Thus, residues near the target residue in the tertiary structure but far from the target residue in the primary structure are critical to binding residue prediction. Most of the existing methods use a sliding window centered at the target residue to generate overlapping segments for every target protein sequence [13, 16, 30]. The use of sliding windows is a key point in converting several variable-length inputs into segments of equal length. However, even if the distance in the sequence between two residues is very long, their spatial distance can be limited because of protein folding. Thus, residues far from the target residue in the sequence may also have an important impact on the location of the binding residues. To obtain more information, these methods have to increase the window size in the data preprocessing stage. The cost of computation and memory for segmentation is not acceptable when the window size increases to a certain width.On the basis of the inspiration from NMT, protein-ligand binding residue prediction can be seen as a particular form of translation. The main differences are the following two aspects: 1. For NMT, the elements in the destination sequences are peer entities to the elements in the source sequences, but the binding site labels are not peer entities to the residues. 2. While the destination and source sequences typically differ in length for NMT, a one-to-one match between each binding residue label and each residue exists. Despite the differences, binding residue prediction can learn from NMT. The foundation of feature extraction in NMT includes local correlation and long-distance dependency, which are common in amino acid sequences and natural language sentences. Thus, the main idea of feature extraction in NMT is applicable to binding residue prediction.Seven types of features are used for the protein-ligand binding residue prediction: position-specific score matrix (PSSM), relative solvent accessibility (RSA), secondary structure (SS), dihedral angle (DA), conservation scores (CS), residue type (RT) and position embeddings (PE).where x is the dimension of the PSSM score and y is the corresponding PSSM feature. For a protein with L residues, the PSSM feature dimension is L\u221720.The RSA is predicted by SOLVE. The real value of RSA is generally converted to a Boolean value indicating whether the residue is buried (RSA <25%) or exposed (RSA >25%). However, the original value is retained so that the network in DCS-SI can learn more abundant features [31].The secondary structure is predicted by PSSpred. The secondary structure type (alpha-helix, beta-strand and coil) is represented by a real 3D value. Each dimension of the real 3D value is in the range of [0, 1] indicating the possibility of existence of the corresponding type [32].A real 2D value specifying the \u03d5/ \u03c8 dihedral angles is predicted by ANGLOR [33]. The values of \u03d5 and \u03c8 are normalized by Norm(x)=x/360.0.Conservation analysis is a widely used method for detecting ligand-binding residues [34, 35]. Ligand-binding residues tend to be conserved in evolution because of their functional importance [2]. The relative entropy (RE) and Jensen-Shannon divergence (JSD) scores of conservation are taken as features in this study.Some amino acids have a much higher binding frequency for the corresponding ligands than do other amino acids. Twenty amino acid residues and an additional dummy residue are numbered from 0 to 20. Then, the numbers representing residue type are restricted to the range of [0, 1] by dividing by the total number of the types.Position embeddings can carry information about the relative or absolute position of the tokens in a sequence [36]. Several methods have been proposed for position embeddings. Experiments with ConvS2SNet and Transformer show that position embeddings can slightly improve performance, but the difference among several position embedding methods is not clear [29, 36]. Therefore, a simple method for position embeddings is adopted in DCS-SI. The absolute positions of the residues are represented as P Ei=i/L, where P Ei of the i-th residue is limited to range [0, 1], and L is the length of the amino acid sequence.The effective context scope for the prediction result or hidden layer representation of a target residue is called the input field. The size of the input field is determined by the stacked convolutional layers instead of being explicitly specified. Stacking n convolutional layers with kernel width k and stride =1 results in an input field of 1+n(k\u22121) elements (including padded elements). The input field can easily be enlarged by stacking more layers, which enables the maximum length of the dependencies to be precisely controlled. The stacked convolutional layers have the ability to process variable-length input without segmentation, which significantly reduces the additional cost. Moreover, deeper networks can be constructed with the slow growth of parameters. However, many proteins have hundreds or even thousands of residues; thus, deep stacked convolutional layers or a very large kernel width is required for long-distance dependencies. The latter is unadvisable because padded elements in the input fields and the growth rate of parameters are incremental over kernel width. By contrast, going deeper enables the method to achieve the desired results.For the encoder network, each residue always has a representation during forward propagation. A group of k\u00d7d convolution kernels transforms the initial m\u00d7d feature map into m\u00d71\u00d72c, where 2c is the output channel number of the convolution kernels. Zero elements are padded at both sides of the initial feature map to maintain m. The transformation and padding aim to satisfy the input demands of the following layers and the feature extraction. The main process of the network can be separated into two stages. Each stage contains N BasicBlocks (described in \u201cBasicBlock\u201d section) that consist of multiple frequently used layers and are designed for cohesiveness and expandability. In each stage, blocks are stacked on top of each other to learn hierarchical features from the input of the bottom block. At the tops of each stage, additional layers are added to stabilize the gradients and normalize the outputs.For the decoder network, the representation of each residue is transformed into the distribution over possible labels. Following the two stages, two fully connected layers consisting of one-by-one (1\u00d71) convolution kernels are used for information interaction between channels. The numbers of output channels of these 1\u00d71 convolution kernels are set to c and 2. The number of elements represented by the output of each block or layer is the same as the number of initial input elements. The first fully connected layer is wrapped in dropout to prevent overfitting [37]. The output of the last fully connected layer is fed to a 2-way softmax classifier, which produces the distribution over the labels of positive and negative samples.where \u03b8 represents the weights in DCS-SI, {x(1),\u22ef,x(t)} is a set of t samples, {y(1),\u22ef,y(t)} is a set of corresponding labels (y(i)\u2208{0,1}) and \u03b3 is the coefficient of the L2 normalization \\(\\|\\theta \\|_{2}^{2}\\).We proposed enDCS-SI on the basis of stdDCS-SI. Note that the prediction of the other residues is called the context prediction. Although stdDCS-SI outperforms existing methods, the performance can be further improved if the context prediction is taken into consideration explicitly. To achieve this goal, we retained the encoder network and modified the decoder network. In addition to the output of the encoder network, the new decoder network receives the context prediction as input. A group of k\u00d72 convolution kernels transforms the context prediction into m\u00d71\u00d72c, where 2c is the number of output channels of the convolution kernels. The following process consists of two parallel stages with M blocks and additional layers (in this study, we use M=2). To extract the features from the left (right) context prediction, we remove 1 element from the end (start) of the context prediction. Then, the input of each convolutional layer is padded by k elements on the left (right) side. The extracted information of the left and right adjacent predictions is directly added to the output of the encoder, where the three tensors have the same shape. ConvS2SNet directly uses the labels as the context prediction during training. Therefore, the forward propagation in training operates in parallel with the sequence. However, no label exists for the input samples during testing. Thus, the prediction for each element is processed serially to generate the context prediction for the next element.where \u03c3 represents the sigmoid function. The output of GLU \\(g([A\\ B]\\!) \\in \\mathbb {R}^{m \\times 1 \\times c}\\) is one-half the size of Y and is the same as the input size of the convolution in BasicBlock.where T P is the number of binding residues predicted correctly, F P is the number of non-binding residues predicted as binding residues, T N is the number of non-binding residues predicted correctly and F N is the number of binding residues predicted as non-binding residues.Experiments indicate that DCS-SI can be optimized effectively on the training sets and achieve good generalization on the test sets without any sampling. Mini-batches are prone to contain only negative samples if the samples are grouped via inappropriate methods. This problem is unlikely to occur in our mini-batches because an amino acid sequence is treated as a unit during our grouping. The severe data skew can be overcome as long as the proportion of positive samples in every mini-batch is close to the actual level. The cost function is minimized through mini-batch gradient descent. With zero-padding, the feature maps of the proteins in a batch are filled to the same size to simplify the programming implementation. The coefficient \u03b3 of the L2-Norm is 0.2, and the dropout ratio is set to 0.5. All DCS-SI models are implemented with TensorFlow. The training process consists of three learning strategies to suit different training stages. The learning rate of each stage decreases exponentially after the specified number of iterations. The gradient may be very steep in the early stage because of the unpredictable error surface and weight initialization. Hence, to preheat the network, the initial learning rate of the first stage is set to a value that can adapt to a steep gradient. Due to the considerable data skew, the training algorithm tends to fall into a local minimum where the network predicts all inputs as negative examples. A conservative learning rate is not sufficient to escape from this type of local minimum. Therefore, the initial learning rate of the second stage can be increased appropriately to search better minimums and further reduce the time cost of training. A robust strategy is required at the end of training to avoid the strong sway phenomenon. The details of the learning strategies are available in our software package.All the features used in this study are obtained from sequence or evolution information through computational methods. However, noise is introduced by the predictions of some features, including secondary structures and dihedral angles. The performance of stdDCS-SI will improve if these features are more accurate.Data augmentation typically contributes to the generalization of deep neural networks. To achieve better generalization, we use redundant proteins to obtain the augmented training sets (Aug-Train). The pairwise sequence identity is allowed to be 100% in Aug-Train, which contains at least nine times as many proteins as the original training sets.The effective utilization of data contributes to the improvement. Traditional classifiers are used in many existing methods, where the classifiers include SVM and traditional artificial neural networks (ANN). The input features for these classifiers are designed manually, and transformations in these classifiers focus on how to separate the input samples. Further feature extraction is inadequate, which limits the representation and generalization of these classifiers. Deep convolutional neural networks take advantage of massive sequence information. The hierarchical structure has the ability to extract low-level features and to organize low-level features as high-level features. The representation ability of the hierarchical features improves with the increase in layers, which requires sufficient data to ensure generalization. Currently, massive sequence information satisfies this requirement. In addition to the representation ability, the hierarchical structure provides the ability to capture long-distance dependencies. Without segmentation, the maximum distance of dependencies is not limited to the window size. Long-distance dependencies can be reflected in high-level features with a sufficiently large input field.Most traditional machine learning methods are sensitive to data skew, which fundamentally affects the generalization. The number of binding residues is far less than that of non-binding residues in our datasets, especially for metal ions and acid radical ions. The proportion of binding residues in the datasets is no more than 4%. We have attempted to replace the network in DCS-SI with SVMs. However, SVMs make the normal convergence on the unsampled training sets difficult. Even if the SVMs converge normally, their generalization is challenging. By contrast, the representation of DCS-SI is sufficiently strong to capture effective features for fitting and generalization without sampling. Training without sampling allows the network to learn more valid samples, which also contributes to the improvement.DCS-SI is better than the baselines in terms of predicting the binding residues of metal ions and acid radical ions. As shown in Table\u00a05, the performance of the baselines decreases when metal ions and acid radical ions are added to SITA. The decrease in MCC is 0.03 \u223c 0.04. Regardless, the performance of DCS-SI remains close to its original level, and the MCC of DCS-SI decreases by no more than 0.02. The contrast indicates that the superiority in predicting the binding residues of metal ions and acid radical ions is a direct source of the improvement.The convolutional architecture in DCS-SI provides the ability to process variable-length inputs.The hierarchical structure of the architecture enables DCS-SI to capture the long-distance dependencies between the residues, and the maximum length of the dependencies can be precisely controlled.Augmentation of the training sets slightly improves the performance, but the computational cost for training increases several times.Without using any template including 3D-structure data, DCS-SI significantly outperforms existing sequence-based and 3D-structure-based methods, including COACH.In future work, we plan to access the residues correlation at long distance by various attention mechanisms. Furthermore, the application of finite 3D-structure data to deep convolutional neural networks may effectively improve the protein-ligand binding residue prediction performance. Generative adversarial nets is a method that is worth applying to attempt to solve the severe deficiency of 3D-structure data relative to sequence data [40].Artificial neural networkConservation scoresDihedral angleDeepCSeqSiteen-DeepCSeqSiteGated linear unitsJensen-Shannon divergenceMatthews correlation coefficientNeural machine translationPosition embeddingsPosition-specific scoring matricesRelative entropyRelative solvent accessibilityResidue typeSecondary structuresstd-DeepCSeqSiteSupport vector machineTemporal convolution networkThis work was sponsored by the Peak Discipline Construction Project of Education at East China Normal University which provide the design of the study, the National Natural Science Foundation of China under grant nos. 61672234, U1401256, U1711262, 61402177 which support the collection, analysis, and interpretation of data, and the National Key Research and Development Program of China under grant no. 2016YFB1000905 which supports the writing and publish of the manuscript.The datasets generated and analysed during the current study are available in the DeepCSeqSite repository, https://github.com/yfCuiFaith/DeepCSeqSite.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.A summary of the datasets used in this study is shown in Table\u00a01. Severe data skew exists in the datasets, which restricts the optimization and performance of many machine learning algorithms. The data skew is considered in the design of DCS-SI.\nTable 1\nSummary of the datasets\n\n1 NProt: number of proteins\n2 NBR: number of binding residues\n3 NNBR: number of non-binding residues\n4 PBR: proportion of binding residues\n5Train: original training set\nIn the training sets, the binding residues that belong to any selected ligand type are labeled as positive samples, and the rest are labeled as negative samples. A deep convolutional neural network is trained as the classifier of stdDCS-SI or enDCS-SI, whose inputs are entire amino acid sequences. The input sequences are allowed to differ in length. The sequences are divided into several batches during training. In each batch, the sequences are padded to the length of the longest sequence in the batch with dummy residues. Batches are allowed to differ in length after padding. Each protein residue is embedded in a feature space consisting of several features to construct the input feature map for the classifier. For a given protein, every residue is predicted to be a binding residue or non-binding residue in the range of the selected ligand types simultaneously. The representation of dummy residues is removed immediately before the softmax layer. The method outline is shown in Fig.\u00a01. The details of the method are described in \u201cArchitecture\u201d section.\n\n\nFig. 1\nMethod Outline. Each residue in the amino acid sequence is embedded in a feature space that consists of seven types of features, namely, position-specific score matrix (PSSM), relative solvent accessibility (RSA), secondary structures (SS), dihedral angle (DA), conservation scores (CS), residue type (RT) and position embeddings (PE). The dimension number d of the feature space is 30. The amino acid sequence is transformed into a feature map as the input for the deep convolutional neural network, which outputs the result of the protein-ligand binding residue prediction. Each cell represents a dimension of the feature map\nPSSM is the probability of mutating to each type of amino acid at each position. Therefore, PSSM can be interpreted as representing conservation information. Normalized PSSM scores can be calculated as follows: \n (1)The architecture of the deep convolutional neural network is shown in Fig.\u00a02. The input to the network consists of m residues embedded in d dimensions. Due to the local correlation among the representations of adjacent residues, 1D convolution along the sequence is applied to the initial feature map and the hidden feature maps. The local correlation is based on the interaction among nearby residues and the covalent bond between adjacent residues.\n\n\nFig. 2\nArchitecture of the deep convolutional neural network in std-DeepCSeqSite (stdDCS-SI). Each cell represents a dimension of a representation. The m\u00d7d representation of an amino acid sequence is the input of the network, where m is the length of the amino acid sequence, and d is the dimension number of the feature space. Block (k\u00d71,2c) represents a BasicBlock with a k\u00d71 kernel size and 2c output channels, and the structure of Plain (k\u00d71,2c) is the same as that of Block (k\u00d71,2c) without residual connection. The situation of k=3, stride =1 and c=3 is described in this figure. Each m\u00d71 cell grid represents the output of a convolution kernel. The right-most representation is the input for the softmax\nThe cross entropy between the training data distribution and the model distribution is used in the following cost function: \n (2)To overcome the serialization in testing, we let enDCS-SI execute forward propagation in the decoder network 2 times. The first forward propagation is similar to that of stdDCS-SI, but the context prediction for enDCS-SI is fed by a zero tensor. The output of the first forward propagation is used as the context prediction for enDCS-SI in the second forward propagation. While training enDCS-SI, the context prediction is also replaced with the labels. All the weights in stdDCS-SI are loaded for enDCS-SI. The rest of the weights of enDCS-SI are initialized. The weights of the encoder network are fixed because the encoding processes of stdDCS-SI and enDCS-SI are the same. The architecture of enDCS-SI is described in Fig.\u00a03.\n\n\nFig. 3\nArchitecture of the deep convolutional neural network in en-DeepCSeqSite (enDCS-SI). The encoder of enDCS-SI is the same as that of stdDCS-SI. The decoder of enDCS-SI is designed to extract the information form the labels or the previous prediction. The decoder of stdDCS-SI is included in the decoder of enDCS-SI, where the weights of the former are fine-tuned during training enDCS-SI. \u2018p\u2019, \u2018s\u2019 and \u2018e\u2019 represent padding, start mark and end mark\nThe input of BasicBlock is processed in the order LN-GLU-Conv. The output of the l-th block is designated as \\(\\textbf {s}^{l}=\\left (s^{1},..., s^{m}\\right) \\in \\mathbb {R}^{m \\times 1 \\times 2c}\\), where m is the length of the input sequences2 and c is the number of input channels of convolutional layer in each block. The output of the l \u2212 1-th block is input to the l-th block. The input of each k\u00d71 convolution kernel is an m\u00d71\u00d7c feature map consisting of m input elements mapped to c channels. Before convolution, both ends of each channel are zero-padded with k/2 elements to maintain the height of the feature map, where the height is m. A convolutional layer with 2c output channels transforms the input of convolution \\(X \\in \\mathbb {R}^{m \\times 1 \\times c}\\) into the output of convolution \\(Y \\in \\mathbb {R}^{m \\times 1 \\times 2c}\\) to satisfy the input requirement of the gated linear units (GLU) of the next possible block and to make the input size and output size of the block consistent [38]. Y corresponds to \\([A\\ B] \\in \\mathbb {R}^{m \\times 1 \\times 2c}\\), where \\(A, B \\in \\mathbb {R}^{m \\times 1 \\times c}\\) are the inputs to the GLU. A simple gating mechanism over [A B] is implemented as follows: \n (3)GLU can select the relevant context for the target residue by the means of activated gating unit \u03c3(B). The gradient of GLU has a path that without downscaling contributes to the flow of the gradient, which is an important reason for the choice of the activation function. The vanishing gradient problem is considered before going deeper. Hence, residual connections from the input of the block to the output of the block are introduced to prevent the vanishing gradient [20]. The input of a block must be normalized before convolution because the input is the sum of the outputs of the several previous blocks. Without normalization, gradients are unexpected during training. Therefore, a LayerNormalization (LN) layer is set at the beginning of the block to provide a stable gradient, which is also conductive to accelerating the learning speed [39]. The function of BasicBlock is summarized in Eq.(4): \n (4)where Wl represents the weights of convolution in the l-th block, \\(s^{l}_{i}\\) is the features of the i-th element represented in the l-th block, k is the width of the convolution kernels and subscript L N means that \\(\\left [s^{l - 1}_{i - k / 2},..., s^{l-1}_{i + k / 2}\\right ]\\) has been normalized by LN. The details are described in Fig.\u00a04.\n\n\nFig. 4\nArchitecture of BasicBlock. The input of a BasicBlock is processed in the order LN-GLU-Conv. The output of a BasicBlock is the sum of the input and the Conv output. The shapes of the input/output for each layer in a BasicBlock are shown in the figure, where m is the length of the amino acid sequence and 2c is the number of output channels of the BasicBlock\nThe main evaluation metrics for binding residue prediction results include the Matthews correlation coefficient (MCC), precision (%) and recall (%), which are defined as follows: \n (5)\n (6)\n\n (7)\nFor the hyperparameter choice, we focus on the number of BasicBlocks N and the kernel width k in the BasicBlocks. N and k both have a decisive effect on the parameter space and the maximum length of the dependencies. Thus, N and k are closely related to the generalization and are separately adjusted to obtain the local optimum. When adjusting N, the kernel size of each BasicBlock is fixed to 3\u00d71 (k=3). When adjusting k, N is fixed to 10. The output channel number of each BasicBlock is set to 512 (c=256) in this study. Experiments show that the network achieves the locally optimal generalization on the validation sets when N=10 and k=53. The details are shown in Tables\u00a02 and 3.\nTable 2\nThe effect of depth on the validation sets\n\nTable 3\nThe effect of kernel width on the validation sets\nDCS-SI tends to predict residues as non-binding residues because the proportion of positive and negative samples in each batch is maintained at approximately the natural proportion. For the binary classification model, the threshold of positive and negative samples has a nonnegligible impact on performance. As shown in Table\u00a04, despite losing some precision, MCC and recall increase with the decreasing threshold, where the threshold is the minimum probability required for a sample to be predicted as positive. When the threshold =0.4, the MCC achieves local optimization.\nTable 4\nPrediction results on the validation sets with different thresholds\n\n1Thr: The threshold of the softmax\nstdDCS-SI and the baselines are tested on SITA and three extended testing sets. The existing 3D-structure-based methods within the baselines (TM-SI, COF and COA) outperform the sequence-based method S-SI on the testing sets. stdDCS-SI is far superior to all the baselines. The improvements of MCC and precision are no less than 0.05 and 15%, respectively. One possible reason for the moderate recall of stdDCS-SI is that the low percentage of binding residues in the training sets leads to prudent prediction of stdDCS-SI. Improving the recall of stdDCS-SI is a topic for future research. The details are described in Table\u00a05, where the hyperparameters are locally best adjusted for stdDCS-SI (k=5, N=10 and threshold =0.4). All the baselines used in the experiments are included in the I-TASSER Suite [31].\nTable 5\nPrediction results for the baselines and stdDCS-SI on the testing sets\n\n1TM-SI: TM-SITE\n2S-SI: S-SITE\n3COF: COFACTOR\n4COA: COACH\n5DCS-SI: DeepCSeqSite\nThe residues adjacent to binding residues have a higher probability of binding than do the other residues. stdDCS-SI does not explicitly consider the aggregation of binding residues. The consideration of aggregation is implicitly included in the transformation of the hidden representation, which is one reason for the good performance of stdDCS-SI. Furthermore, enDCS-SI predicts the binding residues with aggregation explicitly. The decoder network of enDCS-SI can extract useful information from the context prediction. As shown in Table\u00a06, the MCC for each testing set is improved 0.01 \u223c 0.02 by enDCS-SI. Although enDCS-SI requires more time to execute the additional forward propagation in its decoder network, the total time cost of enDCS-SI is not significantly increased. During testing, the predictions for every residue can be executed in parallel. Only the two forward propagations are processed serially. The advantage of enDCS-SI is more prominent if the input amino acid sequences are long and the machines have sufficient computational capacity. Table 6\nPrediction results for stdDCS-SI and enDCS-SI\nAs shown in Table\u00a07, the model trained on Aug-Train has slightly better generalization performance on the testing sets. However, the computational cost has increased several times. Relative to the cost, the improvement from data augmentation is far less than expected. This counterintuitive result indicates that proteins with a high sequence identity contribute little to the generalization of the network. Therefore, data augmentation based on high redundancy is not suitable as the main optimization method in this study.\nTable 7\nThe effect of data augmentation\n\n1Due to the difficulties in training on Aug-Train, networks with k=9 and N=10 are used in this experiment. The average-cross entropy loss per protein on Train and Aug-Train are 0.80 and 7.77, respectively. The cross-entropy loss on Aug-Train does not change substantially with further training. For fair comparison, we do not use more complex networks\n2SIEX1: SITA-EX1, SIEX2: SITA-EX2, SIEX3: SITA-EX3\nWe propose a sequence-based method called DeepCSeqSite (DCS-SI), which introduces deep convolutional neural networks for protein-ligand binding residue prediction. The convolutional architecture effectively improves the predictive performance. The highlights from DCS-SI are as follows: \n1.\nThe convolutional architecture in DCS-SI provides the ability to process variable-length inputs.\n\u00a02.\nThe hierarchical structure of the architecture enables DCS-SI to capture the long-distance dependencies between the residues, and the maximum length of the dependencies can be precisely controlled.\n\u00a03.\nAugmentation of the training sets slightly improves the performance, but the computational cost for training increases several times.\n\u00a04.\nWithout using any template including 3D-structure data, DCS-SI significantly outperforms existing sequence-based and 3D-structure-based methods, including COACH.", "s12859-019-2664-1": "Determining which target to pursue is a challenging and error-prone first step in developing a therapeutic treatment for a disease, where missteps are potentially very costly given the long-time frames and high expenses of drug development. With current informatics technology and machine learning algorithms, it is now possible to computationally discover therapeutic hypotheses by predicting clinically promising drug targets based on the evidence associating drug targets with disease indications. We have collected this evidence from Open Targets and additional databases that covers 17 sources of evidence for target-indication association and represented the data as a tensor of 21,437\u2009\u00d7\u20092211\u2009\u00d7\u200917.As a proof-of-concept, we identified examples of successes and failures of target-indication pairs in clinical trials across 875 targets and 574 disease indications to build a gold-standard data set of 6140 known clinical outcomes. We designed and executed three benchmarking strategies to examine the performance of multiple machine learning models: Logistic Regression, LASSO, Random Forest, Tensor Factorization and Gradient Boosting Machine. With 10-fold cross-validation, tensor factorization achieved AUROC\u2009=\u20090.82\u2009\u00b1\u20090.02 and AUPRC\u2009=\u20090.71\u2009\u00b1\u20090.03. Across multiple validation schemes, this was comparable or better than other methods.In this work, we benchmarked a machine learning technique called tensor factorization for the problem of predicting clinical outcomes of therapeutic hypotheses. Results have shown that this method can achieve equal or better prediction performance compared with a variety of baseline models. We demonstrate one application of the method to predict outcomes of trials on novel indications of approved drug targets. This work can be expanded to targets and indications that have never been clinically tested and proposing novel target-indication hypotheses. Our proposed biologically-motivated cross-validation schemes provide insight into the robustness of the prediction performance. This has significant implications for all future methods that try to address this seminal problem in drug discovery.Drug discovery and development often begin with a drug target, through which the drug exerts its therapeutic effect in patients with a certain disease or clinical condition (termed as an indication). A target is a broad term which includes many biological entities such as proteins, genes, and RNA, whose modulation (increase or decrease in activity) can provide a therapeutic benefit to a patient. Although selecting an efficacious drug target is the first and most important step in drug development, more than half of clinical trials still fail due to lack of efficacy, i.e., modulating the target\u2019s activity did not provide a statistically significant benefit to patients [1, 2]. Target selection is critical in drug discovery given the long-time frame and high expense of drug development.Often drug targets come from research publication where evidence is generated to support a hypothesis that inhibition or activation of a target may result in a therapeutic effect for a specific disease indication. For example, amyloid precursor protein is a target suggested for Alzheimer\u2019s Disease (AD). A piece of important evidence to support this hypothesis is that familial AD patients commonly have genetic mutations in the corresponding gene which lead to the production and deposition in the brain of increased amounts of amyloid beta peptide, a major characteristic of AD [3]. With current informatics technology, it is now possible to construct online repositories that aggregate existing knowledge about the association evidence linking potential targets with disease indications. Open Targets [4] is such a platform that provides drug discovery researchers with multiple evidence types including genetic association, pathways, animal models and drugs, that connect targets with indications for validating potential therapeutic hypotheses. At the same time, these online knowledge repositories are also amenable to computational analysis to discover drug target hypotheses using machine learning.One major challenge in framing this problem from a machine learning perspective is that there are very few positive examples (0.005% of target-indication hypotheses included in Open Targets have approved drugs). However, any insights gleaned from the limited number of pursued targets may be useful in delivering new medicines with lower attrition rates. In this paper, we collated historical outcomes of clinical trials and determined if these clinical outcomes can be predicted retrospectively using multiple machine learning models built on existing evidence of the targets\u2019 biological association with indications. A successful prediction model provides an understanding of how informative the evidence is for clinical success, and is also capable of generating new target-indication hypotheses with a higher potential of being developed into successful medicines.Another difficulty in building such a model is that not all biological evidence is available for every pair of target and indication due to reasons such as technological limitation and limited disease coverage. For example, as of June 2017, Open Targets contained 26,122 targets, 9150 diseases with 2,857,732 positive associations from 15 evidence sources. Though Open Targets contains over 2.8 million associations, that is still only 0.08% of the possible combinations covered by this data, suggesting that a great deal of association evidence (99.92%) is still to be determined by biomedical researchers and clinicians. Traditional paradigms of machine learning algorithms, learning a mapping from input features (biological evidence) to output prediction (clinical outcomes), may be inadequate in this context. We explored if tensor factorization is useful in the analysis of this sparse biological dataset.There are several lines of previous work that are related to this paper. One line of work is focused on answering the question of what makes a good drug target by investigating features of targets that are correlated with clinical successes in the context of genetics [9], tissue mRNA expression [10], human protein interactome [11] and publication trends [12]. Another line of work is focused on disease gene prediction, where the goal is to predict genes mechanistically involved in a given disease [13\u201317]. Our work is different from these efforts in that we leverage a novel computational method to integrate multiple evidence types and directly assess the models\u2019 performance of predicting clinical outcomes of drug target hypotheses.In the following sections, we first describe the dataset we have collected and then introduce the basic formulation of matrix factorization, a special form of tensor factorization. Then we explain our selection of a specific algorithm of tensor factorization based on characteristics of our data. We then discuss how we design experiments to benchmark the method against a series of baseline models under three scenarios of drug discovery. We demonstrate that the model can capture known biological mechanisms of human diseases and can identify opportunities of approved drug targets to novel indications.For clinical outcome data, if at least one drug asset for a given target-indication pair was identified as successful, then the target-indication pair was classified as Succeeded. Of the remaining target-indication pairs, if at least one asset had a clinical failure then it was classified as a Clinical Failure. Open Targets presents evidence from each individual source as a numerical value for a target-indication pair, with a positive value representing the strength of evidence. To simplify the further collation of target-indication evidence with target-only attributes (Table 2), we converted numerical evidence value into binary values: 1 indicates a positive association, 0 means that there is no association and unknown evidence is represented as null. We encoded categorical data, typically present in target-only attributes, as multiple binary values with each category converted into a binary value, i.e., having the property or not having the property. Here, we analyzed data mapped to the 574 non-cancer indications (a subset of 2211 indications) with at least one clinical outcome and the corresponding 875 targets (a subset of 21,437 targets). Oncology indications were excluded, as studies have observed that features of successful targets for cancer differ from features of successful targets for other indications [21, 22], moreover, cancer trials fail more frequently than trials for other indications. [23]The clinical outcomes of existing target-indication pairs can be represented in a matrix format as R\u2009\u2208\u2009\u211dM\u2009\u00d7\u2009N, where the M rows represent targets and the N columns represent indications. Rij\u2009=\u20091 if there is at least one drug that modulates target i and is marketed for indication j. Rij\u2009=\u20090 if all the drugs modulating target i are reported failed for indication j in the clinic (from Phase I to Phase III). For target-indication pairs that have no outcomes in the clinic, the corresponding Rij is empty. The goal is to predict clinical outcomes for all possible pairs of targets and indications i.e. fill out the empty Rij\u2032s. Thus, we can treat the problem as completing the target-indication matrix of clinical outcomes. Matrix completion problem has been widely studied in the machine learning community in the context of recommendation systems [6, 24]. A famous application is Netflix\u2019s movie recommendation system, where each user has ratings on a small number of movies and the task is to recommend movies for each user based on existing ratings of other users with similar patterns of movie ratings. Matrix factorization is recognized as one of the more successful methods for this task [6, 25, 26]. The method assumes that the true completed matrix is of low rank and can be approximated by a product of two low-dimensional latent factor matrices that represent rows and columns of a matrix in a joint D-dimensional latent space, i.e. R\u2009\u2248\u2009UTV, where \\( \\boldsymbol{U}={\\left\\{{\\boldsymbol{u}}_i\\right\\}}_{i=1}^M\\in {\\mathbb{R}}^{D\\times M} \\), \\( \\boldsymbol{V}={\\left\\{{\\boldsymbol{v}}_j\\right\\}}_{j=1}^N\\in {\\mathbb{R}}^{D\\times N} \\) and ui\u2009\u2208\u2009\u211dD, vj\u2009\u2208\u2009\u211dD are column vectors of U and V, respectively. The predicted entries in Rij is achieved by the inner product of ui and vj. Learning of U and V can be formulated as an optimization problem by minimizing the mean squared error between observed and predicted entries. To avoid overfitting, regularization on the latent factor matrices is added to the minimization problem that can be solved by methods such as stochastic gradient descent and alternating least square [6].Many matrix-factorization based methods have been proposed for recommendation systems. To choose an appropriate method to predict clinical outcomes, we considered three aspects of our problem. First, some of the evidence is target-indication specific such as human genetic evidence for each disease, and this has been suggested as related to clinical outcome [9]. Second, in our data, there are several target-only attributes independent of indications, such as target protein location, tolerance of mutation. Thus, the chosen method should also take target-only information into consideration. Third, in drug discovery, it is not uncommon that targets or indications that have never been tested in clinical trials. In the case of movie recommendation systems, this corresponds to recommending movies to users who have not rated any movies in the system or recommending new movies that do not have any ratings in the system. The chosen method should be able to handle this situation.Given these three aspects, we investigated a method based on tensor factorization, called Macau, that is capable of naturally handling all the three aspects in a unified Bayesian framework and was originally used to predict drug-protein interaction [27]. Tensor extends the matrix concept to a multidimensional array, where each dimension corresponds to one mode of a tensor. Our data can be organized into a three-mode tensor: target \u00d7 indication \u00d7 evidence \\( \\mathcal{T}\\in {\\mathbb{R}}^{M\\times N\\times K}, \\)where one entry tijk indicates the association score in kth evidence between target i and indication j and one \u201cslice\u201d of the tensor corresponding to one evidence source organized as a matrix. M, N, K are the number of targets, indications and evidence sources, respectively. To predict clinical outcomes, we appended the clinical outcome matrix R as one extra \u201cslice\u201d to the evidence tensor (Fig.\u00a01a) and factorized the resulting tensor \\( \\mathcal{X}\\in {\\mathbb{R}}^{M\\times N\\times \\left(K+1\\right)} \\). Similar to matrix factorization, tensor factorization decomposes a tensor into a series of low-dimensional latent factor matrices where each matrix represents one mode of the tensor. One direct way to decompose a three-mode tensor is to assume that each entry xijk can be expressed as the sum of the elementwise product of three low-dimensional vectors: ui, vj.and ek, representing ith target, jth indication and kth evidence (including the clinical outcomes), respectively in a joint latent factor space, i.e. \\( {x}_{ijk}\\approx {\\sum}_{d=1}^D{u}_{di}{v}_{dj}{e}_{dk} \\), where D is the dimensionality of the latent factors. The latent factors can be further organized in three factor matrices: \\( \\boldsymbol{U}={\\left\\{{\\boldsymbol{u}}_i\\right\\}}_{i=1}^M\\in {\\mathbb{R}}^{D\\times M} \\), \\( \\boldsymbol{V}={\\left\\{{\\boldsymbol{v}}_j\\right\\}}_{j=1}^N\\in {\\mathbb{R}}^{D\\times N} \\), \\( \\boldsymbol{E}={\\left\\{{\\boldsymbol{e}}_k\\right\\}}_{k=1}^{K+1}\\in {\\mathbb{R}}^{D\\times \\left(K+1\\right)} \\) and ui\u2009\u2208\u2009\u211dD, vj\u2009\u2208\u2009\u211dD,ek\u2009\u2208\u2009\u211dD\u00a0are column vectors of U, V and E, respectively. Here the eK\u2009+\u20091 column of E corresponds with the latent factor of the clinical outcome. The prediction of any entry xijk of the tensor can be achieved by the sum of the elementwise product of the low-dimensional vectors of target ui, indication vjand evidence ek. Since the factorized tensor included the clinical outcome matrix, we can use the low-dimensional vector corresponding to the clinical outcome to perform prediction, i.e. the predicted outcome of modulating target i for the treatment of indication j is 1T\u00a0(ui\u2009\u2218\u2009vj\u2009\u2218\u2009eK\u2009+\u20091), where 1 is an all one vector and \u2218 is the elementwise product.The specified tensor factorization method we chose is based on Bayesian probabilistic modeling, which assumes each observed cell of the tensor \\( \\mathbf{\\mathcal{X}} \\) is a random variable following a normal distribution, \\( {x}_{ijk}\\sim \\mathcal{N}\\left({\\mathbf{1}}^{\\boldsymbol{T}}\\ \\left({\\boldsymbol{u}}_i\\circ {\\boldsymbol{v}}_j\\circ {\\boldsymbol{e}}_k\\right),{\\alpha}^{-1}\\right) \\), where\u00a0\u03b1 is the precison of the normal distribution. In this model, the mean of the normal distribution is determined by the three low-dimensional latent factors: ui, vj.and ek. Each latent factor is assumed to have a Gaussian prior with a Gaussian-Wishart hyper prior placed on its hyperparameters: \\( {\\boldsymbol{u}}_i\\sim \\mathcal{N}\\left({\\boldsymbol{\\mu}}_{target}+{\\mathbf{B}}^{\\boldsymbol{T}}{\\boldsymbol{g}}_i,{\\Lambda}_{target}^{-1}\\right) \\), \\( {\\boldsymbol{v}}_j\\sim \\mathcal{N}\\left({\\boldsymbol{\\mu}}_{indication},{\\Lambda}_{target}^{-1}\\right) \\) and \\( {\\boldsymbol{e}}_k\\sim \\mathcal{N}\\left({\\boldsymbol{\\mu}}_{evidence},{\\Lambda}_{target}^{-1}\\right) \\), where \u0392\u2009\u2208\u2009\u211dG\u2009\u00d7\u2009D linearly projects the target-only attributes gi\u2009\u2208\u2009\u211dG into the latent space, providing prediction ability to targets that do not have any observed clinical outcomes. The inference of model parameters is carried out by sampling from the posterior of the model parameters by Markov Chain Monte Carlo (MCMC) technique, except for \u03b1, which is set to 1 by default and the number of latent factors D, which is determined by a heuristic approach (see Additional\u00a0file 1). Specifically, we used the Julia implementation of the method [28] and followed a common practice of MCMC inference where we \u201cburn-in\u201d samples generated in the beginning and collect samples after that to approximate posterior distribution over model parameters [29]. In our case, the first 500 samples were discarded and the posterior distribution over parameters were estimated using 300 samples after the \u201cburn-in\u201d process. The predictive distribution is approximated from the 300 samples of the model parameters and the average over samples is used to make predictions. Generally, we did not observe further improvement on prediction performance if we let the chain run longer.We performed a nested cross-validation experiment to evaluate the method in three different scenarios (Fig. 1b). In each experiment, we divided the target-indication pairs with clinical outcomes (6140) into K folds and tested the prediction results on a held-out (one of the K) fold using a model trained with the rest (K-1) of folds. This is the outer loop for the cross-validation. In the inner loop, we determine the model parameters using five-fold cross-validation. In the first experiment, we did a standard ten-fold cross-validation in the outer loop, where each fold is randomly determined but retains the same fraction of successes. In drug discovery, we know that certain sub-classes of targets and indications have different properties. In order to, assess if the model can be generalized to sub-classes of targets and indications different from those used in the training stage, we devised two other cross-validation experiments where each time the clinical outcomes of one pre-defined group of targets (indications) are left out as the test set. Specifically, for the second experiment, i.e. leave-one-target-group-out, we used the grouping defined by the Target Class (See Table S1 in Additional file  1). A given target is assigned to one of ten target classes (thus K\u2009=\u200910) based on the target\u2019s protein family retrieved from ChEMBL hierarchical target classification system [30]. For the third experiment, i.e. leave-one-indication-group-out, we defined eight indication groups (thus K\u2009=\u20098) by de novo clustering indications based on the similarity of the indications in terms of their relative positions in MeSH (Medical Subject Heading) hierarchical tree and co-occurrence frequency in the literature (see Table S2 in Additional file 1).Logistic Regression (LR), a simple linear model.LASSO [31], which is a generalized linear model with L1 regularization implemented in the glmnet R package where the regularization parameters were determined using cross-validation.Random Forest (RF), an ensemble model of decision trees where the parameters are determined by the Out-of-Bag error estimate using the tuneRF function in randomForest R package.Gradient Boosting Machine (GBM) [32], a boosting method which is implemented in xgboost [33] where we tuned the following parameters using cross-validation: the feature shrinkage rate, maximum depth of a tree, subsample ratio of features and number of iterations.Matrix Factorization (MF): We also included another baseline model where we only used the clinical outcome matrix and applied matrix factorization to complete the matrix for prediction. Specifically, we used a nuclear norm regularized matrix factorization method that is implemented in the softimpute [34] R package and the regularization parameter is determined through cross-validation.We used two metrics to measure the prediction performance of the evaluated methods: area under receiver operator curve (AUROC) and area under precision-recall curve. (AUPRC). The AUROC measures the probability of a model ranking a randomly chosen positive example higher than a randomly chosen negative example and is commonly used in assessing the performance of models for binary classification tasks. AUROC treats positive and negative examples equally, this metric is of limited value when the number of positive examples is relatively low. Given the low success rate in drug development, we chose AUPRC as the primary evaluation metric as it focuses on the performance of positive examples. Here the precision is the proportion of correctly predicted positives out of all predicted positives and recall is the proportion of correctly predicted positives out of all positives.To mitigate this problem and obtain an unbiased estimation of predictive capacity, we designed two benchmark experiments, where a group of similar targets (Fig. 1b, panels 2 and 3) or indications is held out as a test set and models trained on the other target or indication groups, respectively, are evaluated against the held-out set. We categorized targets into ten target classes largely derived from the ChEMBL hierarchical target classification system [30], and grouped indications into eight clusters that are based on MeSH hierarchy and co-occurrence frequency in the literature (see Additional file 1). In the leave-one-target-class-out cross-validation experiment (Fig. 2), the performance of MF decreases dramatically as there is no information in the training set to predict the clinical outcomes of the held-out target class. All the other methods perform similarly and the overall performance is not as good as in the standard cross-validation setting. This implies that it is difficult to predict candidate indications for targets that have not been assessed in clinical trials. In the leave one disease cluster out validation experiment, the performance of MF again dropped below that of the other methods as there is no information about clinical outcomes of the held-out disease clusters in the training step.However, the Bayesian tensor factorization (BTF) model scored as the best model in the disease group benchmark (AUROC\u2009=\u20090.73\u2009\u00b1\u20090.05, AUPRC\u2009=\u20090.58\u2009\u00b1\u20090.09) and the second to best model in standard cross-validation (AUROC\u2009=\u20090.82\u2009\u00b1\u20090.02, AUPRC\u2009=\u20090.71\u2009\u00b1\u20090.03). It is counter-intuitive that BTF does not out-perform the MF method in the standard cross-validation case, as it incorporated more data. MF approach may be taking maximum advantage of the highly-related nature of the outcomes, given the poor performance of MF in the target class and disease group benchmarks. MF also only needs to learn latent factors to explain the clinical outcomes, while BTF needs to learn latent factors to explain the clinical outcomes and all the evidence as well, which is inherently a more difficult task.In general, the performance of models that explicitly use evidence as predictors did not vary too much across three validation settings. Among these models, ensemble-based methods (RF and GBM) worked slightly better than linear model-based methods (LR and LASSO). Although MF performed relatively well in the standard validation case, its performance was inconsistent among validation settings. BTF combined both evidence and inter-relationship among targets and indications and performed consistently well in all three validation scenarios. In addition to AUROC and AUPRC, we also evaluated performance using F-score, precision@30, and recall@30 (see Additional\u00a0file\u00a04), but the comparison across methods was not affected.To validate the prediction made by the BTF model, we chose 1246 novel target-indication pairs that were in clinical trials (Phase I-III) at the time when we collected the data (May 2016), and thus did not have clinical outcome readouts. We compared the prediction scores generated by the BTF model on these target-indication pairs and noticed that the prediction scores of later phase pairs are significantly higher than those of earlier phase pairs (Fig.\u00a04b), which recapitulates the observation that drugs in later phases on average have a higher likelihood of approval [37]. Since we did not include phase information of these target-indication pairs when training the model, these pairs serve as an independent test set and the results increase our confidence in the predictions of the model.As an example, interleukin 6 (IL6) is an approved drug target for giant lymph node hyperplasia (Table 3). Our results suggest that the current trials for psoriatic arthritis, which includes a Phase IIb trial of a monoclonal antibody against this protein [38], have a greater than random chance of success. Psoriatic arthritis is chronic inflammatory arthritis that is associated with psoriasis and thus somewhat related to the successful indication for IL6, a cytokine with a wide variety of biological functions. It induces the acute phase response and is involved in the final differentiation of B-cells into Ig-secreting cells in lymphocyte and monocyte differentiation. It acts on B-cells, T-cells, hepatocytes, hematopoietic progenitor cells and is required for the generation of T(H)17 cells. It also acts as a myokine and is discharged into the bloodstream after muscle contraction and acts to increase the breakdown of fats and to improve insulin resistance [39]. Genetic polymorphism of IL6 has been shown to be significantly associated with a form of psoriatic arthritis [40], and serum IL6 is considered as a biomarker for assessing disease activity in patients with psoriasis, as well as for predicting responsiveness of joint symptoms to biologic treatment [41].Another target of interest is angiotensin II receptor type 1 (AGTR1), an important effector controlling blood pressure and volume in the cardiovascular system. It has been approved for many cardiovascular indications such as heart failure, myocardial infarction, and hypertension. The predicted indication for AGTR1 is hypercholesterolemia, also known as high cholesterol. AGTR1 antagonism improves hypercholesterolemia-associated endothelial dysfunction [42] and attenuates the inflammatory and thrombogenic responses to hypercholesterolemia in venules [43]. Significant association of AGTR1 polymorphism with hypercholesterolemia was also observed in hypertension patients [44].In this paper, we focused on the problem of predicting clinically promising therapeutic hypotheses using associative knowledge of targets and indications. We compared tensor factorization with other traditional machine learning methods in a variety of benchmarking experiments and identified two interesting findings from the evaluation of this method: 1) the latent factors learned from the model align with known biological relationships among three human disease areas, and 2) the method can be applied to different scenarios of drug discovery and achieves competitive prediction performance.However, there are some limitations worth discussing before deploying tensor factorization to propose novel target-indication hypotheses. First, the model relies on the available compilation of evidence sources. Open Targets provided us with a good foundation, but clearly, more sources could be gathered. Second, we treated every clinical failure equally. Our preliminary analysis has shown that some target-indications pairs have been tried multiple times and are still being pursued clinically, while some failed only once and were never tested again. Although the probabilistic framework of the model can potentially mitigate this problem, the model does not explicitly differentiate definitive failures from those that have not been thoroughly explored and may become successful drugs in the future. Lastly, we only applied the technique to a dataset of targets and indications with at least one clinical outcome; thus, the application as benchmarked here is constrained to applying approved drug targets to new indications. The methodology, however, can be expanded to any target and any indication so long as their evidence is encoded in the data. Such an application may result in the identification of novel target-indication hypotheses with a high predicted probability of being successfully translated into medicines.Computational prediction of drug targets has been widely studied in the context of predicting disease-associated genes [14\u201316, 45\u201347]. These disease-associated genes can facilitate the discovery of drug targets by narrowing down the search space of potential targets. The prediction performance (precision) of the models varies from 0.5 to 0.9 depending on the methods and data used in the studies. Many related studies design models to infer novel associations by leveraging similarity information between biological entities and biomolecular network information encoded in a protein-protein interaction database [47, 48]. One example is FASCINATE [17], which is able to infer cross-layer dependencies on multi-layered biological networks. This method can be used for this problem by collapsing all evidence and augmenting the data with disease similarity information.In this work, we evaluated a machine learning technique called tensor factorization on the problem of predicting clinical outcomes of therapeutic hypotheses using existing association evidence between drug targets and disease indications. We illustrate that the method can achieve equal or better prediction performance compared with a variety of baseline models across three scenarios of drug discovery, and the learned model can capture the known biological mechanism of human diseases. Furthermore, we demonstrated an application of the method to predict outcomes of trials on novel indications of approved drug targets. Future work includes expanding this method to targets and indications that previously have never been clinically tested and proposing novel target-indication hypotheses that can be developed into medicines with predicted high probabilities of success.Area Under Precision Recall CurveArea Under Receiver Operator CurveBayesian Tensor FactorizationGradient Boosting MachineLogistic RegressionMedical Subject HeadingMatrix FactorizationRandom ForestThe authors received no specific funding for this work.The target-indication association evidence was downloaded from Open Target https://www.targetvalidation.org/downloads/data. Clinical outcomes, tissue overexpression from GeneLogic and pathway information from Metabase are derived from commercial databases, thus needs to be licensed. The majority of the data are supplied in Additional\u00a0file 2 (target-indication association) and Additional file 3 (target only evidence).Not applicable.Not applicable.JY, MH, MN, and PA are full-time employees of GSK.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Tensor extends the concept of a matrix to a multidimensional array where each dimension corresponds to one \u201caxis\u201d, called mode, of a tensor [5]. Data in many applications can be naturally organized into a tensor format. Figure\u00a01a shows a three-mode tensor representing different types of evidence associating targets with disease indications and one extra \u201cslice\u201d represents clinical outcomes. Tensor factorization decomposes a tensor into factor matrices that compactly store information encoded in a tensor and integrate interaction across different modes even when a large portion of entries of a tensor is missing [5]. This technique has a wide range of applications such as in recommendation systems [6], knowledge graph systems [7] and multiple biomedical domains [8].\n\nFig. 1\nData representation and model benchmark schematic. a Tensor representation of the dataset. The last \u201cslice\u201d matrix represents the clinical outcomes of target-indication pairs. b Illustration of three schemes of benchmarking models on predicting clinical outcomes. Each matrix represents the clinical outcomes of targets (rows) and indications (columns). Grey and green cells are target-indications pairs used for training and testing, respectively. Blank cells represent unknown clinical outcomes of target-indication pairs\nWe created a dataset which combined clinical outcomes from the commercial database Pharmaprojects [18] with evidence from Open Targets [4] and other sources (Tables\u00a01, 2). In total, we collected 17 association evidence sources that connect potential targets with disease indications. These 17 association evidence sources can be further grouped into seven evidence types: Genetics (is germline mutation in the target associated with the disease?), somatic mutations (is somatic mutation in the target associated with the disease, typically cancer?), pathways (is the target part of a pathway involved in the disease?), mRNA disease expression (does the target\u2019s expression significantly change in the disease?), mRNA tissue overexpression (is the target\u2019s expression overexpressed in disease-related tissues?), literature (is there association between the target and the indication identified through text mining of scientific literature?) and animal models (does the knockout of the target in animal models manifest phenotypes that are concordant with the human disease?). Besides these 17 association evidence sources, we also collected information about properties of targets from six sources (Table\u00a02) as previous studies have found that successful FDA-approved drugs are enriched with targets with properties that are independent of disease indications [19, 20]. The collected evidence covers the data space of 21,437 targets, 2211 indications, and 17 evidence sources.Table 1\n17 sources of target-indication evidence\n\nEvidence data were obtained from Open Targets [1] except for TERMITE: www.scibite.com/products/termite; GeneLogic: GeneLogic Division, Ocimum Biosolutions, Inc., Internal expression data, and those explicitly referenced\nTable 2\nSix sources of target-only categorical attributes\n\nGenes were broken into non-overlapping categories based on available data. Genes were classified as tolerant, intolerant and unclassified based on data from the Exome Aggregation Consortium [51] and the percentile rank of Residual Variation Intolerance Score [52]. Genes were based on the identification of >\u2009=\u200975% protein homology between human and mouse, data downloaded from BioMart [53]. Target Location and Topology were derived from a review of information from Gene Ontology, InterPro, PFAM, and UniProt\nFor comparison purposes, we also ran the cross-validation experiments using four additional machine learning models. For these models, each target-indication pair is treated as a data point and the corresponding 17 association evidence and six target-only attributes are treated as its features. As the target-only attributes are not directly linked with specific indications, for each target, we duplicated its feature values across all indications. The task is being cast as a binary classification problem. To allow these four models to handle missing values, we treated the association scores as categorical variables with three categories: no association (0), positive association (1) and unknown (missing) association. Each categorical variable is then encoded as two binary variables (also called one-hot encoding). The four models that we tested are:1.\nLogistic Regression (LR), a simple linear model.\n\u00a02.\nLASSO [31], which is a generalized linear model with L1 regularization implemented in the glmnet R package where the regularization parameters were determined using cross-validation.\n\u00a03.\nRandom Forest (RF), an ensemble model of decision trees where the parameters are determined by the Out-of-Bag error estimate using the tuneRF function in randomForest R package.\n\u00a04.\nGradient Boosting Machine (GBM) [32], a boosting method which is implemented in xgboost [33] where we tuned the following parameters using cross-validation: the feature shrinkage rate, maximum depth of a tree, subsample ratio of features and number of iterations.\n\u00a05.\nMatrix Factorization (MF): We also included another baseline model where we only used the clinical outcome matrix and applied matrix factorization to complete the matrix for prediction. Specifically, we used a nuclear norm regularized matrix factorization method that is implemented in the softimpute [34] R package and the regularization parameter is determined through cross-validation.\n\u00a0We performed a standard cross-validation experiment to benchmark various types of machine learning models (Fig. 1b, panel 1). The best model is the matrix factorization (MF) model (AUROC\u2009=\u20090.83\u2009\u00b1\u20090.02, AUPRC\u2009=\u20090.77\u2009\u00b1\u20090.02) (Fig.\u00a02), which only factorizes the clinical outcomes matrix without considering any other evidence in the dataset. Due to the highly-correlated structure within the clinical outcomes of target-indication pairs, the standard way of randomly splitting them into training and test sets may overestimate the predictability of clinical outcomes. This may explain the high performance of MF; knowing which targets have succeeded against which indications in the training data may provide enough information to predict the outcome status of new indications for these targets. Many drug targets are from the same gene family, and in a random training-test split, it is likely that targets within the same gene family are split across training and test set, though the same drug may bind to multiple members in each set. This leads to an overestimate of prediction accuracy for truly independent and novel targets. A similar effect may relate indications with different subtypes, as drug targets are often tried against many closely related diseases.\n\nFig. 2\nBenchmark performance of models. Prediction performance comparison in three benchmark schemes in terms of Area Under Receiving Operation Curve (AUROC, Top) and Area Under Precision Recall Curve (AUPRC, Bottom). Error bars are calculated from cross-validation (LR: Logistic Regression; GBM: Gradient Boosting Machine; RF: Random Forest; MF: Matrix Factorization; BTF: Bayesian Tensor Factorization)\nOne advantage of this leave one target/disease group out validation scheme is that we can assess how trained models can be generalized to groups of targets/diseases that the models have never trained on before. Figure\u00a03 shows the prediction performance of the six models on the held-out target classes (Fig.\u00a03a) and disease clusters (Fig. 3b). In the leave-one-target-class-out case, the prediction performance averaged over the six models varies between target classes (AUPRC ranges from 0.24 to 0.58; AUROC ranges from 0.53 to 0.68). Specifically, we notice that the models perform consistently poorly for transcriptional factor targets and miscellaneous enzymes, which implies that these target classes are quite different from the other target classes. On the other hand, most models perform relatively well in protease targets. We note the performance is consistent among models within each target class, but this low variability is not repeated in the leave-one-disease-cluster-out case, where the prediction performance shows higher variability among disease clusters. For example, the BTF model performs better than the other models in the metabolic, GI and urologic and oral disease clusters, and performs as well as any other model in the other disease clusters.\n\nFig. 3\nBenchmark performance of leave one out experiments. Model performance on predicting clinical outcomes of target classes (a) and disease clusters (b) in the leave-one-out experiments in terms of Area Under Receiving Operation Curve (AUROC, x-axis) and Area Under Precision Recall Curve (AUPRC, y-axis). 95% confidence interval is calculated using 1000 bootstraps. Dotted lines mark the AUROC (vertical) and AUPRC (horizontal) of a random guess, which is 0.5 and the fraction of positives in the testing set, respectively. The percentage of target-indication pairs in each held-out set is listed after the pipe symbol (|) in the titles. (LR: Logistic Regression; GBM: Gradient Boosting Machine; RF: Random Forest; MF: Matrix Factorization; BTF: Bayesian Tensor Factorization)\nAfter benchmarking the performance of the BTF model in the cross-validation experiments, we fitted the model to the whole dataset. We chose 11 latent factors (see Additional file 1). Before using the fitted BTF model to make any predictions, we explored whether the latent factors learned from the model are biologically meaningful so that we can increase our trust in the prediction made by the model. To do so, we reduced the 11 latent factors to two dimensions using t-SNE [35] to visualize how indications are distributed and examined whether the learned latent factors can capture inter-relationship among indications. t-SNE is a dimension reduction technique used to visualize high-dimensional dataset where similar points in high dimensional space are transformed to neighboring points in a low dimensional space and dissimilar points are transformed to distant points in the low dimensional embedding. Figure\u00a04a shows the two-dimensional t-SNE embedding of the 574 indications with at least one clinical outcome, where three distinct clusters are present on the map. We further checked the MeSH annotations of the diseases in each cluster and found that the three clusters are enriched with three distinct disease categories including Central nervous system diseases, Digestive system diseases, and Hemic & lymphatic diseases, respectively. Interestingly, auto-immune diseases, such as rheumatoid arthritis, asthma, psoriasis and Crohn\u2019s disease that manifest in different organs are localized in the same neighboring area on the map. This implies that latent factors are able to capture the intrinsic relationships of diseases within these disease areas. For the rest of the diseases, we did not observe distinct clustering patterns using t-SNE. This could either be because the latent factors are not rich enough to capture the relationship among these diseases or these diseases are inherently interconnected by sharing similar pathological mechanisms [36].\n\nFig. 4\nValidation of BTF model prediction. a t-SNE visualization of indications based on the latent factors learned in BTF model. Each dot represents one indication and the size of the dot is proportional to the number of targets that have been clinically failed. The inserted pie charts show diseases composition of representative clusters of indications in the 2D visualization. 2D embedding was obtained by using perplexity\u2009=\u200930 in t-SNE and the visualization is consistent using different perplexity values in the range from 10 to 50. b BTF prediction scores of target-indication pairs in Phase I-III clinical trials. The numbers are P-values (Wilcoxon rank sum tests) from comparing prediction scores of target-indication pairs between any two phases\nNext, we conducted a literature search on the top 63 hypotheses of the 1246 pairs based on a prediction score threshold, which corresponds with 0.8 precision and 0.27 recall in the standard cross-validation experiment. We list 15 of these 63 hypotheses along with a relevant literature reference in Table\u00a03; the complete list of 63 can be found in Additional file 1.Table 3\nHigh Scoring Pairs of Interest from TF Model\n\nNew indications of approved targets in clinical trials (Phase* as of May 27, 2016) that have the highest probability of eventual clinical success as measured by the tensor factorization model. The full list is available in the supplement. For illustrative purposes, we list a related indication approved for assets for each target\n\n\nAdditional file 1:\nSupplementary material. (PDF 490 kb)\n\n\n\nAdditional file 2:\nTarget-indication association evidence data file. (ZIP 227 kb)\n\n\n\nAdditional file 3:\nTarget-only attribute data file. (ZIP 5 kb)\n\n\n\nAdditional file 4:\nResults of benchmark experiments. (XLSX 11 kb)", "s12859-018-2550-2": "Small open reading frames (smORF/sORFs) that encode short protein sequences are often overlooked during the standard gene prediction process thus leading to many sORFs being left undiscovered and/or misannotated. For many genomes, a second round of sORF targeted gene prediction can complement the existing annotation. In this study, we specifically targeted the identification of ORFs encoding for 80 amino acid residues or less from 31 fungal genomes. We then compared the predicted sORFs and analysed those that are highly conserved among the genomes.A first set of sORFs was identified from existing annotations that fitted the maximum of 80 residues criterion. A second set was predicted using parameters that specifically searched for ORF candidates of 80 codons or less in the exonic, intronic and intergenic sequences of the subject genomes. A total of 1986 conserved sORFs were predicted and characterized.It is evident that numerous open reading frames that could potentially encode for polypeptides consisting of 80 amino acid residues or less are overlooked during standard gene prediction and annotation. From our results, additional targeted reannotation of genomes is clearly able to complement standard genome annotation to identify sORFs. Due to the lack of, and limitations with experimental validation, we propose that a simple conservation analysis can provide an acceptable means of ensuring that the predicted sORFs are sufficiently clear of gene prediction artefacts.Small open reading frames (smORF) are sequences that potentially encode for proteins but are shorter than other more commonly translated genomic DNA sequences [1]. Such protein sequences can theoretically range from a minimum of two to ~\u2009100 residues. Various values have been reported for what can be acceptable as the limits to be a small and functional protein. The problem of determining what constitutes the minimum number of codons to be considered as protein coding has been discussed since the earliest genome sequences for Saccharomyces cerevisiae were published [2, 3]. In addition to the term smORF, these sequences have also been referred to as short open reading frames (sORFs) and the proteins that they encode have at times been referred to as microproteins.Despite the term sORF turning up only in more recent literature, the existence of genes that code for proteins of 150 residues and less have been known for more than three decades. Functional sORFs have been identified in a wide range of organisms from prokaryotes to humans. The Sda protein (46 residues) found in Bacillus subtilis is known to inhibit sporulation by preventing the activation of a required transcription factor [4, 5]. Proteins such as TAL (11 residues), found in Drosophila melanogaster, are known to be important for leg development [6, 7]. The Cg-1 protein (<\u200933 amino acids) is involved in controlling tomato-nematode interaction [8]. In Homo sapiens, the humanin (24 amino acids) protein is involved in mitochondria-nuclear retrograde signalling that controls apoptosis [9, 10]. Possibly the smallest ORF reported to date encodes a six residue polypeptide \u2013 MAGDIS; this ORF is referred to as the upstream open reading frame (uORF) in the mRNA of S-adenosylmethionine decarboxylase (AdoMetDC), a key enzyme in the polyamine biosynthesis pathway [11].As genome sequencing capabilities steadily progressed from the late 90s, through the 2000s to the present, many studies have identified and annotated sORFs directly from genome sequence data [12]. Various reports of sORFs discovered from such efforts have been published such as in Escherichia coli (15\u201320 amino acids) [13]; in yeast - Saccharomyces cerevisiae (less than 100 amino acids) [12, 14]; in plants - Arabidopsis thaliana (100\u2013150 amino aicds) [15] and Bradyrhizobium japonicum (less than 80 amino acids) [16]; in insects - Drosophila (less than 100 amino acids) [17]; in mouse (less than 100 amino acids) [18] and in human (less than 100 amino acids) [19]. More recently, Erpf and Fraser reviewed the diverse roles of sORF encoded peptides (less than 150 amino acids) in fungi [20].Nevertheless, it has also been shown that many ORFs with lengths of 100 or less amino acids may have been missed during gene prediction from whole genome sequences because the gene prediction tools are tuned to ignore small genes perceived to be \u2018junk\u2019 or non-protein coding [21]. For example, the early genome annotations of S. cerevisiae had defined 100 residues and 150 residues as the minimum number to be encoded by an ORF thus in a way setting a parameter value for future gene predictions and annotation work [2, 3]. Perhaps as a consequence of such practices being integrated as part of standard gene prediction protocols, the number of sORFs that have been identified over the years has remained relatively small. Although the parameters for the gene prediction can be tweaked and changed in light of a better understanding regarding the existence of sORFs, the challenge of ascertaining that the annotated sORFs are indeed protein coding and not artefacts remains [17].In this work, we have identified potential sORFs from fungal genomes by specifically repeating the gene prediction and annotation processes based on a residue length cutoff of 80 amino acids or less and specified the range of sORFs length distribution among homologs to avoid false positives. The cutoff of 80 residues was chosen as a simplistic means of selecting ORFs that were most likely to have been overlooked by previous gene predictions. The identification of 1986 putative predicted sORFs involved a large sequence dataset extracted from 31 fungal genome sequences with a total of 210,928 ORFs from existing gene prediction and annotation. The predicted sORFs were then compared to identify highly conserved examples within the fungal genomes dataset by adopting the assumption that such highly conserved sequences may code for common or even essential functions and are thus unlikely to be artefacts or randomly matched examples. This can potentially be a quick and inexpensive means of identifying subsets of sORFs that are classified as hypothetical proteins for experimental characterization.The fungal genomes selected were required to have associated annotations for predicted genes thus limiting our dataset to only 31 genomes at the time the work was initiated. These annotations were utilized to identify 5255 sORFs genes that had already been identified in the original annotation to encode for a maximum of 80 amino acid residues. The ORF prediction process was then repeated for all 31 fungal genomes using the computer programs getorf [22, 23] and sORFfinder [24] as detailed in methods section. This process resulted in 16,156,945 sORFs identified by getorf and 902,110 found by sORFfinder. The results of both searches were overlapped to yield a consensus of 42,587 potential sORFs sequences encoding for 80 residues or less. The ORFs predicted by getorf with a cutoff of 240\u2009nt were considered as genes that can either be a region that is free of STOP codons or a region that begins with a START codon and ends with a STOP codon [22, 23]. However, all the sORFs identified in this study have both START as well as STOP codons.The standard gene prediction process may miss ORFs that encode for protein sequences of less than 100 residues [12, 27]. In order to address this, we carried out a two pronged approach using a dataset of 31 available fungal genomes to carry out: (i) identification of ORFs that have already been annotated to be below 80 residues in length and (ii) repeating the gene prediction process for each genome to specifically identify genes that encode for sORFs of 80 residues or less.The bias of the parameter often used to predict genes that require a minimum of 100 codons may bypass the sequences in the intergenic spaces, especially when such regions are less than 100 residues in length, but yet they may actually encode for functional proteins of less than 80 residues. Intronic sequences may also hypothetically code for such sORFs. Therefore, these sequences were used as the target for sORF searches in this work. Predicted sORFs that were found to occur in multiple genomes were selected for further characterization. However, it was anticipated that such searches can return a large number of predicted genes, many of which could be artefacts of the search process itself. In order to address this, the pool of predicted sORFs were then compared to each other to find potentially homologous sequences within the predicted sORFs dataset. It is expected that such short sequences that are conserved in several genomes can be assumed to be of functional importance and thus not an artefact of the gene prediction process, especially more so if those sequences were also extracted from the intronic regions as was the case in this study.The first approach merely involved identifying ORFs from the existing available annotations for sequences that fitted the maximum 80 residues criterion used for this study. This approach was dependent on parameters had been set by the annotators of the deposited data as the minimum number of codons that were to be considered as protein coding. The sORFs retrieved from this extraction provided a reference for what had already been identified. The second approach, which can be considered as the major feature of this work, involved repeating the gene prediction and annotation process by specifically identifying potential ORFs in the intronic, exonic and intergenic regions. We had opted to focus the searches on sequences extracted from the intronic and intergenic regions because a relatively high number of sORFs can be found within these regions as demonstrated by the discoveries of 3241 putative sORFs in the intergenic regions of Arabidopsis thaliana [28] and 15 sORFs in the intronic regions of Drosophila [29].The sORF identification in the second approach involved the use of two computer programs, sORFfinder and getorf. The sORFfinder program was specifically designed to detect small ORFs. The getorf program, which is available as part of the emboss package, employs a less stringent approach that simply involves setting the sequence length parameter for genes to be below 240 nucleotides within the start to stop codons reading frame. In order to throw a wider net, we specifically included intronic and intergenic sequences as inputs for sORF identification. It is not unexpected that the output of both programs would contain false positives. In order to narrow down the selection, we had only selected outputs that were agreed on by both sORFfinder and our getorf runs at the 240\u2009nt cap. This filtering was done in order to reduce the number of sequences for further characterization. It is however possible that true sORFs are present in the dataset that were predicted by only one of the programs and thus not investigated further as a part of this work. This is a clear limitation of the process that we had introduced as a means to acquire a more manageable number of sequences for further characterization.The bypassing of sORFs that are located in the intergenic regions can occur during what is considered as the standard gene prediction process because these stretches of sequence only have sufficient length to encode for polypeptides that may be shorter than 100 residues [12, 27] and are thus overlooked as simply being non-coding filler sequences between two coding sequences. In order to address the possibility that a large number of sORFs in the intergenic regions may have been missed during a standard gene prediction process as demonstrated by the work of Hanada et al. that identified novel small open reading frames that were confirmed to at least be transcribed [28], our analysis also specifically targeted for the presence of sORFs in those sequences.In 274 clusters predicted from the 31 fungal genomes, 892 putative conserved sORFs have been annotated previously as a gene and have known functions. Characterization of the putative conserved sORFs revealed that approximately 3.8% of the newly predicted sORFs have known functions but were not annotated as genes in the available genome annotations. Our sORF annotation workflow also determined that 832 of the putative conserved sORFs predicted are hypothetical proteins or have no characterized function (Fig. 3). Even though these sORFs do not have a known function, their conservation across multiple species imply that their presence is of some functional importance. Out of the total of 848 predicted sORFs from the 31 genomes (Fig. 3), 93 sORFs from the sORFfinder-getorf integration output have homologs in other organisms (Additional file 2).In the cellular component classification - there were 184 predicted sORFs classified into mitochondria (59), nucleus (57), endoplasmic reticulum (13), integral component of membrane (33) and Ssh1 translocon complex (22). Under the molecular function classification - the predicted sORFs were assigned to functions associated to mating pheromone activity (11), DNA and RNA binding (41), ribosome (145), cytochrome (7), protein binding (31), zinc ion binding (22), hydrogen ion transmembrane transporter activity (39), metal ion binding (16), protein heterodimerization activity (1), oxidoreductase activity (1), ATP binding (1), GTP binding (1) and ligase activity (1). For biological process GO classification, the 115 predicted sORFs in this group were classified into ribosome biogenesis (26), carbohydrate metabolic process (2), mitochondrial electron transport (4), DNA repair (1), mRNA export from nucleus (4), translation (10), protein N-linked glycosylation (1), protein targeting and targeting (3), copper ion transport (5), nucleocytoplasmic transport (14), response to stress (2), protein secretion (3), protein import into mitochondrial matrix (7), mitochondrial respiratory chain complex IV assembly (14), regulation of catalytic activity (8), transmembrane transport (1), vacuolar proton-transporting V-type ATPase complex assembly (5) and mitochondrial outer membrane translocase complex assembly (1).Based on the cellular component classification, the secreted sORFs are associated with roles in communication, differentiation and establishing clonal behaviour. The secreted sORFs predicted that was associated to mating pheromone activity were 34\u201335 amino acids in length. There are 51 predicted sORFs that were associated with functions as membrane features or even in modulating cell membrane thickness or fluidity to respond to changing environmental conditions. One such example is the predicted sORF encoding 52 residues that is associated with plasma membrane proteolipid 3 (Pmp3p), which is part of the phosphoinositide-regulated stress sensor that has a role in the modulation of plasma membrane potential and in the regulation of intracellular ion homeostasis [32].The methods that we have developed from available and proven tools are expected to be easily deployable to other genomes as and when they become available with minimal modifications. Recently, a psychrophilic yeast genome had been reported [33] that has other functional data also available such as gene expression during cold stress [34, 35] and the characterization of proteins involved in cold adaptation [36\u201338]. The mining of such genomes for sORFs that can then be integrated to the functional data may be a cost-effective means of identifying sORFs that are involved in psychrophily or other relevant extremophilic adaptations.The results of our work reveal that a high number of potential sORFs could be overlooked by the standard gene prediction workflow. We therefore recommend that the standard genome annotation process be complemented by analyses that specifically target the annotation of sORFs [39, 40], and then have both results integrated to provide a more complete genome annotation. This workflow is applicable for big data analysis because this study involved a large number of sequences from 31 completed fungal genomes that consisted of intergenics, introns, ORFs and genome sequences. Although the functional validation for predicted sORFs cannot be done based solely on the genome sequence without any corresponding transcriptomic or proteomic data, it is still possible to imply a putative status for the predicted sORFs by evaluating their conservation with the assumption, albeit a very simplistic one, that the observed conservation implies a conserved function of some biological importance and thus less likely to be artefacts of the gene prediction process. Furthermore, the predicted sORFs predicted will be incorporated into a database consisting of sORFs from fungal genomes.A workflow was created to predict sORFs from fungal genomes (Fig. 1) and the components and steps involved are provided below. The source code for the programs in this workflow have been deposited on GitHub - https://github.com/firdausraih/sORFs-fungal-genomes (Additional file 3). The data for sORFs were sourced from two datasets: (i) existing annotations made available with the genome sequences and (ii) a purpose built search.The data for 31 fungi genomes were downloaded via FTP from the NCBI at ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_refseq/Fungi/ (Table 1). These 31 fungi genomes were selected from 36 fungi genomes in NCBI based on the completeness of their genome analysis and annotation.Known or existing sORF annotations were first extracted from the existing genome annotations available for the fungal genomes used. This dataset was restricted to annotations for a maximum length of less than 240 nucleotides or 80 amino acids.The intronic and coding regions for the genomes were identified using Artemis [41] [https://www.sanger.ac.uk/science/tools/artemis] based on the chromosome, scaffold or contig sequences and the protein coding sequences for each genome. The intergenic regions were extracted from the genome annotations in the General Feature Format (GFF) format using a Perl script. The intergenic regions were extracted from both the forward and reverse strands.Gene predictions that specifically targeted the identification of sORFs were done by using sORFfinder-getorf approaches that combined two programs: getorf and sORFfinder. The prediction of the sORFs were carried out for each scaffold and/or chromosome. The sORFs predictions using getorf from the EMBOSS package [23, 42] were restricted to a maximum length of 240 nucleotides. Identification of sORFs by sORFfinder [24] was carried out using a 0.5 probability parameter. The results of sORFfinder, which by default is set at a maximum of 100 amino acids, were then filtered for output containing 80 amino acids in length.The predicted sORFs from getorf and sORFfinder search outputs for each fungi genome were combined and clustered using CD-HIT-EST [25, 43] and those with 100% identify were removed. Unique sequences that represented each cluster were then used as BLAST queries to search against a database of open reading frames (ORFs) for 31 fungal genomes using BLASTX [44, 45]. BLAST hits that aligned to less than two thirds of the query sequences and with less than 30% sequence identity were removed and the remainder were used as a potential sORFs dataset.The pre-annotated sORFs and those that were predicted as potential sORFs were then combined and clustered using CD-HIT at 70% identity to remove clusters that contained only a single sequence. For each cluster, sORFs that have homologs in at least two different species in one cluster were considered as potentially conserved sORFs. All conserved sORFs were identified their Kozak sequences using CPC2 [26].Identification of conserved sORFs in the clusters were carried out using the MUSCLE [46] sequence alignment program. A multiple sequence alignment generated using MUSCLE, which included one out group identified by PSI-BLAST [45], was used as input to construct a phylogenetic tree with 1000 bootstrap replications using the Jones-Taylor-Thornton (JTT) model based on the Neighbor joining method using PHYLIP 3.695 [47, 48].The predicted sORFs were annotated using blast, interpro and classified using BLAST2GO into the three main Gene Ontology classes of molecular function, biological processes and cellular component [49\u201351].Basic Local Alignment Search ToolCluster Database at High Identity with ToleranceThe European Molecular Biology Open Software SuiteFile Transfer Protocol is a standard network protocol used for the transfer dataMUltiple Sequence Comparison by Log-ExpectationNational Center for Biotechnology InformationOpen Reading FramesPHYLogeny Inference PackagePosition-Specific Iterated BLASTsmall Open Reading FramesThese investigations were supported by the grants 02\u201305-20-SF0007/3 from the Ministry of Science, Technology & Innovation (MOSTI) and DIP-2017-013 from Universiti Kebangsaan Malaysia as well as a MyPHD scholarship and attachment funding via LEP 2.0/14/UKM/BT/02/2 from the Ministry of Education Malaysia to SMS. Publication costs are funded by Centre for Research Instrumentation and Management (CRIM) and the Faculty of Science and Technology, Universiti Kebangsaan Malaysia.The datasets used and/or analysed during the current study are available from the NCBI at ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_refseq/Fungi/ and all sORFs identified in this study are in the Additional file 2 provided.This article has been published as part of BMC Bioinformatics, Volume 19 Supplement 13, 2018: 17th International Conference on Bioinformatics (InCoB 2018): bioinformatics. The full contents of the supplement are available at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-19-supplement-13.Not applicable.Not applicable.The authors declare no conflicts of interest.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.CD-HIT [25] clustering of the combined 47,842 potential sORFs at 70% sequence identity resulted in 730 sORFs clusters that comprised of 1986 sequences putatively conserved in at least two fungal species (Fig.\u00a01). Four of the sORFs predicted have Kozak sequences based on an ORF integrity value that was derived by calculating their coding potential using CPC2 [26]. The majority of the sORFs predicted were in the under 40 residues range with the shortest sORF in this dataset being composed of only 11 amino acids (Fig.\u00a02).\n\nFig. 1\nResearch workflow\n\n\nFig. 2\nDistribution of sORFs according to length for all 31 fungal genomes\nThe clustering based on 70% similarity of the 47,842 potential sORFs resulted in a total of 1986 putative sORFs that are conserved within at least two fungal species (Additional file 1). Among the 1986 putatively conserved sORFs, 927 have homologs with known functions (35 from the purpose built sORF prediction process; 892 from existing genome annotations) (Fig.\u00a03). The remainder 1059 putatively conserved sORFs have uncharacterized functions and can be further divided into two categories: the first set - 23 sORFs with homologs outside of the 31 fungal genomes; and the second set - 1036 sORFs with no detectable sequence homologs outside of the 31 fungal genomes. The latter set can thus be considered as fungi specific sORFs.\n\nFig. 3\nDistribution of total of conserved sORFs across fungal genomes\nAt the time this work was initiated, 31 fungal genomes were selected because they had relatively complete genome sequences and had accompanying annotations. All the selected genomes were from the kingdom fungi and from various phyla including Ascomycota, Basidiomycota and Microsporidia (Table\u00a01). Due to this diversity, we therefore believe that the workflow developed would be widely applicable for all fungi and possibly for other kingdoms as well.Table 1\nList of sORFs predicted from the whole genome and intergenic regions of fungal genomes\nAlthough there were no predicted sORFs that were conserved in all 31 genomes, there were 68 sORFs from two homologous clusters that were present in 26 of the 31 fungal genomes. Additionally, there are 1663, 215, and 40 sORFs that could be found in \u00bc, \u00bd and \u00be of the 31 genomes, respectively (Fig.\u00a04). The two clusters identified by the genome screening approach consist of sORFs that are homologous to 40S ribosomal protein S28 (Fig.\u00a05a i) and 40S ribosomal protein S30 (Fig. 5a ii). In the first cluster, 11 sORFs from eight species, Cryptococcus neoformans, Candida glabrata, Eremothecium cymbalariae, Kazachstania africana, Naumovozyma castellii, Agaricus bisporus, Aspergillus nidulans and Myceliopthora thermophila that were originally annotated as hypothetical proteins, were updated to be homologs of 40S ribosomal protein S28. The annotation for this homology assignment was obtained using BLAST and domain analysis using InterProScan. Furthermore, the evolutionary analysis on this cluster showed that all of these sORFs are conserved in fungi and are closely related in the fungal group when compared against the outgroup, Ananas comosus (pineapple) (Fig.\u00a06a). This demonstrates the utility of reannotation projects in general and especially when they are designed to identify specific targets such as the one we have carried out in updating the existing annotation.\n\nFig. 4\nClustered conserved sORFs\n\n\nFig. 5\nMultiple sequence alignments for sORFs that are conserved within (a) 26 fungal genomes (i-xx3497 and ii-xx4629) and (b) 2/4 fungal genomes (i-xx4249 and ii-xx6165) based on clustering. The sORFs extracted from genome annotations have identifiers with \u2018*gi*\u2019 while those computed from this work have identifiers with \u2018*sf*\u2019\n\n\nFig. 6\nPhylogenetics of conserved sORFs within (a) 26 fungal genomes (xx3497) and (b) 2/4 fungal genomes (xx4249) based on clustering\nThe function of sORFs in the first alignment set, which are conserved in about half of the 31 genomes (Fig. 5b i), are proteolipid membrane potential modulators that modulate the membrane potential, particularly to resist high cellular cation concentration. In eukaryotic organisms, stress-activated mitogen-activated protein kinases normally play crucial roles in transmitting environmental signals that will regulate gene expression for allowing the cell to adapt to cellular stress [30]. This protein is an evolutionarily conserved proteolipid in the plasma membrane which, in S. pombe, is transcriptionally regulated by the Spc1 stress MAPK (mitogen-activated protein kinases) pathway. There are two sORFs (C. dubliniensis-sf4096_1 and P. pastoris-sf9282_1) from the computational reannotation that were clustered together with conserved alignments, and thus indicating they may have the same function (Table\u00a02). Further evolutionary analysis of this cluster showed that all of the sORFs in this cluster are closely related in fungal group against bacteria, Halmonas xinjiangensis (Fig. 6b).Table 2\nCharacterization of sORFs conserved in 15 fungal genomes\nThe second alignment in Fig. 5b ii shows four sORFs predicted from the reannotation that are homologs to the 60S ribosomal protein. This is a possible indicator that sORFs may have been missed during a standard genome annotation process. Our analysis identified a higher number of sORFs candidates in S. cerevisiae compared to that published by Kastenmayer et al. [14]. The total of 77 sORFs predicted for S. cerevisiae contained all the 16 sORFs predicted by Kastenmayer et al. There are 20 sORFs in this set that were predicted by the sORFfinder and getorf integrated prediction process (Table\u00a03). The other 57 sORFs predicted for S. cerevisiae have already been previously identified and was extracted from the genome screening approach.Table 3\nList of sORFs predicted in S. cerevisiae\nThe total 1986 predicted sORFs were blast searched against the refseq database [31] and classified according to the three major Gene Ontology (GO) classes of molecular function, biological process and cellular component. Of the 1986 sORFs predicted, only 617 predicted sORFs could not be classified according to GO classes. This resulted in 2746 putative sORFs being classified into biological process, 4546 putative sORFs classified as cellular components and 155 predicted sORFs classified to be involved in molecular function (Fig.\u00a07). The number of genes resulting from the Gene Ontology classification are higher than the total number of predicted sORFs predicted because one gene can be associated with multiple classes. The overall classification showed that most of the sORFs predicted have roles in biosynthesis and nucleic acid metabolism.\n\nFig. 7\nClassification of predicted conserved sORFs based on Gene Ontology. The solid colors represents cellular components, the dot patterns represents biological processes and cross patterns represents molecular functions\n\n\nAdditional file 1:\nA listing of 1986 putative conserved sORFs predicted. This file contains a list of 1986 putative conserved sORFs predicted from all fungal genomes that can be viewed using Microsoft excel or text viewer. (TXT 151 kb)\n\n\n\nAdditional file 2:\nList of predicted sORFs with homologs. This file contains a list of sORFs predicted from all fungal genomes with their homologs that can be viewed using Microsoft excel or a text viewer. (TXT 118 kb)\n\n\n\nAdditional file 3:\nPseudocode for sORFs workflow. This file contains a pseudocode for finding sORFs workflow using Linux environment using BASH, PYTHON and the Perl programming language. (ZIP 8 kb)", "s12859-019-2630-y": "Dynamic modelling is a core element in the systems biology approach to understanding complex biosystems. Here, we consider the problem of parameter estimation in models of biological oscillators described by deterministic nonlinear differential equations. These problems can be extremely challenging due to several common pitfalls: (i) a lack of prior knowledge about parameters (i.e. massive search spaces), (ii) convergence to local optima (due to multimodality of the cost function), (iii) overfitting (fitting the noise instead of the signal) and (iv) a lack of identifiability. As a consequence, the use of standard estimation methods (such as gradient-based local ones) will often result in wrong solutions. Overfitting can be particularly problematic, since it produces very good calibrations, giving the impression of an excellent result. However, overfitted models exhibit poor predictive power.Here, we present a novel automated approach to overcome these pitfalls. Its workflow makes use of two sequential optimisation steps incorporating three key algorithms: (1) sampling strategies to systematically tighten the parameter bounds reducing the search space, (2) efficient global optimisation to avoid convergence to local solutions, (3) an advanced regularisation technique to fight overfitting. In addition, this workflow incorporates tests for structural and practical identifiability.We successfully evaluate this novel approach considering four difficult case studies regarding the calibration of well-known biological oscillators (Goodwin, FitzHugh\u2013Nagumo, Repressilator and a metabolic oscillator). In contrast, we show how local gradient-based approaches, even if used in multi-start fashion, are unable to avoid the above-mentioned pitfalls.Our approach results in more efficient estimations (thanks to the bounding strategy) which are able to escape convergence to local optima (thanks to the global optimisation approach). Further, the use of regularisation allows us to avoid overfitting, resulting in more generalisable calibrated models (i.e. models with greater predictive power).Oscillations and sustained rhythms are pervasive in biological systems and have been deeply studied in areas such as metabolism [1\u20135], the cell cycle [6\u20139], and Circadian rhythms [10\u201316], to name but a few. In recent years, many research efforts have been devoted to the development of synthetic oscillators [17\u201320], including tunable ones [21\u201324].Mathematical and computational approaches have been widely used to explore and analyse these complex dynamics [25\u201332]. Model-based approaches have also allowed for the identification of design principles underlying circadian clocks [12, 33] and different types of biochemical [34] and genetic oscillators [35\u201339]. Similarly, the study of the behaviour of populations of coupled oscillators has also greatly benefited from mathematical analysis and computer simulations [40\u201347].A number of approaches can be used to build mathematical models of these biological oscillators [48\u201352]. This process is sometimes called reverse engineering, inverse problem solving or dynamic model identification [53\u201355]. Model calibration (i.e. parameter estimation or data fitting) is a particularly important and challenging sub-problem in the identification process [56\u201364]. Different strategies have been specially developed and applied to calibrate models of oscillators [32, 50, 65\u201371] and to characterise and explore their parameter space [31, 72\u201376].In this study, we consider parameter estimation in mechanistic dynamic models of biological oscillators. From all the issues that plague model calibration [77], we pay special attention to three that are particularly problematic in oscillatory models: huge search spaces, multimodality and overfitting. We also discuss how to handle lack of identifiability.where \\(\\mathbf {x} \\in \\mathbb {R}^{N_{\\mathbf {x}}}\\) represents the states of the system as time-dependent variables, under the initial conditions x0, \\(\\boldsymbol {\\theta } \\in \\mathbb {R}^{N_{\\boldsymbol {\\theta }}}\\) is the parameter vector, u(t) represents any time-dependent input (e.g. stimuli) affecting the system and \\(t \\in \\left [t_{0}, t_{end}\\right ] \\subset \\mathbb {R}\\) is the time variable. \u03a8O represents the set of all possible oscillatory dynamics. The observation function \\(g : \\mathbb {R}^{N_{\\mathbf {x}}\\times N_{\\boldsymbol {\\theta }}}\\mapsto \\mathbb {R}^{N_{\\mathbf {y}}}\\) maps the states to a vector of observables \\(\\mathbf {y}\\in \\mathbb {R}^{N_{\\mathbf {y}}}\\), i.e. the state variables that can be measured. While the methodology here is developed for and tested on oscillatory models, it is not strictly restricted to models that exhibit such behaviour.We denote the solution to this minimisation problem as \\(\\widehat {\\boldsymbol {\\theta }}\\). In principle, this problem could be solved by standard local optimisation methods such as Gauss-Newton or Levenberg-Marquardt. However, as described next, there are many pitfalls and issues that complicate the application of these methods to many real problems.Numerical data fitting in nonlinear dynamic models is a hard problem with a long list of possible pitfalls, including [77]: a lack of identifiability, local solutions, badly scaled data and parameters, oscillating dynamics, inconsistent constraints, non-differentiable model functions, slow convergence and errors in experimental data. It should be noted that several of these difficulties are interrelated, e.g. a lack of practical identifiability can be due to noisy and non-informative data and will result in slow convergence and/or finding local solutions.In the case of oscillators, the above issues apply, particularly multimodality and lack of identifiability. However, there are at least two additional important difficulties that must be also considered: overfitting (i.e. fitting the noise rather than the signal) and very large search spaces (which creates convergence difficulties and also makes it more likely the existence of additional local optima). Although these four issues are all sources of difficulties for proper parameter estimation, the last two have been less studied.The objective of identifiability analysis is to find out whether it is possible to uniquely estimate the values of the unknown model parameters [79]. It is useful to distinguish between two types of identifiability: structural and practical. Structural identifiability [80, 81] studies if the model parameters can be uniquely determined assuming ideal conditions for the measurements and therefore only considering the model dynamics and the input-output mapping (i.e. what is perturbed and what is observed). Structural identifiability is sometimes called a priori identifiability. Despite recent advances [82\u201384], structural identifiability analysis remains difficult to apply to large dynamic models with arbitrary nonlinearities.It is important to note that, even if structural identifiability holds, unique determination of parameter values is not guaranteed since it is a necessary condition but not a sufficient one. Practical identifiability analysis [85\u201387] considers experimental limitations, i.e. it aims to find if parameter values can be determined with sufficient precision taking into account the limitations in the measurements (i.e. the amount and quality of information in the observed data). Practical (sometimes called a posteriori) identifiability analysis will typically compute confidence intervals of the parameter values. Importantly, it can also be taken into account as an objective in optimal experimental design [86].Schittkowski [77] puts emphasis on the extremely difficult nature of data fitting problems when oscillatory dynamics are present: the cost function to be minimised will have a large number of local solutions and an irregular structure. If local optimisation methods are used, they will likely converge to one of these local solutions (typically the one with the basin of attraction that includes the initial guess). Several researchers have studied the landscape of the cost functions being minimised, describing them as very rugged and with multiple local minima [77, 88, 89]. Thus, this class of problems clearly needs to be solved with some sort of global optimisation scheme as illustrated in a number of studies during the last two decades [57, 86, 90\u201393].The simplest global optimisation approach (and widely used in parameter estimation) is the so-called multi-start method, i.e. a (potentially large) number of repeated local searchers initialised from usually random initial points inside the feasible space of parameters. Although a number of studies have illustrated the power of this approach [94\u201396], others have found that it can be inefficient [92, 97\u201399]. This is especially the case when there is a large number of local solutions: in such situations, the same local optima will be repeatedly found by many local searches, degrading efficiency.Thus, several methods have tried to improve the performance of the plain multi-start method by incorporating mechanisms to avoid repeated convergence to already found local solutions. This is the case of hybrid metaheuristics, where a global search phase is performed via diversification mechanisms and combined with local searches (intensification mechanisms). In this context, the enhanced scatter search (eSS) method has shown excellent performance [64, 97, 99, 100]. Here we will use an extension of the eSS method distributed in the MEIGO toolbox [101] as the central element of our automated multi-step approach. We have modified the MEIGO implementation of eSS in several ways, as detailed in Additional file\u00a01. In order to illustrate the performance and robustness of eSS with respect to several state-of-the-art local and global solvers, we provide a critical comparison in Additional file\u00a02.In this study, we consider the common case scenario where little prior information about (some or all of the) parameters is available and therefore we need to consider ample bounds in the parameter estimation. These huge parameter bounds complicate convergence from arbitrary initial points, increase computation time and make it more likely that we will have a large number of local solutions in the search space. Although deterministic methods, which could be used to systematically reduce these bounds exist [102\u2013104], currently they do not scale up well with problem size. Such techniques therefore can not be applied to problems of realistic size. Some analytical approaches have also been used for the analysis of biological oscillators [26, 31]. Alternatively, non-deterministic sampling techniques have been used to explore the parameter space and identify promising regions consistent with pre-specified dynamics [105]. Inspired by these results, we will re-use the sampling performed during an initial optimisation phase to reduce the parameter bounds.Overfitting describes the problem associated with fitting the noise in the data, rather than the signal. Overfitted models can be misleading as they present a low-cost function value, giving the false impression that they are well-calibrated models that can be useful for making predictions. However, overfitted models have poor predictive power, i.e. they do not generalise well and can result in major prediction artefacts [106]. In order to fight overfitting, a number of regularisation techniques have been presented. Regularisation methods originated in the area of inverse problem theory [107]. Most regularisation schemes are based on adding a penalty term to the cost function, based on some prior knowledge of the parameters. This penalty makes the problem more regular, in the sense of reducing ill-conditioning and by penalising wild behaviour. Regularisation also can be used to minimise model complexity.However, regularisation methods for nonlinear dynamics models remain an open question [99]. Further, these methods require some prior knowledge about the parameters and a tuning process which can be cumbersome and computationally demanding. Here, we will present a workflow that aims to automate this process.In order to graphically illustrate several of the above issues, let us consider the ENSO problem, a small yet challenging example taken from the National Institute of Standards (NIST) nonlinear least squares (NLLS) test suite [108].Here we present a novel methodology, GEARS (Global parameter Estimation with Automated Regularisation via Sampling), that aims to surmount the pitfalls described above. Our method combines three main strategies: i) global optimisation, (ii) reduction of the search space and (iii) regularised parameter estimation. In addition to these strategies, the method also incorporates identifiability analysis, both structural and practical. All these strategies are combined in a hands-off procedure, requiring no user supervision after the initial information is provided.The method first performs two pre-processing steps. The first is a structural identifiability analysis test. A second pre-processing step involves symbolic manipulation to generate the components needed for the efficient computation of parametric sensitivities. After the pre-processing steps, the method performs a first global optimisation run using eSS and a non-regularised cost function. This step is used to obtain useful sampling information about the cost function landscape, which is then used to perform parameter bounding and regularisation tuning. This new information is then fed into a second global optimisation run, again using eSS but now with a regularised cost function and the new (reduced) parameter bounds. The outcome of this second optimisation is the regularised estimate, which is then subject to several post-fit analysis, including practical identifiability and cross-validation (using dataset II). Details regarding each of these steps are given below.The structural identifiability analysis step allows us to ensure that based on the model input-output (observation) mapping we are considering, we should in principle be able to uniquely identify the parameter values of the model (note that it is a necessary but not sufficient condition). If the problem is found to be structurally non-identifiable, users should take appropriate actions, like model reformulation, model reduction or by changing the input-output mapping if possible.In our workflow, we analyse the structural identifiability of the model using the STRIKE-GOLDD package [82], which tests identifiability based on the rank of the symbolic Lie derivatives of the observation function. It can then detect each structurally non-identifiable parameter based on rank deficiency.In GEARS we use a single-shooting approach, i.e. the initial value problem (IVP) is solved for each valuation of the cost function inside the iterative optimisation loop. It is well known that gradient-based local methods, such as those used in eSS, require high-quality gradient information.Solving the IVP (original, or extended for sensitivities) is the most computationally expensive part of the optimisation, so it is important to use efficient IVP solvers. In GEARS we use AMICI [110], a high level wrapper for the CVODES solver [111], currently regarded as the state of the art. In order to obtain the necessary elements for the IVP solution, the model is first processed symbolically by AMICI, including the calculation of the Jacobian. It should be noted that an additional advantage of using AMICI is that allows the integration of models with discontinuities (including events and logical operations).The objective of this step is to perform an efficient sampling (storing all the points tested during the optimisation) of the parameter space. This sampling will then be used to perform (i) reduction of parameter bounds, and (ii) tuning of the regularisation term to be used in the second optimisation phase.where NS is the number of function evaluations, N\u03b8 is the number of parameters and each \\(Q_{NLS}\\left (\\boldsymbol {\\theta }^{S}_{i}\\right)\\) is a parameter vector selected by eSS.The sampling obtained in the global optimisation phase 1 is now used to reduce the bounds of the parameters, making the subsequent global optimisation phase 2 more efficient and less prone to the issues detailed above. We first compute calculate a cost cut-off value for each parameter using Algorithm 1. This algorithm is used to determine reasonable costs, whereby costs deemed to be far from the global optimum are rejected. We calculate one cost cut off for each parameter, as different parameters have different relationships to the cost function. Once these cut-off values have been calculated for each parameter, we apply Algorithm 2 to obtain the reduced bounds.where \u03b1 is a weighting parameter regulating the influence of the regularisation term.Once the regularised cost function is built, we need to tune the regularisation parameters. Once again, we start from the cost cut off values calculated in Algorithm 2. We also use the reduced parameter bounds to ensure that our regularisation parameters and reduced parameter bounds do not conflict each other. The procedure for calculating the values for the regularisation parameters \u03b1 and \u03b8ref can be found in Algorithm 3.We denote the solution to this regularised estimation problem as \\(\\widehat {\\boldsymbol {\\theta }}^{R}\\).The next step is to analyse the practical identifiability of the regularised solution. This is done using an improved version of the VisId toolbox [87]. The VisId toolbox accesses practical identifiability based on testing collinearity between parameters. A lack of practical identifiability is typically due to a lack of information in the available fitting data, and in principle can be surmounted by a more suitable experimental design [86].Next, we assess the level of overfitting using cross-validation. This is typically done by comparing the fitted model predictions with experimental data obtained under conditions (e.g. initial conditions) different from the ones used in the fitting dataset. In other words, cross-validation tests how generalisable the model is. Here, we perform cross-validation on both the non-regularised \\(\\widehat {\\boldsymbol {\\theta }}^{I}\\) and regularised solution \\(\\widehat {\\boldsymbol {\\theta }}^{R}\\). This allows us to assess the reduction of overfitting due to the regularised estimation.We use the NRMSE measure to assess both the quality of fit and quality of prediction. One important caveat to note here is that some of these post-fit analyses are based on the Fisher information matrix (FIM) for their calculation. This is a first order approximation and can be inaccurate for highly nonlinear models [114]. In those instances, bootstrapping techniques are better alternatives, although they are computationally expensive.The methodology proposed here has been implemented in a Matlab toolbox, \u201cGEARS: Global parameter Estimation with Automated Regularisation via Sampling\u201d. GEARS is a software package that can be downloaded from https://japitt.github.io/GEARS/, made freely available under the terms of the GNU general public license version 3. GEARS runs on Matlab R2015b or later and is multi-platform (tested on both Windows and Linux). The Optimisation and Symbolic Math Matlab toolboxes are required to run GEARS. In addition to this, GEARS requires the freely available AMICI package (http://icb-dcm.github.io/AMICI/) to solve the initial value problem. Optionally, it also requires the Ghostscript software (https://www.ghostscript.com) for improved exportation of results. For more details please see the documentation within the GEARS toolbox. It should be noted that for the structural and practical identifiability analysis steps, users need to install the VisId [87] and STRIKE-GOLDD [82] packages respectively. These packages are freely available at https://github.com/gabora/visid and https://sites.google.com/site/strikegolddtoolbox/\nrespectively.where y is the observation function considered in the example. The flexibility of the model dynamics makes this model prone to overfitting. Synthetic data was generated taking nominal parameter values {a,b,g}={0.2,0.2,3}. The fitting data was generated with initial conditions of V0=\u22121,R0=1.where variables {x1,x2,x3} represent concentrations of gene mRNA, the corresponding protein, and a transcriptional inhibitor, respectively; yF is the observation function for the estimation problem, and yV is the observation function for the cross-validation procedure. Synthetic data was generated considering nominal parameter values {k1\u22126,Ki,n}={1,0.1,1,0.1,1,0.1,1,10}. The fitting datasets were generated for the initial conditions x1\u22123,0= [ 0.1,0.2,2.5]. It is important to note that we have considered an additional observable for cross-validation, which makes the problem much more challenging (i.e. it exacerbates the prediction problems due to overfitting).where yF is the observation function for the fitting procedure and yV is the observation function for the cross-validation procedure (i.e. an additional observable for cross-validation). Synthetic data was generated considering nominal parameter values {k1\u22126,Ki,n}= [ 0.05,298,8.5,0.3]. The fitting data was generated for the initial conditions given by [p1\u22123,0,m1\u22123,0]= [ 10,0.01,1,1,0.01,10].where y is the observation function. Synthetic data was generated considering nominal parameter values \u03b8= [ 0.4,500,10,10,0.07,7,2.5]. An important point to note here is that these parameter values were chosen to be in the vicinity of, but not inside, the region with chaotic behaviour. For the fitting datasets the initial conditions [\u03b10,\u03b20,\u03b30]= [ 29.1999,188.8,0.3367] were used.All the problems were solved with GEARS using 10 different datasets for fitting and 10 additional datasets for cross-validation. For the RP and GO models an extra observable was considered in the case of the cross-validation. To illustrate the results obtained during the different steps, we will focus on the FHN problem. Detailed results for all the other case studies are given in Additional files\u00a01 and 3. The GEARS software distribution includes all the scripts implementing these case studies.First, the structural identifiability of these problems was analysed using STRIKE-GOLDD [82], concluding that all of them are identifiable a priori. This analysis also revealed that for the GO case study the initial conditions for the unobserved state x2 must also be known, otherwise two parameters are structurally unidentifiable.More detailed results for FHN and all the other case studies can be found in Additional files\u00a01 and 3. Regarding practical identifiability, using the VisId toolbox we found that both the FHN and RP problems are fully identifiable in practice indicating that our dataset contains enough information. In the case of the EO and GO models we found that there are a number of collinear parameters sets indicating that, at least to some degree, these parameters are compensating one another due to a lack of information in the data.In summary, using GEARS we have been able to successfully calibrate these challenging oscillators models, avoiding the typical pitfalls, including convergence to local optima and overfitting, in an automated manner.Parameter estimation in nonlinear dynamic models of biosystems is a very challenging problem. Models of biological oscillators are particularly difficult. In this study, we present a novel automated approach to overcome the most common pitfalls. Its workflow makes use of identifiability analysis and two sequential optimisation steps incorporating three key algorithms: (1) sampling strategies to systematically tighten the parameter bounds reducing the search space, (2) efficient global optimisation to avoid convergence to local solutions, (3) an advanced regularisation technique to fight overfitting.We evaluate this novel approach considering four difficult case studies regarding the calibration of well known biological oscillators (Goodwin, FitzHugh\u2013Nagumo, Repressilator and a metabolic oscillator). We show how our approach results in more efficient estimations which are able to escape convergence to local optima. Further, the use of regularisation allows us to avoid overfitting, resulting in more generalisable calibrated models (i.e. models with greater predictive power).Enzymatic Oscillator modelEnhanced scatter searchFiztHugh-Nagumo modelGoodwin Oscillator modelInitial value problemNational institute of standardsNonlinear least squaresRepressilator modelThis research has received funding from the European Union\u2019s Horizon 2020 research and innovation program under grant agreement No 675585 (MSCA ITN \u201cSyMBioSys\u201d) and from the Spanish MINECO/FEDER project SYNBIOCONTROL (DPI2017-82896-C2-2-R). Jake Alan Pitt is a Marie Sk\u0142odowska-Curie Early Stage Researcher at IIM-CSIC (Vigo, Spain) under the supervision of Prof Julio R. Banga. The funding bodies played no role in the design of the study, the collection and analysis of the data or in the writing of the manuscript.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Here, we consider mechanistic models of oscillatory biological systems given by deterministic nonlinear ordinary differential (ODEs). The general model structure is: \n (1)\n (2)\n\n (3)\nWe now consider the parameter estimation problem considering dynamic models described by the above Eqs. (1 \u2013 3). We formulate this estimation problem as a maximisation of the likelihood function given by: \n (4)where Ne is the number of experiments, Ny,k the number of observables in those experiments, Nt,k,j is the number of time points for each observable, \\(\\tilde {y}_{kji}\\) represents the measured value for the ith time point of the jth observable in the kth experiment, and \u03c3kji represents its corresponding standard deviation. Under specific conditions [78], the maximisation of the likelihood formulation is equivalent to the minimisation of the weighted least squares cost given by: \n (5)Using the above cost, the estimation problem can be formulated as the following minimisation problem: \n (6)subject to the dynamic system described by Eqs. (1\u20133), and also subject to the parameter bounds: \n (7)To visualise the multimodality of this problem, we can use contour plots of the cost function for pairs of parameters, as shown in Fig.\u00a01. In this figure we also show the convergence paths followed by a multi-start of a local optimisation method (NL2SOL [109]), illustrating how most of the runs converge to local solutions or saddle points close to the initial point. We can also see how different runs converge to the same local solutions, explaining the low efficiency of multi-start for problems with many local optima. We also provide additional figures for this problem in Additional file\u00a01.\n\n\nFig. 1\nENSO problem NL2SOL contours: contours of the cost function (nonlinear least squares) for parameters b4 and b7, and trajectories of a multi-start of the NL2SOL local solver\n\nIn contrast, we plot the convergence paths of the enhanced scatter search (eSS) method in Fig.\u00a02, showing how the more efficient handling of the local minima allows this strategy to successfully and consistently find the global solution even from initial guesses that are far from the solution. It should be noted that while NIST lists this ENSO problem as being of \u201caverage\u201d difficulty, this is largely due to the excellent starting point considered in their test suite, which are extremely close to the solution. Indeed, we can see in the aforementioned figures that the choice of parameter bounds and initial guess can dramatically change the difficulty of the problem.\n\n\nFig. 2\nENSO problem eSS contours: contours of the cost function (nonlinear least squares) for parameters b4 and b7, and trajectories of the enhanced scatter search (eSS) global optimisation solver initialized from various starting points\n\nAn overview of the entire procedure can be seen in Fig.\u00a03. The initial information required by the method includes the dynamic model to be fitted (as a set of ODEs), the input-output mapping (including the observation function) and a data set for the fitting (dataset I). A second data set is also needed for the purposes of cross-validation and evaluation of overfitting (dataset II). Additionally, users can include (although it is not mandatory) any prior information about the parameters and their bounds. If the latter is not available, users can just declare very ample bounds, since the method is prepared for this worst-case scenario.\n\n\nFig. 3\nWorkflow of procedure: a schematic diagram of the GEARS method\n\nThe cost function used is a weighted least-squares criterion as given by Eqs. (5\u20137). The estimation problem is solved using the enhanced scatter search solver (eSS, [112]), implemented in the MEIGO toolbox [101]. Within the eSS method, we use the gradient-based local solver NL2SOL [109]. In order to maximise its efficiency, we directly provide the solver with sensitivities calculated using AMICI. After convergence, eSS finds the optimal parameters vector for the fitting data \\(\\widehat {\\boldsymbol {\\theta }}^{I}\\). While this solution might fit the dataset I very well, it is rather likely that it will not have the highest predictive power (as overfitting may have occurred). During the optimization, we store each for function evaluation the parameter vector \\(\\boldsymbol {\\theta }^{S}_{i}\\) and its cost value \\(Q_{NLS}\\left (\\boldsymbol {\\theta }^{S}_{i}\\right) = \\zeta _{i}\\), building the sampling: \n (8)\n (9)\n\n\n\n\n\n\n\n\nThe next step builds an extended cost function using a Tikhonov-based regularisation term. This is a two norm regularisation term given by: \n (10)\n (11)\nwhere W normalises the regularisation term with respect to \u03b8ref, to avoid bias due to the scaling of the reference parameter. The extended cost function is as follows: \n (12)\n\n\n\nOnce we have calculated both the values of the regularisation parameters and the reduced parameter bounds, we are now able to perform the final regularised optimisation. We use the same set up for the global optimisation solver (eSS with NL2SOL as the local solver, and AMICI as the IVP solver). We then solve the regularised parameter estimation problem given by: \n (13)Subject to the system described in Eqs. 1\u20133, and the reduced parameter bounds given by: \n (14)In addition to cross-validation, several post-fit statistical metrics are also computed: normalised root mean square error (NRMSE), R2 and \u03c72 tests [113], parameter uncertainty (confidence intervals computed using the Fisher information matrix, FIM), and parameter correlation matrix (also computed using the FIM). The normalised root mean square error is a convenient metric for the quality of fit, given by: \n (15)Next, we consider four case studies of parameter estimation in dynamic models of biological oscillators. The general characteristics of these problems are given in Table\u00a01. While these problems are small in terms the number of parameters, they exhibit most of the difficult issues discussed above, such as overfitting and multimodality, making them rather challenging. For each case study, synthetic datasets (i.e. pseudo-experimental data generated by simulation from a set of nominal parameters) were generated. For each case study, 10 fitting datasets with equivalent initial conditions were produced, plus a set of 10 additional cross-validation data sets (where the initial conditions were changed randomly within a reasonable range). All these data sets were generated using a standard deviation of 10.0% of the nominal signal value and a detection threshold of 0.1.\nTable 1\nSummary of case studies considered\n\nThis problem considers the calibration of a FitzHugh-Nagumo model, which is a simplified version of the Hodgkin-Huxley model [115], describing the activation and deactivation dynamics of a spiking neuron. We consider the FHN model as described by Eqs. 16\u201321: \n (16)\n (17)\n\n (18)\n\n (19)\n\n (20)\n\n (21)\nThe Goodwin oscillator model describes control of enzyme synthesis by feedback repression. The GO model is capable of oscillatory behaviour in particular areas of the parameter space. Griffith [26] showed that limit-cycle oscillations can be obtained only for values of the Hill coefficient n\u22658 (note that this information could be used to bound this parameter but here we will not use it, assuming a worst case scenario with little prior information available). The GO model suffers from some identifiability issues as well as tendency to overfitting. The dynamics are given by Eqs. 22\u201328: \n (22)\n (23)\n\n (24)\n\n (25)\n\n (26)\n\n (27)\n\n (28)\nThe Repressilator is a well-known synthetic gene regulatory network [17]. We consider the particular parameter estimation formulation studied by [69] with dynamics given by Eqs. 29\u201337: \n (29)\n (30)\n\n (31)\n\n (32)\n\n (33)\n\n (34)\n\n (35)\n\n (36)\n\n (37)\nThe enzymatic oscillator is a small biochemical system model that illustrates the effect of coupling between two instability-generating mechanisms [116]. Parameter estimation in this system is particularly challenging due to the existence of a variety of modes of dynamic behaviour, from simple periodic oscillations to birhythmicity and chaotic behaviour. The chaotic behaviour is restricted to a particular region of the parameter space as discussed in [116]. Its dynamics are difficult even for regions with simple periodic behaviour: the existence of extremely steep oscillations causes small shifts in the period of oscillations to have a large impact on the estimation cost function. We consider the dynamics described by Eqs. 38\u201343: \n (38)\n (39)\n\n (40)\n\n (41)\n\n (42)\n\n (43)\nNext, the GEARS workflow proceeded performing an initial estimation. The samples obtained were analysed by applying a cut off to each parameter as seen in Fig.\u00a04 for the FHN problem. The cost cut-off values were then used to significantly reduce the parameter bounds, as shown in Table\u00a02 and Fig.\u00a05. Next, GEARS performed the regularised estimation step (Table\u00a03). We are successful able to avoid the multiple local solutions that are frequently found using local optimisers (see Fig.\u00a06). As expected, this second estimation reduced the quality of the calibration to the fitting data, as shown in Table\u00a04, Figs.\u00a07a and 8. However, it increased the generalisability of the model, as can be seen in Fig.\u00a07 and Table\u00a04. Finally, GEARS confirmed a satisfactory practical identifiability for the resulting calibrated FHN, as indicated by the rather small confidence intervals for the estimated parameters.\n\n\nFig. 4\nFHN case study: parameter samples. Parameter samples, showing the cost cut off values and the reduced bounds for each parameter\n\n\nFig. 5\nFHN case study: reduction of parameters bounds. Original and reduced parameter bounds, also showing the parameter confidence levels for the first fitting data set\n\n\nFig. 6\nFHN case study: distribution of local solutions. Histogram of the local solutions found with a multi-start local solver; the arrows indicate examples of underfitting and overfitting\n\n\nFig. 7\nFHN problem: effect of regularisation on fitting and cross-validation. An example of how regularisation affects the calibration of the FHN model: reduction of the quality of the fitting (subplot (a)), but improvement on the quality of the cross-validation (subplot (b)); the corresponding numerical results are given in Table\u00a04\n\n\nFig. 8\nFHN problem fit with uncertainty: final regularised fit with uncertainty intervals coonsidering the first fitting data set\nTable 2\nFHN case study: bounds reduction. A table showing the bounds reduction performed on the FHN model for the first fitting data set\nTable 3\nFHN case study: summary of the regularised results. A summary of the final results of the procedure applied to the FHN model for the first fitting data set\nTable 4\nFHN case study: summary of NRMSE results. A table of the NRMSE for the FHN case study. The fitting is to one particular experiment, while the cross-validation covers ten experiments for the fit to the first fitting data set\n\nConsidering now all the case studies, it is important to assess the consistency of the effect of regularisation on their generalisability. In Fig.\u00a09 we show how the regularised estimation always decreases the quality of calibration to the initial fitting data, as expected. However, the regularised estimations lead to better cross-validation results, i.e. these calibrated models have better predictive power because we have avoided overfitting. It should be noted, however, that there are a few cases where the procedure is unable to significantly improve the generalisability of the model. The explanation is that no real overfit was present in the initial calibrations.\n\n\nFig. 9\nEffect of regularisation for all the case studies. Violin plots illustrating the effect of regularisation on the fitting and cross-validation errors (given as normalized root mean square error, NRMSE) for each model and over all the data sets considered. It is shown how regularisation increases the NRMSE for the fitting, but with the benefit of generally improving the predictive power, i.e. reducing the NRMSE in the cross-validations\n\n\n\nAdditional file 1\nRemarks on the eSS optimisation solver and detailed results for the Goodwin Oscillator problem and additional ENSO contour plots. This file describes our modifications to the eSS global optimisation solver. It also contains tables and figures showing the detailed results for the Goodwin Oscillator problem, and additional contour plots for the ENSO example. (PDF 1730 kb)\n\n\n\n\nAdditional file 2\nCritical comparison of optimisation solvers. In GEARS, the optimisation problems are solved using the hybrid metaheuristic eSS. A comparison of the eSS global optimisation solver used in GEARS with other competitive local and global optimisation solvers. (PDF 880 kb)\n\n\n\n\nAdditional file 3\nDetailed results for the Fitzhugh-Nagumo, Repressilator and Enzymatic Oscillator problems. This file contains tables and figures showing the detailed results for the Fitzhugh-Nagumo, Repressilator and Enzymatic Oscillator problems. (PDF 1680 kb)", "s12859-019-2638-3": "Biomarker discovery studies have been moving the focus from a single target gene to a set of target genes. However, the number of target genes in a drug should be minimum to avoid drug side-effect or toxicity. But still, the set of target genes should effectively block all possible paths of disease progression.In this article, we propose a network based computational analysis for target gene identification for multi-target drugs. The min-cut algorithm is employed to cut all the paths from onset genes to apoptotic genes on a disease pathway. If the pathway network is completely disconnected, development of disease will not further go on. The genes corresponding to the end points of the cutting edges are identified as candidate target genes for a multi-target drug.The proposed method was applied to 10 disease pathways. In total, thirty candidate genes were suggested. The result was validated with gene set enrichment analysis software, PubMed literature review and de facto drug targets.Studies on biomarker discovery have been moving the focus from single genes to multiple genes that interact in a cell [1\u20134]. The recent drug development researches are underway in this trend, because the single target approach may remain a certain possibility of disease progression since it may be developed along the other paths. On the other hands, the multiple target approach is expected to be more effective by simultaneously blocking multiple paths of disease progression. However, it is reckless to consider all possible combinations of genes since it may be not only computationally intractable but also impractical. The number of genes to be targeted should be limited since it will increase the possibility of unwanted side-effect or toxicity which may be caused by a member drug belonging to the multi-targeted drug\u00a0[5]. In a word, a multi-target drug with the minimum number of target genes will be most desirable. In this regard, the gene set should play a role of blocking disease progression from onset genes to apoptotic genes. To this end, the min-cut network algorithm can be applied to a disease pathway network and it will provide a minimum target gene set. There exist many well-established implementations for the min-cut algorithm\u00a0[6].\u00a0Barab\u00e1si emphasized the importance of network-based approaches to human diseases in identifying new genes for complex diseases [7]. A network based computational analysis\u00a0also can be used to enhance the efficiency of the drug development process. Wu et al. proposed a computational approach that finds drug targets by clustering networks through heterogeneous biomedical data that include genes, biological processes, pathways, and phenotypes [8]. Considering that the conventional means demand considerable cost and time, the approach of Wu et al. (i.e., target gene identification using available sets of biomedical data) would be an effective pre-run process ahead of proteomic analyses or in vivo tests. However, in the network of a gene set, known inflows and outflows influence the interactions between genes, and most pathway data include this kind of directional information\u00a0[9]. Because such biological processes cannot be retrogressive, in silico methods should reflect these directional relations. In particular, for target gene identification, directional or causal information can be more important because the states of molecules change to innate directions and not to their opposite states. However, in the aforementioned study, the directional relations were not implemented on the network. Nevertheless, many studies have recently used directed networks that incorporate biological pathways [6, 10\u201313]. Chen et al. suggested a sub-pathway-based approach for analyzing drug responses, which is more computationally effective than when examining the entire pathway [10]. However, this approach is also problematic in that other genes are ignored if excluded from the subset of a pathway. Given a directed network of genes, the well-established graph algorithms can be used. By representing genes as nodes and directions as edges, various biomedical issues can be intuitively explained. To gain insights about disease progression, graph-cut algorithms can be used to identify target genes. A graph cut refers to the process of dividing nodes in a network into two groups such that no path links one group with another. Interesting studies have been conducted that use graph-cut algorithms, including for the prediction of protein functions, to address the consistency problem in multiple sequence alignment, and for hippocampus segmentation in MR images [14\u201316].In this study, we propose an applied graph min-cut algorithm (Min-cut) for use with disease pathways in identifying drug-targeted genes. A cut is defined as a set of edges. The target genes we define here are those linked by these edges. A cut on the pathway network blocks the progression of a disease. Assuming that all edges have the same weight value, the minimum number of edges results in a minimum number of linked nodes. Min-cut is the minimum cut achieved with the smallest total weight of the edges. Our motivation for employing Min-cut to this study is as follows. Drug compounds can target one specific or sometimes several genes. Csermely indicated that multi-target drugs based on a network approach can help systematic drug design [17]. A graph-cut algorithm can search multiple target genes simultaneously and thus meet the requirements of drug design. However, approximately 22,000 known human genes exist, some of which may be a candidate target gene (CTG) [18, 19]. It is nearly impossible to consider all possible combinations of disease genes [20\u201322]. In terms of a graph cut on a pathway network, this means that every cut can provide a multiple number of CTGs. To circumvent this difficulty, we employ Min-cut to limit the number of CTGs. The proposed method is applied to 10 disease pathways including Alzheimer\u2019s disease and type 2 diabetes mellitus. To validate the results of our experiments, we employ gene set enrichment analysis (GSEA) software and review PubMed literature and de facto drug targets reported in the Kyoto Encyclopedia of Genes and Genomes (KEGG) database.Table 2 lists the source (disease onset) and sink (apoptotic) genes defined in each pathway. One or more genes per pathway were manually selected from descriptions or curated studies in KEGG. Every pair of genes (source, sink) was fed to Min-cut.\u00a0For example, the number of source and sink genes for AD was two and three, respectively, and experiments were run a total of six times. This approach was similarly applied to the remaining disease pathways. The combination of (source, sink) per pathway is summarized in the last column of Table 2. Source genes tend to be specified with each disease pathway, such as APP for AD and Htt for Huntington\u2019s disease. APP is an integral membrane protein that is expressed in many normal tissues, particularly in the synapses of neurons. Sometimes APP forms a protein basis on amyloid plaques, which are found in the brains of AD patients [26]. And the HTT gene provides instructions for making a protein called huntingtin which actives highly in the brain playing an important role in nerve cells (neurons) [27]. By contrast, sink genes such as CASP3 are commonly involved in several pathways, which thus classifies them as apoptotic genes. Apoptosis is a form of programmed cell death that occurs in multicellular organisms [28]. This means that CASP3 can be a sink gene of several diseases such as AD, HD, and NAFLD as shown in Table 2.Our study is based on the notion that target genes interrupt the progress of a disease. The resulting CTGs of our Min-cut are points at which the flow from disease onset genes to apoptotic genes can be cut. The visualized CTGs on the pathway network can help in understanding the mechanisms involved in disease progression and the roles that CTGs play therein. And the proposed method offers new insights into disease treatment and drug development. The CTGs can be recommended as preferential subjects to improve the treatment of diseases and drug design. Although CTGs have not been fully validated, we believe that they have the potential to be primary drug targets from of a considerable number of genes.In this study, we proposed the pathway Min-cut algorithm for target gene identification. It is assumed that if the network of a disease pathway is disconnected, development of the disease will not continue. To find points along the pathway that can be \u201ccut,\u201d while performing this task at a minimal cost, Min-cut algorithm was developed. We then applied it to a network augmented with additional information on gene-gene relations, including the causalities between them. Given source and sink genes, the proposed algorithm found an edge set that blocks every flow from a source to a sink gene. The candidate genes were validated through diverse means, namely, gene expression profiling by GSEA, the findings from various studies, and existing drugs. This work can be complemented if the biological domain produced a greater number of novel discoveries in areas such as gene-gene relations, disease pathways, gene expression and mutation, and so on.In our method, each disease pathway is represented as a network G\u2009=\u2009(V,\u2009E) that consists of genes as nodes V and relations between genes as edges E. In this initial network, a significant number of nodes are not connected. Therefore, the network is augmented with biological domain knowledge and is endowed with causality on its edges. There are some technical benefits to this network augmentation. First, the network becomes denser; if the network is sparse, applying Min-cut is difficult. Second, directionality reduces the solution space by eliminating unnecessary paths from the network. The directional information on protein interaction network data (directed PPI) is derived from the study of [12]. Genes that are not connected in the initial pathway are connected if their relations are indicated in the directed PPI, as shown in Fig. 7 (a). Therefore, edges in the initial network\u00a0E are augmented with edges in the directed PPI\\( \\overrightarrow{E} \\).In the augmented network, defining sets of source nodes S and sink nodes T, as shown in Fig. 7(b), is required. In the case of source nodes, some genes can be found in the KEGG description or the well-known study in PubMed. They tend to be located at the beginning of the pathway because the pathway describes sequential changes of state from normal to abnormal. Although the source genes have a normal status, they may cause the disease. For example, amyloid precursor protein (APP), which appears at the beginning of the Alzheimer\u2019s disease (AD) pathway, can be defined as a source node s\u2009\u2208\u2009S. However, sink genes are generally found at the end of the pathway in which apoptosis or a disorder status are described. In several pathways including the AD pathway, CASP3 can be defined as a sink node t\u2009\u2208\u2009T. This protein is a member of the cysteine-aspartic acid protease (caspase) family. Sequential activation of caspases plays a central role in the execution-phase of cell apoptosis.We interpreted the resulting CTGs by profiling gene expression. GSEA is a computational method that indicates whether predefined gene sets (pathway) reveal statistically significant, considering the two phenotypes [39, 40]. Much research has been conducted based on the assumption that differentially expressed genes (DEGs) may be potential biomarkers [41\u201343]. In case of the AD, a gene expression dataset (GSE1297) was obtained from GEO that contains 13,321 gene expression values for two classes, one for AD and the other for a control. GSEA provides a ranked list that is based on the gene differential expression between the classes for the entire range of genes. More importantly, an enrichment score (ES) is calculated by moving down the ranked list and increasing a running-sum statistic whenever a gene in a set is encountered, while decreasing it when genes are not in an a priori defined set of genes such as a pathway. This will then reflect the degree to which a set is overrepresented at the extremes (top or bottom) of the entire ranked list. For details on GSEA, see the study of [39].Alzheimer\u2019s DiseaseAmyotrophic lateral sclerosisAmyloid precursor proteinCandidate Target GeneCommon variable immunodeficiencyDegree CentralityDifferentially expressed genesdirected protein-protein interactionEnrichment scoreGene Expression OmnibusGene Set Enrichment AnalysisHuntington\u2019s diseaseKyoto Encyclopedia of Genes and GenomesMalignant melanomaNonalcoholic fatty liver diseaseProstate cancerPrion diseasesRenal cell carcinomaType 2 diabetes mellitusPublication of this article was funded the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (2017R1E1A1A03070345)The results of extracted examination criteria are accessible in http://www.alphaminers.net.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.We applied the proposed method to the simulation data. Fig. 1(a) is the simulated network which was generated with 45 nodes and 56 directed edges and Fig. 1(b) is to plot degree distribution showing that the network is a scale free network. In order to apply Min-cut algorithm, we set source nodes (indexed 1,2,8) which have no incoming edges and sink nodes (indexed 13,20) which have no outgoing edges. There are previous target identification approaches based on network analysis. The simplest and conceptual reference is to count the degree of edges to define the most important target genes on the undirected graph, so called Undirected Degree Centrality (U_DC). And Degree Centrality (DC) is defined as the number of outgoing links incident upon a node, while U_DC includes both incoming and outgoing edges. And the Hubs Centrality (HC) are basically singular vectors of the adjacency matrix of the graph [23, 24]. The ratio of cut-edges via total edges was used as a performance measure so that the method which minimizes the edge disruption (cut edges) will be assessed as a good target gene identification method. Fig. 1(c) shows that the ratio of cut-edges from Min-cut is 0.9 and the ration of connected edges of resulting top 6 nodes from three centralities is 0.32, 0.23, and 0.63 respectively. We got a result that the Min-cut based method can suggest the minimum cut-edges by considering the disruption impact on the connection (edge) of genes, rather than the genetic changes of each node (gene).\n\nFig. 1\nThe result of the proposed method on simulated scale free network. a directed scale free network. b the plot of degree distribution. c result of performance comparison between the proposed Mincut based algorithm with peer methods, U_DC, DC, and HC\nTable\u00a01 summarizes the real\u00a0data that were used to verify the proposed method: disease pathways, directed PPI, and protein-drug relations. Disease pathways were utilized to construct initial pathway networks and to set the role of genes, whether source or sink. We collected 10 disease pathways from the KEGG [25]. In order to extract unique results by setting disease specific onset or apoptotic genes, we selected one or more disease pathways involved in 6 different disease classes such as neurodegenerative diseases, metabolic diseases, and cancer, so on. (Details about disease specific genes are explained in the description of Table\u00a02.) The KEGG database provides a manually drawn pathway map. Details of the 10 disease pathways, pathway name/ID, corresponding disease name/ID, and class of the disease, are listed in bottom of Table 1. The total number of genes involved in are 1208. For network augmentation, we employed directed PPI, which was developed by [12] to investigate intracellular signal transduction. Their resulting network includes 2626 directional relations between 1126 proteins.Table 1\nData description\nTable 2\nSource and sink genes\nInitial pathway networks were built from each of the pathways. And the initial networks were augmented for becoming denser so that there will be not any technical problem when we apply Min-cut algorithm to the networks. First of all, we collected directional information on protein interaction network data (directed PPI) derived from the study of [12]. Then Genes that are not connected in the initial pathway are connected if their relations are indicated in the directed PPI. Therefore, edges in the initial network are augmented with edges in the directed PPI. Figure 2\u00a0shows the results of the pathway augmentation. The left side of the figure represents the toll of nodes and the other side represents the toll of edges. Bars indicating the initial network are shaded and outlined; those of the augmented network are represented with solid bars. In the case of AD, the number of connected nodes (genes) included in the network was 31 (18%) and the number of edges was 24, thus resulting in a sparse network. However, after network augmentation with directed PPI, the number of nodes and edges were increased to 210 and 467%, respectively. Across the 10 disease pathway networks, the average rate of increase in the number of nodes and edges was 207 and 454%, respectively. Not only the number of connected nodes and edges, but also additional information on the direction between nodes complemented the initial network.\n\nFig. 2\nNetwork augmentation results\nThe pie charts in Fig.\u00a03 show the results of CTGs identified by Min-cut. Note that the number of runs was different for each disease pathway. These different run proportions to the total number of runs should be considered. The number in parentheses indicates the number of occurrences of that gene during Min-cut runs for every combination of (source, sink) genes. The higher the occurrence rate, the more significant was the gene as a CTG. For example, in the case of AD, PSEN1 occurred twice as many times as CTG during six runs. The proportion of PSEN1 in AD was 33.3% (=2/6\u2009\u00d7\u2009100) The most frequently occurring CTGs in AD were PSEN1, PSEN2, and SNCA. In addition, their occurrence rates were all 33.3%. However, in the case of primary immunodeficiency (CVID) and renal cell carcinoma (RCC), we could not identify CTGs because there was no connection between source and sink genes.\n\nFig. 3\nCTGs. Source and sink genes appearing in Table 2 are excluded from these charts\nAs a representative example, Fig. 4 shows the resulting networks of Alzheimer CTGs with source and sink genes. Solid edges in Fig. 4(a) are from pathway data of 24 relations between 31 genes of 171 total disease-related genes (connected nodes\u2009=\u200918%). The dotted edges indicate directed PPI. In this example, 112 relations between 65 genes were augmented. Figure 4(b) shows the results of Min-cut applied to the augmented network. In the figure, APP and CASP3 were set as a pair of (source, sink). Min-cut successfully disconnected the network with minimal effort; five edges were cut. Along the respective edges, five CTGs were identified: PSEN1, CASP8, SNCA, PSEN2, and APAF1A. Those are marked with solid circles. And Fig.\u00a05(a) is Illustration for cut-edges and the CTGs in AD pathway from KEGG an illustration for cut-edges and CTGs in the AD process sourced from KEGG pathway. As it shows, SNCA plays an important role changing amyloid beta to the senile plaques which are extracellular deposits of amyloid beta in the grey matter of the brain.\u00a0Fig. 5(b) shows that the ratio of cut-edges from Min-cut is 0.15 and the ration of connected edges of resulting top 7 nodes from three centralities is 0.32, 0.39, and 0.69 respectively.\n\nFig. 4\nVisualization of resulting networks from Min-cut on the pathway of AD. a AD pathway network constructed with gene-gene interactions in the AD pathway (solid line) and directed PPI (dotted line). b Results of CTGs by Min-cut\n\n\nFig. 5\na Illustration for cut-edges and the CTGs in AD pathway from KEGG. b comparison of the resulting CTGs on AD with previous network-based essential gene identification methods, Degree Centrality and Hubs method for AD and ALS\nTo verify CTGs identified in our experiments, we conducted GSEA, and reviewed PubMed literature and de facto drug targets reported in the KEGG database. In this study, we provide validation results for AD. Figure\u00a05. shows a comparison of the two sets (AD and control). We found that most genes involved in the KEGG AD pathway were DEGs of the AD phenotype in GSEA. This indicates that our obtained initial network from the AD pathway is a reasonable and an efficient means to find markers. Each of the AD CTGs that we identified are shown in the different panels in Fig.\u00a06. where The upper panel (a) shows ES patterns for the control: a KEGG notch signaling pathway containing 34 genes. The overall profile of the result indicates that ES is positively correlated with a phenotype, the maximum deviation of ESs from zero reaches 0.567, and the nominal p value is 0.010. By contrast, the lower panel (b) shows ES patterns for AD: a KEGG Alzheimer\u2019s disease pathway containing 140 genes. The overall profile of the AD result indicates that ES is negatively correlated with a phenotype, the maximum deviation of ESs from zero reaches \u2212\u20090.576, and the nominal p value is 0.008. The two patterns are significantly different, and a sudden increase in ES in the lower panel provides evidence that the genes in the AD pathway are significant. In our AD CTGs, four genes are located at the rightmost side of the graph, and thus appear to be target genes: SNCA (ES: -0.415), CDK5 (ES: -0.402), CDK5R1 (ES: -0.320), and PSEN1 (ES: -0.094).\n\nFig. 6\nGene set enrichment analysis: a Control: KEGG notch signaling pathway. b AD: KEGG Alzheimer\u2019s disease pathway\nThe following are typical findings from previous studies on CTGs of AD. The CTGs of AD in Fig. 3 were derived from PubMed literature. More results are provided in Table\u00a03. SNCA: Non-A\u03b2 component of AD amyloid precursor SNCA gene may contribute to an increased risk of AD. SNCA gene polymorphism may be associated with an increased risk of AD [29, 30]. CDK5: Cyclin-dependent kinase 5 is reported to intimately associate with the process of the pathogenesis of AD. CDK5/CDK5R1 protein kinases involved in abnormal tau phosphorylation in AD. Tau proteins are widely known to be associated with dementias of the nervous system such as Alzheimer\u2019s disease and Parkinson\u2019s disease [31\u201333]. PSEN1: Mutations in the presenilin 1 gene are the most frequent cause of familial AD. There are reports about PSEN1 mutations in various species including Turkish, Chinese, and Korean [34\u201336].Table 3\nThe list of validation results on PubMed literatures\nAmong the CTGs we discovered and as shown in Fig. 3 are  de facto drug targets. The following target genes and drugs are also listed in Table\u00a04: PSEM1 for Alzheimer\u2019s disease, INSR for Type II diabetes mellitus, MAP2K for Melanoma, and AR for prostate cancer. These have been already developed as drugs to treat the diseases in practice. The rest of our CTGs also have potential to be biomarkers as drug targets.Table 4\nValidation of de facto drug targets\nFigure 7 illustrates the overall procedure of our study. First, a network is composed of a disease pathway. Each node indicates disease related genes and a directed edge between two different genes indicates that one gene may have biological changes in that direction. And then the initial network augmented with directed PPI information to endow causality on flows on the network, as shown in Fig. 7(a) [12]. Solid black edges are from pathway data and dotted blue edges are from directed PPI data. Second, as shown in Fig. 7(b), source and sink genes are chosen, where source genes may be considered responsible for the onset of a disease and sink genes may lead to apoptosis.\u00a0One or more genes per pathway were manually selected from descriptions or curated studies in KEGG. Every pair of genes (source, sink) was fed to Min-cut. Finally, as shown in Fig. 7(c), Min-cut finds the smallest sum of edges necessary to disconnect (i.e., \u201ccut\u201d the pathway of) a disease-onset gene and an apoptotic gene\u00a0[37, 38]. The resulting multi-genes, which are linked by the cut edges, are identified as CTGs. Details of the method are provided in the following subsections.\n\nFig. 7\nProposed Method: a Disease pathway network augmentation with directed PPI, b source and sink genes, (c) Min-cut for candidate target gene identification\nThe proposed method employs a Min-cut algorithm [37, 38] to find CTGs. We assume that cutting an edge with direction from one gene to the other means that blocking the change from one gene to the other. That is, we define genes connected to the cut edges as CTGs. The ultimate goal of the algorithm is to find a set of genes connected to the cut edges. The number of edges cut by Min-cut can be multiple if multiple paths exist from the source to the sink. The algorithm minimizes the number of cut edges, technically to improve efficiency of the marker development process and biologically to avoid unwanted side effects by selecting too many genes together. In this process, the genes connected to the cut edges become candidate target genes because the multiple cut edges completely disconnect the onset gene (source) from the apoptotic gene (sink). For example, when there are two nodes (genes) A, B and a connected directional edge (AB) \u20d7from A to B, cutting (AB) \u20d7 indicates biologically blocking gene A to be transformed to gene B. This is what we traditionally try to do in the targeted treatment and drug development. In short, cutting a certain edge refers to developing treatments by targeting two genes of the cut-edge. Once source\u00a0S and sink nodes T are determined in the network, Min-cut finds the edges minimizing\u00a0 the following functional:\\( \\operatorname{Minimize}\\ \\mathrm{c}\\left(S,T\\right)={\\sum}_{\\left(i,j\\right)\\in E}{w}_{ij}{e}_{ij}={\\sum}_{\\left(u,v\\right)\\in \\left(S,T\\right)\\cap E}{w}_{uv} \\)where c(S,\u2009T) denotes the s-t cut capacity, which is the sum of edge weights,\u00a0wijeij. The value of wij is large if the connection is strong, and vice versa. In addition, eij is 1 if nodes i and j are connected, and 0 otherwise. Note that edges in the pathway network are not weighted, and thus\u00a0eij is ignored. Regarding source and sink genes, the capacity becomes the sum of\u00a0wuv, where (u,\u2009v)\u2009\u2208\u2009(S,\u2009T). In our pathway application, the cut edges found by Min-cut may be regarded as the border of disease progression from normal to abnormal status. Then, it is assumed that the genes connected by the cut edge become a set of CTGs. Figure 6(c) illustrates the idea of cutting edges at the minimum capacity. Figure\u00a08 provides the pseudo-code giving further details.\n\nFig. 8\nPseudo-code for pathway Min-cut", "s12859-019-2667-y": "Blood pressure diseases have increasingly been identified as among the main factors threatening human health. How to accurately and conveniently measure blood pressure is the key to the implementation of effective prevention and control measures for blood pressure diseases. Traditional blood pressure measurement methods exhibit many inherent disadvantages, for example, the time needed for each measurement is difficult to determine, continuous measurement causes discomfort, and the measurement process is relatively cumbersome. Wearable devices that enable continuous measurement of blood pressure provide new opportunities and hopes. Although machine learning methods for blood pressure prediction have been studied, the accuracy of the results does not satisfy the needs of practical applications.This paper proposes an efficient blood pressure prediction method based on the support vector machine regression (SVR) algorithm to solve the key gap between the need for continuous measurement for prophylaxis and the lack of an effective method for continuous measurement. The results of the algorithm were compared with those obtained from two classical machine learning algorithms, i.e., linear regression (LinearR), back propagation neural network (BP), with respect to six evaluation indexes (accuracy, pass rate, mean absolute percentage error (MAPE), mean absolute error (MAE), R-squared coefficient of determination (R2) and Spearman\u2019s rank correlation coefficient). The experimental results showed that the SVR model can accurately and effectively predict blood pressure.The multi-feature joint training and predicting techniques in machine learning can potentially complement and greatly improve the accuracy of traditional blood pressure measurement, resulting in better disease classification and more accurate clinical judgements.Blood pressure is an important physiological parameter that reflects the state of the cardiovascular system and is playing an increasingly important role in clinical work. Regular monitoring of blood pressure is conducive to the early detection and diagnosis of various types of blood pressure disorders to ensure timely treatment and prevention. Therefore, the development of medical devices to rapidly and accurately measure blood pressure is of great significance.Two methods are used clinically to measure blood pressure, i.e., the direct approach and indirect approach. On the one hand, in the direct method, the blood pressure measuring system is directed at blood vessels, even those in the region of the heart. The direct method has the characteristics of low signal distortion, but the measurement is complex and has corresponding health safety risks. On the other hand, the indirect measurement has become increasingly accurate and is widely used in clinical practice. Auscultation and oscillometry are commonly used indirect and intermittent blood pressure measurement methods. Auscultation measures systolic blood pressure (Ps) and diastolic blood pressure (Pd) through the Korotkoff sound, of which the reading suffers from subjective effects, error, poor repeatability and susceptibility to noise interference. The oscillographic approach has better reliability and accuracy. The oscillographic method can also measure mean blood pressure [1]. Oscillographic approaches include the amplitude coefficient method [2, 3] and the waveform characteristic method [4, 5]. Improved oscillographic methods include the variable coefficient method [6], the difference ratio method, the inflection point method and the combination method [7]. Oscillographic approaches are based on changes in the characteristics of the waveform data in the cycles of a pulse wave. In the amplitude coefficient method and its improved version, the accuracy of the amplitude coefficient is difficult to guarantee due to the existence of individual differences. For the waveform feature method and its improvement version, the pulse intensity and pulse changes in a cycle vary among users due to differences between individuals, which may cause the characteristic data fail to satisfy the requirements of the blood pressure prediction method [8].In addition, the application of intelligent devices in the medical field has gained increasing attention [9], and the use of portable intelligent devices instead of medical equipment to detect human vital signs, such as heart rate (HR) [9\u201316], respiratory rate [11], blood oxygen saturation level (SpO2) [11], and pulse rate [13] has become more common. Considerable medical data with great research value have been generated in the process. Analysing the data through data mining, machine learning and other analytical techniques is an important current trend. Furthermore, this trend has inspired us to use machine learning algorithms to investigate the hidden knowledge in the substantial amount of medical data collected by intelligent devices. These data can assist the medical staff to perform disease diagnosis, especially blood pressure disease [14, 17] prediction and prevention. We can identify and analyse the complex mapping relationships between blood pressure and human physiological indexes by machine learning mechanisms to establish efficient blood pressure prediction models. The multi-feature joint training and predicting techniques in machine learning can potentially complement traditional blood pressure measurement and will greatly improve the accuracy, resulting in better diseases classification and more accurate clinical judgements.Many studies have applied machine learning algorithms to blood pressure prediction. Wu et al. [15] analysed the association between blood pressure and personal circumstances, including body mass index (BMI), age, exercise level, drinking and smoking, by means of artificial neural networks. Golino et al. [18] used categorical trees to predict blood pressure variance trends based on BMI, waist circumference (WC), hip circumference (HC), and waist\u2013hip ratio (WHR). Both studies focused on the correlations between human body health data and blood pressure to estimate the readings. Some other physiological parameters, such as electrocardiograph (ECG), photoplethysmography (PPG), and heart sound signals, have a more direct relation with blood pressure. In [19], two neural network algorithms were used to predict Ps by correlated factors (gender, serum cholesterol, fasting blood sugar and ECG signal). The paper [20] dealt with the accurate evaluation of the blood pressure by an Artificial Neural Network and the PPG signal. The authors in [21] used the ensemble neural network algorithm to model the relationship between PPG and blood pressure. Moseley et al. [22] used cardiovascular reactivity and recovery to predict long-term blood pressure and HR. Cardiovascular reactivity and recovery are laboratory stress-induced cardiovascular changes that can be used to predict trends in blood pressure and HR over the next three to ten years. Ghosh et al. [23] combined ECG with PPG characteristic data signals and calculated the pulse wave transmission time (PTT), which has been used in LinearR models to predict blood pressure. Peng et al. [24] used the heartbeat signal characteristics to establish a support vector machine (SVR) regression model for continuous and cuffless blood pressure measurement, but the predictive accuracy was not high. Kurylyak et al. [25] used a PPG signal to establish a neural network model to predict blood pressure continuously. However, the accuracy was not ideal when the model was evaluated with respect to the absolute error and relative error index. Hsieh et al. [26] used a dynamic PTT to establish a LinearR model for noninvasive blood pressure prediction using the MAE and R2 to evaluate model. However, the Ps was not ideal. He et al. [27] attempted to use a random forest model to predict blood pressure and evaluated the model in terms of accuracy and MAE. However, the MAE was large, and the accuracy of the systolic pressure prediction model was low.The optimal parameters C and g of the SVR models are found based on 10-fold cross-validation, based on which the SVR model is established.The performance of the SVR model is evaluated in terms of six evaluation indexes.Based on SVR prediction method proposed, it becomes promising to realize a highly efficient and real-time continuous blood pressure monitoring.The rest of this paper is organized as follows. The methods section describes the data collection and processing, some important features, the six evaluation indexes, and the principles of the SVR model and prediction process. The experimental data and results are given in results section. The discussion and conclusion sections, respectively, state the limitations of the study and the main conclusions.Refrain from any unhabituated strenuous exercise sessions at least 24 h prior to each testing day.Refrain from alcohol consumption for at least 24 h prior to testing.Maintain normal hydration by drinking to thirst.Consume a light carbohydrate meal no less than 2 h prior to testing.The final experimental data include 10 characteristics: 6-lead ECG (I, II, III, aVF, aVL, and aVR), PPG features from the Case GE system, PTT, HR and SpO2 obtained via feature extraction. The feature selection process was designed in consideration of two aspects, namely, Spearman\u2019s correlation [31] between features and a mutual information (MI) [32] coefficient between each feature and the target value. The range of Spearman\u2019s correlation coefficient is (-1, 1). The greater the absolute value is, the stronger the correlation is, and the sign of the value indicates the relevant direction. MI measures the dependency between the variables of the feature and target. MI is equal to zero if and only if two variables are independent, and higher values indicate stronger dependence. We conducted a reasonable feature selection process to obtain more effective blood pressure predictions; the process and basis are presented in the feature analysis and selection subsection of the results section. The following contents briefly introduce five important features: PTT, HR, PPG, aVF, and SpO2.Among the wide range of human physiological indicators that can be collected by intelligent devices, the main indicators impacting human blood pressure are PTT, HR, PPG, aVF and SpO2. PTT is calculated based mainly on the pulse wave mapped from internal human heart activity state to determine the blood pressure of the human body. HR is a vital sign that has been well described in the normal population and in various pathological states. HR and blood pressure are closely correlated, and hypertensive patients have higher resting HR than normotensive patients [33]. Blood pressure can be measured continuously based on ECG and SpO2 [34] or based on ECG and PPG signals [23]. PPG, aVF and SpO2 are common important physiological parameters that affect human blood pressure.The leftmost layer, known as the input layer, consists of a set of neurons {xi | x1, x2,...,x3} representing the input features (PTT, HR, PPG). Each neuron in the hidden layer (including 100 neurons) transforms the values from the previous layer with a weighted linear summation w1x1+w2vx2+w3x3, followed by a non-linear activation function the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values. The two neurons in the output layer represent the predicted value of Ps and Pd respectively.A support vector machine consists of a series of supervised machine learning algorithms that is used to solve classification, regression, and abnormality detection problems. A support vector machine contains a variety of models that can be classified as linear separable support vector machines, linear support vector machines and nonlinear support vector machines. The basic idea of linear separable and linear support vector machines is to construct the linear classifier that has the largest distance in the feature space and can address linearly separable data objects. For complex nonlinear classification problems, the nonlinear support vector machine is adopted. The nonlinear support vector machine transforms linearly non-separable problems in low-dimensional space into linearly separable problems in high-dimensional space via a kernel function. A subset of support vectors are used in the training set to represent the decision boundary. Due to the nature of nonlinear regression using complex characteristics of human physiological index data to predict blood pressure, we envisioned the feasibility of using the nonlinear support vector machine to predict human blood pressure.The RBF kernel is adopted in the SVR algorithm by mapping the original feature space X = (PPG, PTT, HR) onto the new feature space X\u2019 = (x1, x2, x3,... xn). The finite set of blood pressure indicator data can be expressed by a linear regression formula in the new feature space to establish a nonlinear mapping model between human physiological index data and blood pressure. In addition, the RBF kernel has the advantage of fewer parameters than the polynomial kernel, and the number of parameters directly affects the complexity of model selection. Therefore, an SVR model based on the RBF kernel function has low complexity. This paper also attempts to establish an SVR model based on a polynomial kernel function and a sigmoid kernel function, but the polynomial SVR model need a too long parameter optimization process to convergence and the sigmoid SVM model has fast parameter optimization but the predictions are poor.Two parameters, namely, C and gamma, are involved in the model training process when using the RBF kernel function. Parameter C is a common parameter for all kernel functions used in support vector machines and can be used to balance the classification error in the training set and the smoothness of the decision plane [36]. A smaller C makes the decision surface smoother, whereas a larger C allows the model more freedom to use more samples as support vectors so that all the training samples can be accurately classified or predicted. Gamma is used to adjust the influence of a training sample. The larger the gamma value is, the smaller the influence of the training sample. Thus, proper values of parameters C and gamma are critical to the performance of the SVR model.Establish a Pd and Ps SVR model, and initialize the model using the optimal parameters C and g, which were obtained by the optimization function based on 10-fold cross-validation using the training set.Feed the training set, including the feature vector of the human physiological index data and the blood pressure target vector, into the SVR model to train the model. The prediction model is obtained from the training process.Feed the characteristic set of the test set into the trained model to predict the corresponding blood pressures.Compare the predicted blood pressure values with the values measured by medical devices to calculate the deviations and the values of the six evaluation indexes.Figures\u00a05 and 6 show the results of the Ps and Pd prediction models with the optimal parameters, in which the parameters (C, gamma) are set to (100, 10) and (100, 10) and the RBF kernel function is used. The algorithm runs on a computer with a Windows 7 64-bit operating system, 4 GB RAM, and an Intel (R) Core (TM) i5-3210 CPU @ 2.50 GHz processor. The simulation environment is MATLAB.Table\u00a06 also shows the pass rate of the SVR model for three different relative error requirements (\u22640.03, \u22640.05, \u22640.07, \u22640.1). The results are illustrated in Fig.\u00a09, which shows that the prediction accuracy of the models of Ps and Pd increases as the relative error range increases. Similarly, in the minimum relative error range (\u22640.03), the accuracy of the SVR model is 96.00% and 96.57% for Pd and Ps, respectively, significantly higher than the accuracies of the other models.In addition, this paper evaluates each model in terms of MAPE, MAE, R2 and Spearman\u2019s correlation. As a general guideline, an MAPE less than 10% indicates high prediction accuracy. Table\u00a06 shows that the MAPE values of the Pd and Ps predicted by the SVR model are 0.4875% and 0.3173%, respectively, which are far better than 10%, indicating very good prediction performance. Furthermore, the MAE is 0.3374 and 0.4135 for Pd and Ps predictions obtained by SVR, far better than those obtained by LinearR and BP. Similarly, the R2 values are 0.9665 and 0.9835, and Spearman\u2019s coefficients are 0.9911 and 0.9917 (the corresponding P-value are approximately 0, i.e., less than the conventional significance level of 5% (P<0.05)) for Pd and Ps, respectively. These values are better than those of the LinearR and BP models. From an inspection of the above evaluation indexes, it can be concluded that the values predicted by the SVR model are close to the real values. The R2 shows that the SVR model describes a large degree of the variation in blood pressure. The SVR model demonstrates its advantages compared with the other models.The prediction results of the SVR, LinearR and BP models are evaluated in terms of accuracy, relative error, MAPE, MAE, R2 and Spearman\u2019s rank correlation coefficient. The SVR model produces better predictions than the LinearR and BP models. However, to maximize the accuracy, more data are required, which inevitably increases the training time of the model and the difficulty of optimizing parameters C and gamma. In addition, the data were collected from healthy people. The efficacy of the SVR model for predicting blood pressure in the elderly, predicting abnormal blood pressure or predicting the blood pressure for different ethnic groups has yet to be verified.There is a close relationshape between selected features (PTT, HR, PPG) in our paper and blood pressure. The blood pressure estimation approach of using PTT has been extensively studied over the past 15 years [38\u201343]. In recent years, blood pressure measurement with PPG has shown a lot of promise. Xing and Sun [44] provided a theoretical explanation of PPG waveforms predicting blood pressure. In addition, some studies are exploring the relationship between blood pressure and other physiological indicators, such as HR. Reule and Drawz [45] reviewed the relationship between HR and peripheral and central blood pressure. Therefore, it can be said that the use of these features to predict blood pressure is theoretically supported and feasible.With the popularity of smart devices, it has become easier to collect various human physiological data, which provides an opportunity for multivariate analysis. Compared with the univariate prediction of blood pressure, it starts from multiple influencing factors and considers more comprehensively, so as to achieve more accurate prediction of blood pressure. In addition, the conclusions obtained from the multivariate model prediction can in turn guide further medical research and provide possible research directions.The main contribution of this paper is the use of the SVR algorithm of machine learning to investigate the implicit association between human physiological index data and blood pressure measurements collected by medical devices to obtain an efficient and accurate prediction model for human blood pressure. The SVR model proposed in this paper has achieved 98.43% and 97.71% accuracy for Pd and Ps prediction, respectively, within the American ANSI/AAMI SP10-1992 standard specified error range (\u00b15 mmHg). In the relative error range of 5%, the prediction accuracies of Pd and Ps are 97.14% and 98.00%. The MAPEs for Pd and Ps are 0.4875% and 0.3173%, respectively, well below the generally accepted 10% standard. The MAEs for Pd and Ps are 0.3374 and 0.4135, respectively. The R2 for Pd and Ps are 0.9911 and 0.9917, respectively, which are both close to 1. We also compared the SVR model results with those obtained by a LinearR model and a neural network model: SVR achieved significantly better prediction performance.Future research will attempt to use more physiological indicators, such as respiratory rate, body temperature, age, weight, and sleep, to construct a prediction model to discover the relationship between blood pressure diseases and other diseases and to improve and enrich current health care provisions.Body mass indexBack propagation neural networka 6-lead ECGA patient monitorElectrocardiographA cuffless blood pressure measurement deviceHip circumferenceHeart rateLinear regressionMean absolute errorMean absolute percentage errorMutual informationDiastolic blood pressurePhotoplethysmographySystolic blood pressurePulse wave transmission timeStandard deviationBlood oxygen saturation levelcriterion systemR-squared coefficient of determinationRadial basis functionSupport vector machine regressionWaist circumferenceWaist\u2013hip ratioThis work is supported by the National Key R&D Program of China under Grant No. 2016YFB0800700, the National Natural Science Foundation of China under Grant Nos. 61802332, 61772449, 61772451, 61572420, 61807028 and 61472341, the Natural Science Foundation of Hebei Province China under Grant No. F2016203330, and Yorkshire Innovative Fund project the validity of EIMO for BP measurement (Providing the Experiment Data).The data that support the findings of this study are available from the website https://figshare.com/articles/BMC_BIO/5426542.The study was approved by the Department of Sport, Health and Exercise Science, the University of Hull Ethics Committee and all experimental procedures conformed to the Declaration of Helsinki. All participants provided written informed consent after having all experimental procedures explained to them both verbally and in writing.Not applicable.The authors declare no conflict of interest.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.In this paper, the SVR model using radial basis function (RBF) is applied to mine the relationship between human physiological data and blood pressure to establish a Ps and Pd model for effective and accurate blood pressure prediction. The main physiological indexes of the human body include PTT, HR, PPG. The blood pressure measurements include Ps and Pd, which are measured by existing electronic blood pressure gauges. The SVR model based on RBF can handle the case when the relation between target labels and attributes is nonlinear, which performs better than other models. In this paper, parameter optimization of the SVR training model was performed based on 10-fold cross-validation, and the optimal parameters C and g were found, respectively, in the Ps and Pd SVR prediction models. Then, the SVR prediction models were established based on the optimal parameters. The training set and test set were randomly divided with a ration of 4:1. The training set was used to train the SVR model, and the test set was used to assess the model predictions. Finally, the accuracy, pass rate, MAPE, MAE [28], R2 [29] and Spearman\u2019s correlation coefficient [30, 31] were used to evaluate the predictive ability of the model to verify the feasibility and efficiency of the SVR model in the prediction of blood pressure. Finally, the prediction results of two other classical machine learning algorithms, namely, LinearR, BP were compared. The results show that the prediction of blood pressure by the SVR model is the best. \n1)\nThe optimal parameters C and g of the SVR models are found based on 10-fold cross-validation, based on which the SVR model is established.\n\u00a02)\nThe performance of the SVR model is evaluated in terms of six evaluation indexes.\n\u00a03)\nBased on SVR prediction method proposed, it becomes promising to realize a highly efficient and real-time continuous blood pressure monitoring.\n\u00a0Eighteen participants (12 males, 6 females) displayed in Table\u00a01 were recruited for the study. Participants were included in the study if they were apparently healthy males or females between 18 and 50 years old, non-smokers, asymptomatic, with less than or equal to one coronary artery disease risk factor, no family history of myocardial infarction, obesity, hypertension, dyslipidemia, or heart disease, no history of cardiovascular, renal, hepatic, thyroid disease, and no history of physical inability to exercise. Participants were required to adhere to the following pre-measurement guidelines: \n1)\nRefrain from caffeine for at least 4 h.\nTable 1\nParticipant characteristics\n\nSD = Standard Deviation\n\n\n\u00a02)\nRefrain from any unhabituated strenuous exercise sessions at least 24 h prior to each testing day.\n\u00a03)\nRefrain from alcohol consumption for at least 24 h prior to testing.\n\u00a04)\nMaintain normal hydration by drinking to thirst.\n\u00a05)\nConsume a light carbohydrate meal no less than 2 h prior to testing.\n\u00a0Refrain from caffeine for at least 4 h.\nTable 1\nParticipant characteristics\n\nSD = Standard Deviation\n\nFollowing familiarization, participants completed three rest and exercise sessions incorporating concurrent Ps and Pd measurements from both a cuffless blood pressure measurement device (EIMO) and the criterion system (SunTech Tango automated monitor). Each session included a 30 min rest period during which six blood pressure measurements were obtained. Participants then completed three 15 min exercise periods at 25, 50, and 75 w for a total of 45 min. ECG and PPG were measured continuously by a patient monitor (CM400), a 6-lead ECG (Case GE ECG system) and the EIMO device during the rest and exercise phases. Twelve blood pressure measurements were taken by the SunTech Tango system at 2 min intervals during the exercise period, with a measurement taken after 2 min of recovery cycling. The setup of the data collection is shown in Fig.\u00a01.\n\n\nFig. 1\nThe setup of the data collection\n\nData analysis and processing was conducted in 3 stages to more easily rationalize the analysis steps. A workflow of the analysis process is shown in Fig.\u00a02. After the first two steps, the feature data set and the blood pressure data set were matched by their time stamps, and two features (PTT, HR and SpO2) were extracted in the third step. PTT was computed using windowed correlation between the ECG and PPG signals, and HR was extracted from the ECG signal and SpO2 was extracted from the PPG signals.\n\n\nFig. 2\nData analysis flow\n\nFor regression model of blood pressure prediction, this paper defines the accuracy evaluation index under different error ranges, as shown in formula (1), where x represents the number of correct predictions and y represents the total number of model predictions. If the difference between the predicted and actual values is within a certain range (the acceptable error according to the ANSI/AAMI SP10-1992 standard is \u00b15 mmHg), the prediction results are considered to be correct. \n (1)In general, the relative error, defined in formula (2), where pre_val and real_val represent the predicted and actual values respectively, can accurately reflect the credibility of a measurement. In this paper, the pass rate of the blood pressure prediction model is required to be in accordance with different relative errors (\u00b10.03, \u00b10.05, \u00b10.07, \u00b10.1). That is to say, we first calculate the relative error of the predicted blood pressure and then obtain the ratio of the number of predicted values that satisfy different relative error ranges to the total number of test samples. \n (2)The MAE is the average of the absolute values of the deviation between the predicted and actual values. Since the deviation is absolute, there is no positive and negative offset. MAE is not sensitive to the effect of anomalies, but it can reflect the actual situation of the prediction error. MAE is calculated by formula (3), where n is the number of samples, and yi and \\(\\hat {y_{i}}\\) represent the actual blood pressure and that predicted by the model for the i-th sample. \n (3)To show the range of model prediction error overall, the MAPE is defined as the mean of the absolute value of the relative error. In the formula (4), n represents the number of samples, and re_err represents the relative error, as shown in formula (2). \n (4)The R2 can be used to determine whether the features describe the target value. In this paper, the R2 refers to the comprehensive effect of all the characteristics on the blood pressure prediction, rather than the effect of each individual characteristic on blood pressure. R2 ranges from 0 to 1. The closer the value is 1, the stronger the degree of the interpretation. R2 is calculated as shown in formula (5), where n represents the number of samples, yi represents the actual blood pressure value of the i-th sample, \\(\\hat {y_{i}}\\) represents the predicted value, and \\(\\bar {y}\\) represents the average of all predicted values, as shown in formula (6). \n (5)\n (6)\nSpearman\u2019s rank correlation coefficient, which is a nonparametric statistical method, does not require the distributions of the original variables to be known. Spearman\u2019s rank correlation coefficient is a measure of the degree of correlation between hierarchical variables. It is also called the rank correlation coefficient and takes between -1 and 1. The greater the absolute value is, the stronger the correlation is, and the sign of the value indicates the relevant direction. Spearman\u2019s rank correlation coefficient is calculated by formula (7), where Ri represents the rank of yi, Qi represents the rank of \\(\\hat {y_{i}}\\), yi and \\(\\hat {y_{i}}\\), respectively, represent the actual blood pressure value and the value predicted by the model for the i-th sample. In this paper, the P-value is also calculated to test the null hypothesis of no correlation against the alternative hypothesis that there is a nonzero correlation. Small P-value, for example, less than 0.05, indicate that the correlation is significantly different from zero. \n (7)In LinearR target value is expected to be a linear combination of the input variables. In mathematical notion as shown in formula (8) where we designate the vector w = (w1,..., wp) as parameters of features and w0 as intercept, if \\(\\hat {y}\\) is the predicted value. \n (8)LinearR fits a linear model with coefficients w = (w1,..., wp) to minimize the residual sum of squares between the observed responses in the data set and the responses predicted by the linear approximation. Mathematically it solves a problem of the form as shown in formula (9): \n (9)Considering that coefficient estimates for Ordinary Least Squares rely on the independence of the model terms, we have mapped all features into a uniform distribution to normalize them. After training, the parameters of Ps and Pd models were obtained as following Table\u00a02. w1- w3 represents the parameters of three features (PTT, HR, PPG), w0 as intercept.\nTable 2\nThe parameters of LinearR\n\nBP is a supervised learning algorithm that learns a function f (\u00b7): Rm \u2192 Ro by training on a data set, where m is the number of dimensions for input and o is the number of dimensions for output. Given a set of features X = x1, x2,..., xm and a target y, it can learn a non-linear function approximator for either classification or regression. Between the input and the output layer, there can be one or more non-linear layers, called hidden layers. The structure of a three-layer neural network model designed in this paper is shown in the following Fig.\u00a03.\n\n\nFig. 3\nThe network structure of BP model\n\nKernel functions can simulate the projection of the initial data in a feature space with higher dimension where the data are considered as linearly separable, so that kernel functions can help to establish nonlinear support vector machines models. At present, there are three main classes of kernel functions [35]: polynomial kernel functions, RBF and sigmoid kernel function as shown in formula (10)\u2013(12). Different kernel functions will produce different algorithms, and the data will be mapped onto different feature spaces. \n (10)\n (11)\n\n (12)\nThe basic idea of SVR is to map the input space onto a high-dimensional feature space by nonlinear mapping and then linearly solve nonlinear problems [37]. In this paper, the vector in input space is x = (PPG, PTT, HR), assuming that the nonlinear model is: \n (13)Assuming that all training samples can be fitted by a linear function without error under, the relaxation factor is introduced to handle data that cannot be estimated by the function at a specified precision threshold. In (13), \u03c9 and b can be obtained by solving the following optimization problem: \n (14)subject to: \n (15)Using the dual principle, the Lagrangian multiplier and the kernel function, formula (14) is transformed into the following dual optimization problem: \n (16)subject to: \n (17)where \u03b1i, \\(\\alpha _{i}^{*}\\) is the Lagrangian multiplier, and k (xi, xj) is the kernel function. After solving (16), the regression function (support vector) of formula (13) becomes: \n (18)Figure\u00a04 shows the detailed prediction process. The process of blood pressure prediction includes 5 steps: \n(1)\nAfter extration and combination of features from the data collected, we have mapped all features to a uniform distribution with values between 0 and 1 to normalize them and divided the data into a training set and test set (4:1 ratio).\n\n\nFig. 4\nSchematic diagram of diastolic pressure prediction\n\n\n\u00a0(2)\nEstablish a Pd and Ps SVR model, and initialize the model using the optimal parameters C and g, which were obtained by the optimization function based on 10-fold cross-validation using the training set.\n\u00a0(3)\nFeed the training set, including the feature vector of the human physiological index data and the blood pressure target vector, into the SVR model to train the model. The prediction model is obtained from the training process.\n\u00a0(4)\nFeed the characteristic set of the test set into the trained model to predict the corresponding blood pressures.\n\u00a0(5)\nCompare the predicted blood pressure values with the values measured by medical devices to calculate the deviations and the values of the six evaluation indexes.\n\u00a0After extration and combination of features from the data collected, we have mapped all features to a uniform distribution with values between 0 and 1 to normalize them and divided the data into a training set and test set (4:1 ratio).\n\n\nFig. 4\nSchematic diagram of diastolic pressure prediction\n\nThe 15628501 human physiological index data contain the characteristic data of the human body in different states during rest and under different exercise loads. The characteristic items include PTT, HR, PPG, I, II, III, aVF, aVR, aVL, SpO2, and BP(including Pd and Ps). The data were sampled at 100 Hz (i.e., in 0.01 second intervals) and saved in a single file. The blood pressure data were collected by an electronic blood pressure instrument and saved in another file. The feature data set and the blood pressure data set were matched by time stamps. Given the short time interval between adjacent feature records, the blood pressure values do not undergo sudden changes; hence, a 2N window is used in the feature records to include N records prior to the exact matched value and N-1 records after the matched time while maintaining the same blood pressure values. The performance of the model is improved by a larger N, but the performance becomes relatively stable or even degrades and time consumption increases at increasingly larger values of N. Therefore, N=5 was used in our experiment to include the records 0.05 s before and after the specified time point. When N=5, the data set contains 3500 samples. The data sets corresponding to 3 different N values were randomly divided into training sets and test sets in a 4:1 ratio, and three sets of 10-fold cross-validation model prediction experiments were conducted. The three sets of model prediction results are shown in Figs.\u00a05 and 6.\n\n\nFig. 5\nComparison of the accuracy and pass rate of models with different N. In Fig.\u00a05, (a1) and (a2), respectively, represent the accuracy and pass rate for the models of Ps, while (b1) and (b2) represent those of the models of Pd. The blue, green, and orange lines represent the performance of the models with different 2N windows\n\n\nFig. 6\nComparison of four evaluation indexes for models with different N. In Fig.\u00a06, a, b, c, and d, respectively, represent the performance of models (including Ps and Pd) with respect to MAPE, MAE, R2 and Spearman\u2019s correlation for different 2N windows. The red and blue lines represent the Ps and Pd models, respectively\n\nTwo aspects, namely, the correlation between features and the dependence of each feature on blood pressure, are considered when performing feature selection. The correlation among the 10 features and that between each feature and the target value (Ps, Pd) are analysed based on Spearman\u2019s correlation coefficient and the MI coefficient (the results are shown in Tables\u00a03 and 4). All features are ranked from high to low according to their degree of dependence on blood pressure, which is indicated by MI (the results are shown in Table\u00a04). Tables\u00a03 and 4 show that there is a strong dependence between blood pressure (Ps and Pd) and PPG, PTT, and HR. In addition, there is no substantial difference between the dependence of the other features (I, II, III, aVR, aVL, aVF and SpO2) and blood pressure. However, there is a significant correlation among the six features I, II, III, aVR, aVL, and aVF. Considering these two aspects comprehensively, Table\u00a05 lists the combinations of features and the corresponding reasons. The performances of the models with 4 different feature combinations (when N=5) are shown in Figs.\u00a07 and 8.\n\n\nFig. 7\nThe accuracy and pass rate of the SVR models with 4 different feature combinations. In Fig.\u00a07, (a1) and (a2), respectively, represent the accuracy and pass rate for the models of Ps, whereas (b1) and (b2) represent those for the models Pd. The blue, green, orange and grey lines represent for the performance of the models with the feature combinations shown in Table\u00a05\n\n\nFig. 8\nMAPE, MAE, R2, Spearman\u2019s correlation of SVR models with 4 different feature combinations. In Fig.\u00a08, a, b, c, and d, respectively, represent the performance of models (including Ps and Pd) in terms of MAPE, MAE, R2 and Spearman\u2019s correlation for the feature combinations shown in Table\u00a05. The red and blue lines represent the Ps and Pd models, respectively\nTable 3\nCorrelation analysis of all the features\nTable 4\nRanking of the MI between the features and target\nTable 5\nThe four feature combinations\n\nIn this paper, the SVR model was evaluated in terms of six evaluation indexes, and the prediction performance was analysed and compared with that of other predictive models, namely, LinearR and BP. The experimental results show that the SVR model has significant advantages over the other models. The experimental results are shown in Table\u00a06, where Pd and Ps represent the Pd and Ps predictions.\nTable 6\nComparison of the blood pressure predictions of each model\n\n\u00b13, \u00b15, \u00b17, \u00b110 and \u22640.03, \u22640.05, \u22640.07, \u22640.1 indicate the difference in error range and relative error range between the actual and predicted values\n\nTable\u00a06 shows that the SVR model reaches a Ps prediction accuracy greater than 96% in all four error ranges (\u00b13 mmHg, \u00b15 mmHg, \u00b17 mmHg, \u00b110 mmHg). As the error range is relaxed, the accuracy of the predictions increases. The accuracy of the SVR model predictions for Pd and Ps are 97.14% and 96.43% in the range (\u22123 mmHg, +3 mmHg), which is much higher than those of LinearR and BP. The accuracies of the models with different error ranges are shown in Fig.\u00a09.\n\n\nFig. 9\nAccuracy and pass rate Comparison of 4 models (SVR, BP, LinearR). In Fig.\u00a09, (a1) represents the accuracy for the models of Ps, whereas (a2) represents that of the models of Pd. (b1) represents the pass rate for models of Ps, whereas (b2) represents that of the models of Pd. The blue, green, orange respectively, represent the SVR, LineaR, BP", "s12859-019-2669-9": "The investigation of intracellular metabolism is the mainstay in the biotechnology and physiology settings. Intracellular metabolic rates are commonly evaluated using labeling pattern of the identified metabolites obtained from stable isotope labeling experiments. The labeling pattern or mass distribution vector describes the fractional abundances of all isotopologs with different masses as a result of isotopic labeling, which are typically resolved using mass spectrometry. Because naturally occurring isotopes and isotopic impurity also contribute to measured signals, the measured patterns must be corrected to obtain the labeling patterns. Since contaminant isotopologs with the same nominal mass can be resolved using modern mass spectrometers with high mass resolution, the correction process should be resolution dependent.Here we present a software tool, ElemCor, to perform correction of such data in a resolution-dependent manner. The tool is based on mass difference theory (MDT) and information from unlabeled samples (ULS) to account for resolution effects. MDT is a mathematical theory and only requires chemical formulae to perform correction. ULS is semi-empirical and requires additional measurement of isotopologs from unlabeled samples. We validate both methods and show their improvement in accuracy and comprehensiveness over existing methods using simulated data and experimental data from Saccharomyces cerevisiae. The tool is available at https://github.com/4dsoftware/elemcor.We present a software tool based on two methods, MDT and ULS, to correct LC-MS data from isotopic labeling experiments for natural abundance and isotopic impurity. We recommend MDT for low-mass compounds for cost efficiency in experiments, and ULS for high-mass compounds with relatively large spectral inaccuracy that can be tracked by unlabeled standards.Stable isotope labeling experiments have been increasingly popular in quantitative, targeted metabolomics [1\u20134]. Metabolite isotopologs that are labeled differently can be distinguished by mass spectrometry. The resolved mass distribution vectors (MDV) for all possible mass isotopologs of individual metabolites are independent of metabolite levels and correspond to the degree of isotopic tracer labeling [4]. With tracer analysis and metabolic flux analysis, MDV provides quantitative information on pathway activity and pathway contribution variation [5]. Because naturally occurring isotopes [6] and tracer isotopic impurity from the nutrient [5, 7, 8] contribute to the measured signal, the fractional abundance of measured isotopologs (FAM) collected from the instrument must be corrected to obtain MDV.Existing correction methods are typically based on a correction matrix constructed by calculating theoretical contribution from isotopic natural abundance of each element and isotopic impurity of the tracer element using combinatorics [9]. Such calculations work well on low resolution instruments. However, modern mass spectrometers with high resolving power can easily resolve isotopologs with the same nominal mass, and, thus, including all isotopologs in the correction matrix is no longer justified. To address that limitation, fractional abundances of metabolites measured from an unlabeled sample can be used to construct the correction matrix [10]. The resolution effect can also be theoretically incorporated using mass difference theory based on nominal instrument resolution and exact mass differences between isotopologs from different chemical elements [8]. Nevertheless, the existing implementations of both methods have mathematical defects.Here we present ElemCor with correction and improvement of those two methods and a user-friendly graphical interface. We validate both methods using simulations and experiments, and we show their improvements upon other existing methods introduced above. We also discuss the strength of the two different methods and suggest applications to different types of studies.A detailed comparison between different MDT and ULS methods is shown in the inset of Fig. 1a. The modified ULS implemented in ElemCor yielded significantly more accurate results than the other three methods. The original ULS yielded accuracy similar to that of the two MDT methods, which are both theoretical and do not incorporate additional experiment measurements. Although the difference between the modified MDT in ElemCor and original MDT is mathematically significant, in practice, the modified MDT in ElemCor did not yield noticeably different results (<\u20090.1%) than the original MDT, as shown by the overlapped blue and red markers. That is because the difference between them is the balanced combination of all isotopes (Fig.\u00a05b), which has very small numerical contribution to the correction matrix during the calculation of multinomial probabilities. The contribution may increase as the molecular weight of a metabolite increases.We then simulated FAM for ten 34S enriched small metabolites including thiamine pyrophosphate, glutathione disulfide, S-adensyl-methione, cystathione, cystine, glutathione, cysteine, thiamine, taurine, and methionine (Fig. 1b). The original MDT did not include correction for 34S tracer and, thus, was not evaluated here. Similarly, for all ten metabolites considered and all degrees of theoretical enrichment, the modified MDT and ULS in ElemCor were remarkably more accurate than NRE and FAM. The original ULS, however, yielded lower accuracy than NRE, indicating that the resolution effect was not properly modeled. The reason of ULS being inaccurate in 34S simulation is that the most abundant isotopes of sulfur has much less fractional abundance than that of nitrogen (95.0% 32S vs. 99.6% 14N). Therefore, deconvolution of fractional natural abundance of sulfur from the column vector will make a significant difference in the diagonal of the correction matrix.Figure\u00a02d shows the errors of correction from different methods for all simulated data. The error in FAM before correction is up to 10%. The modified ULS in ElemCor was most accurate, while the modified MDT in ElemCor was in the second place with slight lower accuracy. Although the original MDT did not differ noticeably from the modified MDT, the tracer element is limited to 13C, 2H, and 15N. The original ULS generally undercorrected the data with an error up to 1.5%. Correction without considering the resolution effect mostly overcorrected the data with an error up to 10%. Taken together, these results demonstrate that the resolution effect can contribute significantly when correction for natural abundance is performed. The two methods used in ElemCor outperform the other methods in accuracy.ElemCor can be used to correct and analyze data in both tracer analysis and metabolic flux analysis. For tracer analysis where direct comparison of isotopic enrichments is needed, the GUI can accurately and rapidly perform such analysis without requiring a programming background. For metabolic flux analysis where MDVs will be used to calculate pathway fluxes, since popular flux analysis software suites such as FiatFlux [11] and INCA [12] are also MATLAB-based, the provided MATLAB function of ElemCor can easily be adapted to those large-scale workflows.MDT is a mathematical theory and requires only chemical formulae to perform correction. ULS, on the other hand, is a semi-empirical method that incorporates the measurement of unlabeled samples into the correction matrix. It does not require chemical formulae but requires additional experimental measurement of unlabeled samples. They have previously been used to perform corrections for isotopic labeling experiments with defective implementations [8, 10]. The software tool ElemCor provides correction for those mathematical defects and performance improvement for those two methods. Compared to the original ULS method in Ref. [10], the improved ULS method used in ElemCor is significantly more accurate and removes negative MDV thanks to non-negative regression. The improved MDT used in ElemCor is as accurate as the original MDT method in Ref. [8], but it extends the correction algorithms to support more tracer elements and mass analyzer types. In addition to those performance improvements, ElemCor also provides a user-friendly graphical interface that enables easy data import and direct visualization of the correction process and corrected MDV.MDT and ULS are both sufficiently accurate to perform correction for natural abundance. In our simulated data tests, ULS was slightly more accurate than MDT across all compounds. In our experimental data tests, MDT was marginally more accurate for only specific compounds. MDT is sufficiently accurate for low-mass metabolites, and the additional accuracy provided by ULS may be overshadowed by experiment error. Considering the cost of additional experiment measurement, we recommend using MDT for small metabolites. We generally recommend using ULS for large metabolites, since the accuracy of MDT is noticeably lower (Fig.\u00a02b and d). Moreover, the unlabeled samples can also track instrument bias and provide correction for instrument spectral discrepancy which becomes significant for large metabolites [8]. If heavier labeled fractions are under-measured due to instrument bias, ULS will use the distorted, unlabeled FAM as the input and therefore has a better chance of achieving a more accurate enrichment calculation.We present here a software tool, ElemCor, that corrects LC-MS data from isotopic labeling experiments for natural abundance and isotopic impurity. ElemCor uses two methods\u2014mass difference theory (MDT) and unlabeled samples (ULS) \u2014to account for the resolution effect. We demonstrate that ElemCor corrects the mathematical errors found in previously published methods, and includes more options for tracer elements and analyzers than previously published methods.We used simulated data with enrichment in different tracer atoms to evaluate MDT and ULS. For all compounds considered, correction without considering resolution effect (NRE, used in IsoCor) was significantly less accurate than other correction methods and not noticeably more accurate than directly using the uncorrected fractional abundances of measured isotopologs (FAM). Those findings confirm that the resolution effect needs to be considered during correction. The modified ULS method used in ElemCor is more accurate than the original ULS method and is, in fact, the most accurate of all methods tested. The modified MDT used in ElemCor was not noticeably more accurate than the original MDT. Nevertheless, the modified MDT improves upon the limitation of tracer elements and analyzer type by including two more tracer elements (oxygen and sulfur) and one more analyzer (FTICR).In summary, considering the significant cost of experiments, we recommend MDT for low-mass compounds, where the additional accuracy provided by ULS is barely noticeable and may be overshadowed by experiment errors. For high-mass compounds, we recommend ULS. Additional inclusion of an unlabeled standard can track instrument bias, which is important for large metabolites with relatively large spectral inaccuracy. Overall, ElemCor addresses the limitations of previous stable isotope correction methods and facilitates accurate correction of mass spectrometry-based stable isotope tracer data.where z\u00a0(z0,\u2009z1,\u2009z2,\u2009\u2009\u2026\u2009,\u2009zN)T includes the fractional abundances of measured ions (FAM) and x\u2009=\u2009x\u2032/|x\u2032|1 (x0,\u2009x1,\u2009x2,\u2009\u2009\u2026\u2009,\u2009xN)T is the MDV with the contribution of natural abundance and isotopic impurity removed from FAM [5]. Here T stands for transpose and |x|1 stands for sum or L1-norm of x. The sum of z is 1 by definition. Since C typically has a norm less than 1, the sum of x\u2032 is usually larger than 1. The constraints include: 1) the sum of x is 1; and 2) all components of x are non-negative. The total correction matrix C is the product of: i) the individual correction matrices for natural abundance of non-tracer elements; ii) the correction matrix for natural abundance of the tracer element; and iii) the correction matrix for isotopic impurity of the tracer element. Note that matrix multiplication is not commutative, and the order of multiplication should not be changed from the one given above.Here pi, j (i\u2009=\u20090, 1, 2,\u2009\u2026\u2009, Nt) are the isotopolog natural abundance of T where Nt\u00a0is the number of isotopologs for T excluding base mass. pi, j are the probabilities of finding +i mass due to natural abundance in the remaining j positions of tracer atoms [6].The isotopolog natural abundance of element T (or Q) can be calculated using combinatorics. When there are two stable isotopes for T (e.g., 12C/13C) with natural abundance 1\u2009\u2212\u2009\u03b1 and \u03b1 respectively, and the total number of T atoms in the molecule is equal to j, C2 can be expressed explicitly with \\( {p}_{i,j}={C}_j^i{\\left(1-\\alpha \\right)}^{j-i}{\\alpha}^i \\). When there are more than two stable isotopes (e.g., 16O/17O/18O), C2 has to be obtained numerically through multinomial distribution or iterative convolution [8, 9].Here ri, j (i\u2009=\u20090, 1, 2,\u2009\u2026\u2009, Nt) are the probabilities of finding the ith isotopolog when the nutrient has isotopic impurity given that the jth (j\u2009=\u2009i, i\u2009+\u20091,\u2009\u2026\u2009, Nt) isotopolog is found when the nutrient is pure. When there are two stable isotopes for T, \\( {r}_{i,j}={C}_j^i{\\beta}^{j-i}{\\left(1-\\beta \\right)}^i \\), where \u03b2 is the impurity of the tracer element. Similarly, when there are more than two stable isotopes, ri, j can be obtained numerically using multinomial distribution or iterative convolution. Note that isotopic purities are reported at the atomic level. For example, U-13C6 glucose with 99% isotopic purity has 94% glucose with all carbons labeled by 13C. The number of individual correction matrices to be included is dependent on the chemical formula of the compound of interest. For example, if a compound has one tracer element and two non-tracer elements, the total correction matrix is then \\( C={C}_1{C}_1^{\\prime }{C}_2{C}_3 \\), where C1 and \\( {C}_1^{\\prime } \\) correspond to the individual correction matrices for the two non-tracer elements, respectively.The aforementioned formulation fails to consider resolution of the isotopologs and, therefore, may be inaccurate for high-resolution instruments. To address that limitation, measured fractional abundances of the compound in an unlabeled sample can be used to approximate the effect of resolution on the correction matrix [10]. Theoretically, metabolites from an unlabeled sample have no isotopic enrichment in MDV, namely x\u2009=\u2009(1,\u20090,\u2009\u2009\u2026\u2009,\u20090)T. Therefore, according to Eq. (1), the FAM from an unlabeled sample corresponds to the first column of the correction matrix for natural abundance. However, that vector should not be used as-is to construct every column of the correction matrix, as done by others [10].Since unlabeled samples do not contain any information about the labeling agent, the FAM from an unlabeled sample only helps to construct the correction matrix for natural abundance. The correction matrix for isotopic impurity of the tracer needs to be constructed using Eq. (4). We implemented those corrections in ElemCor.Mass difference theory (MDT) can also be used to zero out certain components of the correction matrix based on the actual instrument mass resolution [8]. A non-tracer-labeled ion can be resolved from the tracer-labeled ion and excluded from the correction matrix if the mass (m/z) difference satisfies \\( \\Delta  M\\ge 1.66{M}^{1.5}/R\\sqrt{M_R} \\) or \u2206M\u2009\u2265\u20091.66M2/RMR for Orbitrap or FTICR analyzers. Here \u2206M is the mass difference between the two ions, M is accurate mass of the tracer-labeled isotopolog, R is nominal instrument resolution, and MR is the m/z where the nominal instrument resolution is defined and is classically 200 for Orbitrap and 400 for FTICR [8, 14]. This criterion is used to calculate the correction limit for each non-tracer heavy isotope. For example, the smallest resolvable mass difference, \u2206M, for glutamine (C5H10N2O3) isotopologs under a nominal resolution of 100,000 in Orbitraps is 2.07\u2009\u2219\u200910\u22123. The mass difference between a 17O1-glutamine ion and a labeled 13C1-glutamine ion is 8.62\u2009\u2219\u200910\u22124. Therefore, the correction limit of 17O is the nearest integer less than or equal to 2.07\u2009\u2219\u200910\u22123/8.62\u2009\u2219\u200910\u22124, and is equal to 2. That is, only two 17O atoms can be \u201cdisguised\u201d as 13C atoms and need to be considered in the correction matrix. Similarly, the correction limits of all the M\u2009+\u20091 heavy isotopes such as 15N, and 2H are both zero (mass differences of 6.32\u2009\u2219\u200910\u22123 and 2.92\u2009\u2219\u200910\u22123 respectively), yielding both of them resolvable under the resolution. For M\u2009+\u20092 heavy isotope 18O, the mass difference between an 18O1-glutamine ion and a labeled 13C2-glutamine ion is 2.46\u2009\u2219\u200910\u22123\u2009>\u20092.07\u2009\u2219\u200910\u22123, and thus it has a correction limit of zero as well. As a result, the original MDT only considers 13C and two 17O atoms in the correction matrix for glutamine at a resolution of 100,000.The determination of isotope exclusion in the correction matrix based on correction limits of individual isotopes is not self-consistent. For example, for glutamine at a resolution of 100,000, the correction limits for 17O and 18O are 2 and 0, respectively, indicating that 18O is resolvable and, therefore, excluded. However, due to the opposite signs of the two mass differences, a non-tracer labeled ion with two 17O atoms and one 18O atom has a mass difference of 7.39\u2009\u2219\u200910\u22124 (< the smallest resolvable mass difference 2.07\u2009\u2219\u200910\u22123) from the corresponding labeled ion and should actually be included in the correction matrix. The weakness of the correction limits used in the original MDT can be circumvented by directly calculating the sum of mass differences from all isotopes for determination. Figure 5(b) illustrates the combinations of different oxygen atoms of glutamine considered in the correction matrix for different methods.We developed a software tool, ElemCor, for correction of stable isotope tracer data using both ULS and MDT to construct the correction matrix and a non-negative constraint in the regression. ElemCor is a stand-alone application with a friendly graphical interface. Non-negative linear regression of Eq. (1) followed by normalization to the sum of 1 is used to obtain MDV in ElemCor, and isotopic enrichment is calculated as \\( {\\sum}_{i=1}^Ni\\cdot {x}_i \\) [9, 15]. The correction matrix in Eq. (1) was constructed using MDT and/or ULS as described above. ElemCor was developed under MATLAB (2016b) environment. An ElemCor function without graphical interface is also available so it can be easily adapted to most metabolic flux analysis software suites, which are also MATLAB-based [16, 17].We used both simulated and experimental data to validate ElemCor. Simulations at incremented nutrient enrichments were performed using the isotope simulation module in Xcalibur (Thermo Fisher Scientific). The chemical formula describing the isotopolog mixture for 20% 15N glutamine (C5H10N2O3\u2009\u00d7\u20090.64\u2009+\u2009C5H10[15]NNO3\u2009\u00d7\u20090.32\u2009+\u2009C5H10[15]N2O3\u2009\u00d7\u20090.04) and resolutions of 140,000 and 280,000 were used for the simulation. Experiments were performed on yeast (S. cerevisiae) grown in 1% glucose and yeast nitrogen base (0.5% NH4Cl) with 20.0% 15N in ammonium for labeled samples and 0% for unlabeled samples [8]. The isotopic purity of the nutrient is 99%. Each sample was harvested from 4\u2009mL of yeast cell culture when the OD600 reached 0.6. Cell extract was used in the LC-MS analysis (Orbitrap Q Exactive PLUS Mass Spectrometer, Thermo Fisher Scientific).Fractional abundance of measured isotopologsMass difference theoryMass distribution vectorUnlabeled samplesThis work was supported by Cancer Prevention and Research Institute of Texas grant RP130397 and NIH grants P30 CA072720, 5R01DK063349\u201312, P30 CA016672, and 1S10OD012304\u201301. The funding bodies did not play any roles in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.The software tool, example input and output data, and a detailed tutorial are available at https://github.com/4dsoftware/elemcor.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.We first simulated FAM for 24 15N enriched small metabolites including ADP, ATP, CTP, GDP, N-acetyl-glutamate, N-acetyl-glutamine, N-carbamoyl-L-aspartate, UDP, UDP-D-glucose, UDP-N-acetyl-glucosamine, UTP, arginine, asparagine, citrulline, glutamate, glutamine, glutathione, glutathione disulfide, lysine, ornithine, phenylalanine, serine, tryptophan, and uridine. Different correction methods were used to obtain MDV and calculate isotopic enrichment, which was compared to theoretical value. Root mean square errors (RMSEs) between the isotopic enrichments obtained for all 24 metabolites and their theoretical value (20%, See Methods and Materials) were calculated for all methods. For all 24 metabolites considered and all degrees of theoretical enrichment, MDT and ULS resulted in significantly lower RMSE from theoretical enrichment than FAM, correction without considering resolution effect (NRE, directly using the correction matrix defined in Theoretical Correction Matrix section), and even the mean standard deviation of experimental results (Fig.\u00a01a).\n\nFig. 1\na Root mean square error (RMSE) of 15N enrichments from simulated data of 24 metabolites after correction using different methods. The gray region corresponds to the mean standard deviation in 15N experiments shown later in Fig. 2. The inset shows the same figure in logarithmic scale. EC stands for ElemCor. b RMSE of 34S enrichments from simulated data of 10 metabolites after correction using different methods. Nominal instrument resolution for both (a) and (b) is 140,000\nWe also evaluated the accuracy of correction for larger metabolites, with a focus on coenzyme A (Fig.\u00a02a-c). The metabolites considered include coenzyme A (CoA), acetyl-CoA, succinyl CoA, and HMG-CoA. RMSEs between the isotopic enrichments obtained for those 4 metabolites and their theoretical value 20% were calculated for all methods. For all tracers considered, the modified MDT and ULS methods in ElemCor yielded accurate corrected results, as indicated by lower RMSE, whereas the accuracy of NRE was generally not acceptable and was even worse than FAM for 34S at 10% theoretical enrichment. The accuracy of the original ULS was excellent for 15N but was remarkably lower for 13C and 34S, which were accurately calculated by ElemCor. Similarly, the modification of MDT did not make a noticeable numerical change. Note that for large metabolites, higher instrument resolution is typically used. Therefore, we used 280,000 in our simulation.\n\nFig. 2\nRoot mean square error (RMSE) of (a) 13C, (b) 15N, and (c) 34S enrichments from simulated data of four CoA metabolites after correction using different methods. EC stands for ElemCor. Nominal instrument resolution for (a), (b), and (c) are 280,000. d The errors of correction for all simulated metabolites under all degrees of theoretical enrichments\nWe also performed a yeast experiment to validate ElemCor. 15N was chosen as the tracer element due to the independent incorporation of tracer atoms, allowing the binomial calculation of theoretical MDV. Not all metabolites studied in simulation have measurable enrichment from natural abundance in unlabeled samples, and therefore only ten metabolites were considered in the experiments. For all ten metabolites considered, MDT and ULS yielded more accurate results than FAM and NRE (Fig.\u00a03a). The slight advantage of ULS over MDT shown in simulation was not present in experiments because the standard deviation of experimental measurements was larger than the advantage itself. The enrichment of one metabolite, glutathione, was not properly corrected by ULS; Fig. 3b illustrates that ULS yielded a slight undercorrection for glutathione, which is likely due to inaccurate measurement of the unlabeled samples near the limit of detection.\n\nFig. 3\na Error in the measured 15N enrichment of ten metabolites after correction using different methods. The red crosses represent outliers outside interquartile range. b MDV of glutathione after correction using different methods. EC stands for ElemCor\nElemCor has a friendly user interface that guides users through six easy steps (Fig.\u00a04). In Steps 1 and 2, labeled and unlabeled data (*.xlsx) are loaded. Step 2 is optional, and when it is not performed, ElemCor runs based on MDT only. In Steps 3 to 5, isotopic purity of the tracer, nominal instrument resolution, tracer element, and the type of mass analyzer are specified. In addition to 13C, 2H, and 15N, ElemCor allows 18O and 34S as the tracer element for correction. Finally, in Step 6 the loaded data are analyzed, and isotopic enrichment is calculated for each compound. When a user selects a cell in the data table, MDV and FAM for the corresponding compound and sample are shown in the figure window. The results are automatically saved in separated sheets in the original file.\n\nFig. 4\nUser interface of ElemCor\nThe correction is essentially a linear regression defined by the total correction matrix C, (1)The correction matrix for a non-tracer element Q is expressed as (2)Here \\( {q}_{i,{N}_q} \\) (i\u2009=\u20090, 1, 2,\u2009\u2026\u2009, Nt) are the isotopolog natural abundance of Q where Nq is the number of mass isotopologs for Q excluding base mass and Nt is the number of mass isotopologs for a tracer element T excluding base mass. Note that qi, j\u2009<\u2009i\u2009=\u20090 if Nt\u2009>\u2009Nq. Only the isotopolog abundances at the lowest Nt\u2009+\u20091 masses are likely to be detectable and included in MDV, and thus matrix C1 is typically truncated with Nt\u2009+\u20091 rows remaining, yielding a square matrix [11\u201313]. The correction matrix for the tracer element T is expressed as (3)The correction matrix for isotopic impurity of the tracer is expressed as (4)In fact, the column vectors in the correction matrix are different since the number of tracer atoms considered for natural abundance is different for every column (Eq. 3) [6, 9]. It has been shown by researchers that convolution is an efficient way to construct the column vectors of the correction matrix [9]. This is because the Nth order multinomial coefficients are components of a vector obtained by N convolutions of the event probability vector. Since the number of tracer atoms considered for natural abundance is different by one for neighboring columns [6], besides the padded zeros, the difference between two neighboring columns in the correction matrix for natural abundance is simply a convolution of the fractional natural abundance of the tracer element. As a result, one can obtain the correction matrix for natural abundance by deconvolution of the fractional natural abundance of the tracer element Nt times from the FAM from an unlabeled sample. Figure\u00a05(a) shows the effect of deconvolution on the correction matrix for natural abundance for acetyl-CoA, which was used as an example in [10]. Deconvolution remarkably changes the components on the main diagonal as indicated by their colors.\n\nFig. 5\na Comparison of the correction matrices for natural abundance before and after deconvolution of acetyl-CoA using the ULS method. FAM from labeled samples is from Ref. [10]. The left panel shows the correction matrix used in Ref. [10], and the right panel shows the correction matrix used in ElemCor. Matrices are truncated to show the first six rows and columns for illustration purposes. b Combinations of oxygen atoms included in the correction matrix for different methods. The example is for glutamine at an instrument resolution of 100,000. Note that the total mass excess over base mass due to the heavy isotopes of oxygen should not exceed the number of tracer (carbon) atoms in the molecule", "s12859-019-2642-7": "Drug combinations have the potential to improve efficacy while limiting toxicity. To robustly identify synergistic combinations, high-throughput screens using full dose-response surface are desirable but require an impractical number of data points. Screening of a sparse number of doses per drug allows to screen large numbers of drug pairs, but complicates statistical assessment of synergy. Furthermore, since the number of pairwise combinations grows with the square of the number of drugs, exploration of large screens necessitates advanced visualization tools.We describe a statistical and visualization framework for the analysis of large-scale drug combination screens. We developed an approach suitable for datasets with large number of drugs pairs even if small number of data points are available per drug pair. We demonstrate our approach using a systematic screen of all possible pairs among 108 cancer drugs applied to melanoma cell lines. In this dataset only two dose-response data points per drug pair and two data points per single drug test were available. We used a Bliss-based linear model, effectively borrowing data from the drug pairs to obtain robust estimations of the singlet viabilities, consequently yielding better estimates of drug synergy. Our method improves data consistency across dosing thus likely reducing the number of false positives. The approach allows to compute p values accounting for standard errors of the modeled singlets and combination viabilities. We further develop a synergy specificity score that distinguishes specific synergies from those arising with promiscuous drugs. Finally, we developed a summarized interactive visualization in a web application, providing efficient access to any of the 439,000 data points in the combination matrix (http://www.cmtlab.org:3000/combo_app.html). The code of the analysis and the web application is available at https://github.com/arnaudmgh/synergy-screen.We show that statistical modeling of single drug response from drug combination data can help determine significance of synergy and antagonism in drug combination screens with few data point per drug pair. We provide a web application for the rapid exploration of large combinatorial drug screen. All codes are available to the community, as a resource for further analysis of published data and for analysis of other drug screens.Drug combination can improve treatment efficacy and overcome drug resistance, combinations with synergistic effects are generally seen as superior to those with additive effect because they are more likely to provide efficacy not otherwise achievable with possibly added benefits of lower toxicities [1]. Large scale experimental testing of combinations is challenging however and predicting which combinations are synergic and in which context is difficult. In addition, for anti-cancer drugs it is clear that single drug efficacy varies widely depending on the genome of the targeted tumor [2, 3]. This suggests that context specificity will be possibly even more difficult to predict for drug combinations. Systematic drug combinations testing performed across large collection of cancer cell lines presents the opportunity to discover new beneficial drug combinations as well as to build better algorithms for the prediction of synergy. To identify synergism, several methods use surface dose response where all possible dose pairs within the chosen concentration ranges are tested (full dose matrix) [4, 5] and therefore requires a large number of different doses per combination: For instance, 9 doses per drug amounts to a surface of 81 data points. Hence, such methods are difficult to implement for testing a large number of drug combinations across many models. Experimental designs leveraging sparse drug dosage pairs allow for a tractable number of tests but the limited number of drug doses and/or the absence of replicates pose a challenge for the statistical assessment of synergism and antagonism.We previously acquired a combinatorial drug data across 40 melanoma cell lines and 5778 combinations at 2 dose pairs. In this dataset, synergism was initially determined from a single combination data point (i.e. a single well) and two singlets (single drug wells). This large dataset did allow for the identification of novel combination with synergistic activity [6]. Nevertheless, experimental noise constitutes a challenge for broader data interpretation and further exploration of large-scale combination datasets. We thus aimed to develop a methodology to improve synergy calling and assign statistical value to synergies. In particular, we aimed to solve the issue of noise in the single agent data propagating through the synergy calculation: If the single agent effect of a given drug is incorrectly captured experimentally then all synergies calculated based on this estimate would be incorrect.Several models to assess synergy have been proposed. Models can be separated into two distinct classes: effect based models, which for two drugs at a fixed dose, model the combination effect from single agent effects, and dose-effect based models, which model the dose response of the single agents and the combination (a dose response surface in the case of the combination - details of each method are reviewed in [7]). Briefly, the dose effect methods are mainly four methods: (i) combination subthresholding compares the combination effect with untreated cells using statistical tests. (ii) The highest single agent null model stipulates that the combination effect will be equivalent to the highest single agent effect. (iii) The additive effects postulate that non synergic combination is the sum of effects of the single agent; effect is defined as one minus viability. Finally, (iv) Bliss models the effect of a drug as a multiplicative factor applied to the number of cells tested compared to the untreated cells (i.e. the viability measure). Bliss independence stipulates that the combination viability is the product of the two singlet viabilities, as if the two drugs were applied successively. Note that this model holds whether the drug viabilities are less than one (killing cells) or greater than one (growing cells), and does not require the viabilities to be modelled as probabilities.All these model have shortcomings, especially in the context of sparse dose response testing. Combination subthresholding requires replicates to perform the statistical tests. The Additive model does not have a clear physical model, and can lead to inconsistency, like predicting a negative number of cells when the sum of the two singlet effects is greater than 100%. The highest single agent model predicts the combination effect to be equal to the highest single agent effect. This prediction can be quite inaccurate in sparse dose testing, when there is only one dose per singlet and noise in the assay. The Bliss model can produce inflated viabilities when drugs are inactive, and therefore slightly greater than one by random chance. Here, we used a Bliss independence model based on logarithm transformation of the viabilities.The main dose-effect model, Loewe additivity, requires determination of the singlet doses that achieve the same effect as the combination. In the dataset analyzed here, in 58% of the drug pair-cell line assays, one of the singlet does not reach the effect of the low dose combination. This prevented us to use Loewe additivity in this work. This is mainly due to the fact that a large range of doses was not tested in this screen: Loewe additivity is not suitable for analysis of sparse doses screens.Methods to determine drug combinations synergism encompass several mathematical approaches that each have advantages and drawbacks and while in many cases they can yield concordant conclusions that is not always the case [8]. In addition, in some cases a given method can also yield discrepant conclusions regarding whether the tested drugs have a synergistic effect or not [7]. The most commonly used methods can be broadly sorted in two types depending on which reference model they use: Those based on the Bliss independence hypothesis and those based on the Loewe Additivity principle [9, 10]. Large screening campaigns come with limit to the number of drug doses that can be tested to allow for high-throughput and manageable costs. In these conditions, Loewe Additivity based methods are not applicable because full dose response curve of each single agents (and preferably more than one combination of doses) is required to obtain a synergy estimate [7]. On the other hand Bliss hypothesis based models can be applied on sparse data as long as the singlet doses are also used in combination. The advantages and drawbacks these models have been discussed in details previously [8].The single drug viabilities are used in many different combinations, and the error of measurement of single viabilities propagates to many Bliss scores because the error in measuring Vx propagates to the combination Bliss scores Sxj (for all 1\u2009<\u2009j\u2009<\u2009N); for instance, when a singlet viability is overestimated, this will lead to overestimated Bliss scores for all combinations involving that singlet (Fig. 2b, gray arrows show purple stripes of likely overestimated Bliss scores in our test dataset). Indeed, at the same positions in Fig. 2a, we can see stripes of low viabilities for these two drugs, but the viabilities of the singlets are near 1, as indicated by the color of the discs on the left side of the heat map. One possible interpretation is that these two drugs produced synergies with almost all the other 107 drugs we tested; the implicit assumption made by simply calculating a Bliss score from this data (Fig. 2b). A more parsimonious explanation for the high number of synergies for combinations involving these drugs is that the viability of the singlet, measured only once in a single well of the 1536 well plate, was overestimated due to experimental noise. Therefore, the stripe of synergies indicated by gray arrows in Fig. 2b are likely false positives with very large Bliss values, and risk occulting true positives.When single drugs have little or no effect, observed singlet viabilities are often greater than one (over 100% viability) due to random error on measurement, and their product can produce high Bliss scores even if the combination\u2019s viability is greater than one. For instance, if two drugs have each a singlet viability of 1.2, and the combination viability is 1, the combination will have a large Bliss score of 1.22\u2009\u2212\u20091\u2009=\u20090.34 despite the fact that no substantial growth inhibition was seen with any of the 3 treatments. This problem is particularly visible in the top right corner of Fig. 2b (green arrow): when the singlets have high viabilities, the Bliss scores seem to be systematically high even though the combination had little effect (Fig. 2a top right corner); even more problematic for the interpretation of the full dataset, it seems that such Bliss score are among the highest in the full dataset as can be seen in the heat map. A simple approach is to cap all viability data at 1 but this is an ad-hoc solution that is inferior to approaches that could estimate the experimental noise and suppress it in statistically rooted manner.if the experiment is noisy, any model will have a poor fit to the data: since the Bliss score is the deviation from the model of independence of drug effects, a noisy experiment will tend to yield over estimated Bliss scores overall: Since viability cannot be negative the data distribution is likely to be skewed towards positive synergy values (underestimating antagonistic interactions). Figure\u00a02h shows an experiment with the poorest fit at high dose; it is also the cell line that gave the highest number of combinations with Bliss scores greater than 0.3.To address these issues, we developed a drug combination data modeling approach that corrects for experimental noise of the single agent data, that is further overall robust to experimental noise and that provides robust estimates of synergy and associated p values. The model uses the full combination dataset to infer the activity of single drugs and identify synergies and associated p values. To illustrate our approach, we show the raw data for single agent activity and corrected (modeled) data in various graphical forms in Fig. 2 (a-g), focusing on the cell lines tested (COLO792) at the high concentration pair dosage.Before applying our model, to determine the validity of applying the Bliss independence model, we compared the viability of each drug combinations with the product of the viability of the two single drugs (Fig. 2e). The Bliss model postulates that these two variables are equal if there is no synergy or antagonism. Inspection of the viabilities by scatter plot for each cell line shows that the two variables are related, as the combinations lie along the x\u2009=\u2009y line. Furthermore, there is approximately the same number of points in the upper left semi-plane and in the lower right, suggesting that there is not a strong bias towards synergy (upper left) or antagonism (lower right). Therefore, solving the model should give a reasonable approximation of the singlet viabilities, and for determining synergism from the data.To determine the singlet values from the combination dataset we linearized the Bliss equation (eq. 1) and applied a Bliss-based linear model to estimate the singlets with combination data only. We postulated a linear model where the singlet viabilities at a given dose are the unknowns (108), and used the 5778 measured combination viabilities to solve the singlet viabilities with far more accuracy than if we used the measured singlet viabilities. In Fig. 2f, we plotted the observed viabilities for cell line COLO792, against the product of the estimated viabilities by the linear model, as opposed to the product of the observed singlet viabilities plotted in Fig. 2e. The model fits the data very well (R2\u2009=\u20090.90) and values are aligned along the x\u2009=\u2009y axis (black line). An interesting outcome of the singlet modeling is that the number of high predicted viabilities (viabilities greater than one) is greatly reduced (Fig. 2e, f, dots above the dashed horizontal line). Viabilities are not expected to be often greater than one, because cancer drugs are expected to kill or inhibit proliferation of cancer cell lines rather than improve proliferation over vehicle treated controls. This suggests that such high viabilities were largely due to noise in measurement of the singlets. As expected, high measured singlet viabilities (>\u20091) yielded overall high Bliss scores (Fig. 2b, high values in the top right corner). However, this trend was not seen when using the model-estimated singlets (Fig. 2c, d). Furthermore, singlet modeling suppressed outlier combinations where one singlet viability seemed to be overestimated and led to a stripe of overestimated Bliss score associated to one drug (gray arrows). These observations illustrated here on cell line COLO792 hold true for almost all the cell lines in the combination dataset analyzed (see below for statistics).In the high dose assay, all cell lines had an R2 of 0.66 or above except 501Mel (R2\u2009=\u20090.23, Fig. 3a and Fig. 2h). Interestingly, when not using singlet modeling, this is also the cell line with the most synergies (excess over Bliss score greater than 0.3): it shows 2577 synergies out of 5778 possible drug combinations in the high concentration assay (Fig. 3e), a proportion unlikely to be accurate. Indeed, we observed a high variance in observed synergy values, both in terms of R2 of our linear bliss models and in terms of viability in the DMSO (untreated) wells (Fig. 3f) suggesting high experimental noise for this particular set of plates: this high number of synergies was not observed at the lower dose assay. Therefore, it is overall unlikely that 501Mel is particularly prone to exhibit synergic drug interactions, and we concluded that this cell line data has a higher measurement error than the rest of the dataset. Because our method uses the variance in DMSO wells as the variance for the null hypothesis (no synergy or antagonism) it requires very strong deviation from the null hypothesis for significance in a case like 501Mel, reducing the impact of experimental noise onto the synergy calling. Indeed, our method detected a low number of synergies for 501 Mel, among the lowest compared to other cell lines (Fig. 3g). Therefore, by accounting for the variation in DMSO wells, our method automatically discounts many potential false positives, compared to the application of the Bliss formula directly to the raw data, and does not necessitates manual exclusion of this cell line from the data analysis.After we applied the singlet estimation for each assay (one cell line at one dose), we explored the consistency of the singlets across the cell lines between the high dose and the low dose pairs (Fig. 3c, d). Although the viability obviously depends on dosing, we expect some level of correlation between the two doses when considering all data available. We found that the correlations increased significantly when using the estimated singlets compared to the measured singlets (0.48 median correlation with estimated singlets, 0.14 with measured singlets, t test p value <\u20097*10\u2212\u200918). Only 28 of the 108 observed singlets were significantly correlated between the high and the low dose, versus 79 estimated singlets (testing that the Pearson\u2019s correlation is different from zero, with R function cor.test, p value cutoff of 0.05), rejecting the null hypothesis that the singlet viabilities are un-correlated between the two doses, for most of the 108 drugs. Further supporting the value of the approach, several drugs with a negative correlation between doses effect using the observed viabilities had a significant positive correlation of the estimated singlets (Fig. 3d). In addition to singlet viabilities, synergy values of drug combinations showed higher correlations across cell lines between the high and the low dose when using the Z value from the linear model than when using standard excess over bliss (p value <\u20092*10\u2212\u200928, t test).Figure\u00a04f shows that some drugs were engaged in many more synergies than others. Some of these broadly synergistic drugs that promote activity of many other drugs have been referred to as \u201cpromiscuous\u201d [11]. Most of the promiscuous drugs showed a significant agreement between the high dose and the low dose, with the exception of FTY720/fingolimod, which had the highest number of synergies at high drug dose but not at low dose. FTY720 also had the lowest median singlet viability of all drugs at high dose (0.08). It is possible that the variance of the log ratio \\( \\log \\left(\\frac{V_i\\ast {V}_j}{V_{ij}}\\right) \\) may be underestimated when Vij is very small, and therefore synergies involving FTY720 singlets should be considered with caution. We note however that FTY720 is also implicated in a large number of synergies when applying the excess over bliss formula to the raw data.The sensitizing potential of a drug has been historically called potentiation; it has been described as a distinct effect from synergy, not specific to the biology of the drug combined, for instance when drugs act on pumps that expels other drugs out of the cell, leading to broad potentiation of the effect of these drugs [10]. Here, we distinguished specific synergy from promiscuous synergy and broad potentiation through the use of the specificity score (see Methods).The specificity score compares the absolute synergy score of drug combination A-B with the mean of all the combinations including drug A and the mean of all the combinations including drug B. It allows to prioritize specific synergies over more general ones. Based on the specificity and the absolute synergies scores, we can establish a ranked list of drug combinations corresponding to strong drug interactions and highest confidence (Additional file 1: Table S1). Many of the top combinations have synergistic interactions supported by published studies and known biological functions. For example, AZD-7762 an inhibitor of the DNA damage response kinases CHK1/2 presents with high synergy scores with the Wee-1 inhibitor MK-1775 and high specificity score (Synergy score 20 and specificity score 5.2; top 0.1% with the top 1% score across 5778 combinations tested corresponding to a specificity score of 2.26). Wee-1 and CHK1/2 are known to regulate mitotic entry and progression and indeed combined inhibition was recently shown to be synergistic via forced mitotic entry [12]. We also identify the combination of AZD-7762 and gemcitabine (a DNA damaging agent) as the second top synergy among combinations with the CHK1/2 inhibitor as well as the combination with fludarabine another DNA damaging agent as the top 4th; These are expected outcomes for combinations of DNA damaging agents with an inhibitor of the DNA damage response. Addressing a different cancer pathway, the catalytic mTOR/PI3K inhibitor BEZ-235 was seen to display strong synergy with the non-catalytic mTOR inhibitor rapamycin \u2013 an outcome that is strongly mechanistically supported in a recent publication showing that rapalogues and catalytic mTORC inhibitors can suppress mTOR activity synergistically [13]. Among less expected outcomes we also flagged the combination between ABT\u2013263 and Wnt agonist as highly synergistic with 28 cell lines showing synergy and a specificity score of 3.82 (top 5% overall). Although we are not aware of any literature evidence for this exact drug combination, an interesting corroboration is found in a large pharmacogenomics study, revealing that activating mutations of beta-catenin predict for sensitivity to ABT-263 [14]. The Wnt agonist in our screen would indeed be expected to recapitulate the effect of the beta-catenin mutation. Another set of synergies of interest are seen with stibogluconate, a compound with an unclear mechanism of action but that was recently shown to inhibit the tyrosine phosphatases SHP1 and SHP2. Our analysis identifies the MEK1/2 (MAPK Kinase) inhibitor selumetinib and the SRC inhibitors SU6656 and dasatinib as top synergies for stibogluconate. SHP2 is indeed a known positive regulator of the MEK1/2-ERK1/2 pathway [15] at least in part through activation of SRC [16] and other evidence supports a SRC regulatory role of SHP2 [17].Our dataset of drug-drug synergies across melanoma cell lines can be represented by a cube of 108 drugs\u2009\u00d7\u2009108 drugs\u2009\u00d7\u200940 cell lines. In order to allow rapid, in-depth exploration of this large dataset by other investigators, we designed an interactive, user-responsive, web application based on the d3 library [18]. The web-based portal (http://www.cmtlab.org:3000/combo_app.html) displays an interactive visualization of the drug by drug data square, symmetric with respect to each drug listed in identical order along the x and y axis (Fig. 5a). The color coding of the square represents the count of significant synergies obtained with the corresponding x\u2009+\u2009y combination across the 40 cell lines, or the synergy specificity score, or both, based on user preferences. With a mouse over or a click, the name of the combined drugs, the number of synergic cell lines and the synergy specificity scores are displayed above the data matrix, along with the known drug target, which allows easy look up of the drug names and targets with a search engine. In addition, to explore the third dimension of the cube (synergies behavior for a given combination across cell lines), a dynamic query is sent to the server when a data point in the matrix is selected, and the combinatorial viability data across all cell lines is instantly displayed as a dot plot (Fig. 5b). The dot plot shows viability of the drug combination (red) and the expected viability under the assumption of Bliss independence (black), with error bars. Significant synergies and indicated with a black tick near the cell line name, and significant antagonisms with a pink tick.Combining drugs is largely considered a requisite for durable clinical responses in oncology, particularly in solid tumors. Rational combination building with small number of combinations testing remains very challenging and is unlikely to address the vast majority of clinical cases in the near future. Thus, systematic combination screening is needed in order to discover combinatorial therapeutic strategies. Furthermore, because of the known heterogeneity of tumors even within a given cancer type or genetically defined subtype (such as BRAF V600E melanoma) there is a need to perform these combination screens across large numbers of models. This would allow to understand how broadly effective across patients a candidate combination might be and ideally give insight into associated predictive biomarkers to be used for patient selection. Even for targeted therapeutics, toxicity is a major challenge and often limits efficacy because dose reduction becomes necessary during the course of treatment. Although, there are example of combinations being better tolerated than single agents (BRAF plus MEK inhibitors in melanoma) this is unlikely to be observed in the vast majority of cases. Hence, combination treatments are generally more toxic than single agent ones. Thus, it is largely considered that synergistic drug pairs (or higher order combinations) are preferable over those presenting with additive effect not only to yield efficacy not observed with single agents but also to minimize toxicity. Combinatorial drug screening is however technically and analytically challenging and very resource intensive. Thus, in order to screen large sets of tumor derived models, often compromises have to be made to allow for enough testing of each combination in a sufficient number of models. This translate generally into a reduction in the number of experimental tests aimed used to estimate each combination effect and synergy. Estimating synergy is complex in part because a given drug pair might not be synergistic in some dosing conditions but strongly so in others. For example, if the targets of the drugs are insufficiently engaged it is likely that this might not disturb their function sufficiently to observe effect or synergy. Unfortunately, for many drugs in development there is often insufficient information available to robustly determine which doses should be tested. Thus, sparse combination datasets represent an even greater challenge than more dense ones not only because of lack of technical redundancy but because of the potential to miss the conditions that would yield maximum synergy in the tests performed. Overall, these challenges call for a more robust estimation of synergy ideally allowing for a statistical estimate rather than relying on arbitrary choice of arbitrary threshold to designate outcome as synergistic. Here, we describe a method for the assessment of significance of drug combination synergies from viability cell line screens with sparse data that reduces noise in synergy calling and provides a statistical value for synergism for each test performed. To overcome the absence of replicates and low number of tested drug doses, we exploited information redundancy: Each drug was part of a large number of combination tests. To do so we used a Bliss based linear model of 5778 equations (the total number of pairwise combinations that were tested in the dataset) to yield better estimates of singlet drug viabilities and therefore yield better synergy estimates. To our knowledge, this is the first time the Bliss model is applied as a large linear model to solve for singlet viability with higher accuracy before computing excess-over Bliss scores. Because there is currently no other publicly available combination screening with sufficient overlap in drugs and cell lines with the one used here, we cannot rely on a gold standard set of results to measure the accuracy of our method, an issue that will be encountered by other researchers until a large compendium of combinatorial drug screening data in cancer cell lines is available. To quantitatively estimate the improvement that our approach, we used viabilities measured at different doses with the same combination in the same cell line as a proxy for replication. We showed that, after applying our regression method to estimate the singlets, the correlation between high dose and low dose singlets improved. In addition to the singlet viabilities, we also observed that our synergy Z values are moderately more consistent between high and low dose. We note however that we should not expect consistency of synergy to be high, since the difference between the two doses is quite large (5x), and synergy between two drugs might be observed only at one of the two doses. Estimation of error rate through large number of experimental replicates is prohibitive at scale but could nevertheless be built in the future to establish better gold standard for estimating error rate and advantages of new modeling methods, such as the one presented here.Our model was designed to address challenges of synergy estimates and noise in sparse combination datasets. However, our approach could be applied to denser datasets such as those allowing to build drug dose response surfaces. In this case our method could be applied by considering each dose test separately and solving for singlets in a large Bliss linear model. Another beneficial aspect of our method is that it gives a p value per unique test (here each well in an assay plate) in the screen. In these cases of denser datasets, our method could also be extended to yield a single p value for a full combination surface response.Our analysis of a large combination dataset across cancer cell lines shows that the resulting matrix of data can be interpreted using the Bliss model of synergy. Based on the Bliss hypothesis, using linearization of the matrix of combination viability outcome, we efficiently and robustly identify statistically significant synergistic events directly from the combination data without relying on single agent data. Importantly, we present evidence that our approach reduces the noise in the dataset and allows for identification of context specific synergies. Unlike the traditional use of synergy calculation using the Bliss hypothesis directly on the primary experimental data or other approaches based on different models of synergy, our approach associates a statistical value to each combination outcome, allowing for a less biased decision of hit calling threshold. Taken together, by comparing our results with previously described synergies, we show that by deriving a robust synergy score across the full dataset we identify well defined combinations as well as more novel ones with initial supporting mechanistic evidence in the literature. In addition to a statistical (p value) score for each combination tested in each cell line we present an approach to determine a specificity score for each drug pair allowing for prioritization of hits for follow-up analyses and experimentation.The main statistical procedure to determine synergism is performed on a single cell line and a single concentration pair. The 5778 combinations are spread on four 1536 well plates (Fig. 1a). The transformation of the Bliss independence eq. (1) with the logarithmic function yields a linear system that can be solved to obtain single drug viabilities (Fig. 1b). Conveniently, the residuals of the model represent the deviation from Bliss independence, our null hypothesis, and therefore indicate synergy and antagonism. We tested whether the residuals (each residual corresponds to one drug combination) are significantly different from zero. The mean of the null distribution is zero, and the variance, ideally, should be the variance that is due to the error on measurement and other variability not due to interaction between the two drugs tested in this combination. We approximated the variance using the sample variance of the multiple un-drugged wells present in each plate (as a measure of the experimental noise) plus the standard errors of the solved singlet viabilities. Using this null distribution, we could assign p values to the residuals (Fig. 1c) that is the probability of a viability to be lower (resp. higher) than observed under the null hypothesis; This p value was used to determine synergism (resp. antagonism). The variance of the null hypothesis allows to attribute statistical significance to synergism in each cell line independently, and therefore serves as a normalizing factor to compare synergism scores of a given combination across various cell lines. The p value was corrected for false discovery with the Benjamini Hochberg method [19] over the 5778 drug combinations. We consider a drug pair to yield synergy in a given cell line if one of the two drug doses passed significance (Fig. 1d).For each drug pair (i,\u2009j), we counted among the 40 cell lines how many showed synergy, and we called it the absolute synergy score Tij. (Fig. 5b, lower triangle; darker color shade corresponds to higher synergy score). Since some drugs are promiscuous and produce many more synergies that other drugs, we aimed at obtaining value compensating for this imbalance across the dataset. We computed \u201cspecificity score\u201d for synergy events (Fig. 5b, upper triangle). The specificity score allows to better identify (i) cases where synergy is observed solely because of a promiscuous drug, from (ii) cases where the synergy score for the combination was above and beyond an expected absolute synergy score. The score compares the synergy (i,\u2009j), to all (i,\u2009k)\u00a0and (k,\u2009j) synergies with k representing all other drugs tested in combination with i and j. Specificity score Scij compares Tij\u00a0to\u00a0all the synergies involving either drug i or drug j as follows:Where <Tik>k\u2009\u2208\u2009D\u00a0is the average absolute synergy score of the combinations of drug i with all other drugs, and SD(Tik)k\u2009\u2208\u2009D is the standard deviation. Therefore, the specificity score measures synergy effect that is above the synergy caused any potential promiscuous effect of drug i or drug j.We median polished [20] the logarithm of nuclei counts using the rows and columns of the 1536 well plate (one iteration), and re-exponentiated the results. Using the logarithm for median polish avoids the occurrence of negative counts and negative viabilities. For each plate, we computed a DMSO control value: the trimmed mean on the control wells nuclei counts (10% trimmed on each side). Finally, we computed viabilities by dividing the nuclei counts by the plate\u2019s DMSO control value. The median polish procedure increased the R2 of the independence model for almost all the cell lines tested, suggesting that it did remove noise in the assay (Fig. 3a-b).where Vi is the viability of the singlet i and Vij is the viability of the combination of the drugs i and j. Since replicates are not available in our dataset, it is difficult to assess statistical significance of synergy after computing a Bliss score. Furthermore, single measurements of single drugs (singlets) are noisy and measurement error in one singlet propagates to all Bliss values that involve that singlet across all 108 combinations involving drug A, leading to inflated Bliss scores. In order to overcome these difficulties, we built a linear model based on the assumption that deviations from Bliss independence are centered on zero (i.e. synergism is as (in)frequent as antagonism). We reasoned that if this assumption holds the model Vij~Vi. Vj should fit the data. Indeed, we observed that for the large majority of the cell lines the combination viability is similar to the product of the viability of the single agents, thus confirming the validity of the Bliss independence assumption for the drugs and doses we used (Fig. 2e).Since the Bliss model seems to fit the data, we used the log transform of the Bliss independence assumption [9, 10]. in order to create a linear model where a linear combination of the log singlet viabilities yield the drug combination viability:where Wij\u2009=\u2009\u2009\u2212\u2009log10Vij, Vij is the viability of the combination of the drugs i and j, Wi\u2009=\u2009\u2009\u2212\u2009log10Vi corresponds to the single drug i, and \u03f5 is a random noise term centered on zero.We solve the system of 5778 equations to obtain estimates for the 108 singlets at once. This avoids relying on measurement of one well for singlet viability determination: this system is largely over-defined as we use all the combinations to infer only 108 singlet viabilities. This allows an accurate estimates of singlets, and by extension, better estimates of synergy than relying on experimental singlet values (see results). In contrast, the Excess Over Bliss score (or Bliss score) is defined by: Sij\u2009=\u2009Vi.Vj\u2009\u2212\u2009Vij. The measurement error in one singlet Vi propagates to Bliss scores Sik for all k within the 108 drugs.The distribution of the number of synergies per combination found across cell lines was compared with randomizations of the synergy cube of two kinds. In the first method, we computed an indicator variable that indicates whether the synergy is significant, at a threshold of 0.05 FDR-corrected p value. We then randomly permuted the 5778 synergies within each cell line. We show that the observed distribution, compared to the randomization, 1) has more combinations that show synergy across a large number of cell lines and 2) has far more combinations that consistently show no synergy at all across the cell lines (Fig. 4a). Some drugs showed many synergies, but most did not (Fig.\u00a04). The goal of the second randomization was to ask whether the non-random structure of the observed synergy cube was only due to the presence of promiscuous drugs that sensitizing the cells to many other drugs, or whether it was evidence of a number of specific synergies. We simulated the drug-drug matrix containing the number of synergies from a binomial distribution such that the number of synergies per drug is conserved in the random matrix. The binomial parameter p for combination (i,j) is function of the sum of two drug specific parameters pi and pj; and N is 40 (for the 40 cell lines). 1000 random matrices were generated and the distributions compared to the observed distribution. None of the random matrices had large numbers of synergies as seen in the observed one (comparison of the number combinations with more than 12 synergies, p\u2009<\u20090.001; 141 drug pairs produced synergies in more than 12 cell lines compared to a mean of 45.30 in the randomizations \u2013 Fig. 4d-e).In order to allow rapid, in-depth exploration of this and other similar large datasets, we designed an interactive, user-responsive, web application based on the d3 library [18]. The server was written in Node JS. The data table is stored in memory as a javascript object, which natively implements a hash table, to permit a short response time when multiple users explore the web application.The full code to generate the results presented here, obtain statistics and run the web application are freely accessible on Github at https://github.com/arnaudmgh/synergy-screen.This work was funded by a grant from the Wellcome Trust (102696). (role of the funding body in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript: none).The raw data analyzed during the current study are available in the Github repository that holds the code: https://raw.githubusercontent.com/arnaudmgh/synergy-screen/master/data/rawscreen.csv. Results or the analysis can be re-generated by running the code. Git commit version: 19ebe5a.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.We reasoned that the combinations between a given drug A and the 107 other drugs in the screen contained recurrent and leverageable information about the singlet viability of drug A, and that this information could be used to overcome singlet data noise and overall experimental noise. Here, we use this concept of information redundancy built into the combination data to derive better estimates of the singlet viabilities. Based on this we further propose a method for the assessment of significance of synergy and antagonism as well as specificity of drug interaction. Our computational pipeline is graphically summarized in Fig.\u00a01 and described in the Methods Section. We found that this method identified correctly drug pairs previously described as synergistic or expected to be synergistic based on previously published mechanistic studies. The method also captures a number of less expected synergies with good initial support in the literature. Below, we describe first the data and the shortcomings of the existing methods, especially the simple application of the Bliss model. Then, we describe this novel method and its advantages compared to the singlet versus combination viability derived Bliss score. We provide a concise and easily readable R code allowing users to reproduce our results even on a personal computer. In addition, we developed a web application that allows any user to quickly explore the results through an interactive drug-drug heat map.\n\nFig. 1\nAnalysis Pipeline. This figure describes the analytical pipeline, from the raw data to the synergy scores. a All pairwise combinations between the selected drugs (108) where platted in a pseudo-random order on four 1536 plates, and viabilities were computed by dividing the number of cells in the well by the mean number of cells in the untreated wells. b Combination viabilities were modeled with the Bliss independence assumption, after passing to the logarithm, yielding a linear model of 5778 equations modeling the combination viabilities, and 108 unknown, representing the singlet viabilities. c Residuals of the linear system were used as a score for synergy. Variance in the DMSO wells was used to model sample error on the measurement of combination viability, yielding p values and q values for each combination. d For each cell line, if one of the two dose showed synergy, well considered the combination synergic in that cell line. e We counted the number of cell lines were the combination synergy was significant (absolute synergy score). f We computed the synergy specificity score from the absolute synergy scores (see methods). Synergy scores could be modeled using genomic features, as they are available for most cell lines used here on the GDSC project website (www.cancerrxgene.org)\nIn order to systematically explore a substantial number of drug combinations, we recently performed a large-scale drug screen across 40 melanoma cell lines using a limited number of drug doses. The drug combination response surface was limited to two concentration pairs: both drugs at a \u201cstandard\u201d dose estimated to inhibit fully the intended target(s) while avoiding overly broad off target, or both drugs at low dose (1/5 of the standard dose) in order to further emphasize on target combination and synergy detection based on partial target inhibition [6]. Using this design, all possible pairwise combinations between 108 drugs were tested systematically (108*107/2\u2009=\u20095778 pairwise combinations) on 40 cell lines, representing more than 439,000 data points. A heat map representing the 5778 combination viabilities for one assay (cell line COLO792 at high drug dose) is shown in Fig.\u00a02a, where the intersection of row i and column j shows the viability of the combination of drugs i and j, and singlet viability are represented as crossed rectangles on the sides of the heat map. Viability is used to measure drug effect, and is defined as the number of cells in the treated well divided by the average number of cells in untreated wells.\n\nFig. 2\nHeat maps of all pairwise drug combination results for cell line COLO792 low drug dose. a Viability (i.e. nuclei count divided by the average nuclei count in the DMSO treated wells) b-c Excess Over Bliss scores before (b) and after (c) linear modeling of the singlet viabilities (d) synergy Z values. In all heat maps rows and columns have the same order (sorted by singlet viabilities). Each heat map has a single row and single column of discs representing measured singlet viabilities (a-b) or estimated singlet viability by the Bliss linear model (c-d). Gray arrows indicate drugs where singlet viability was high compared with viabilities in combinations with other drugs, producing horizontal dark red rows in (a). This produces spuriously high Excess Over Bliss scores (b, gray arrows). Moreover, singlets with very high viability tend to produce a large number of high Excess Over Bliss scores even when the drug combination has no effect on the cells (a-b, top right corners). Such problems are not observed after the singlets are estimated from the linear model (c-d, top right corners and gray arrows) (e) Comparison between the solutions of the model (singlet viabilities) and the measured singlet viabilities (that were not used in the model). Error bars in the y axis indicate plus or minus 2 standard errors. Units of the model are shown: negative log10(1\u2009+\u2009viability). f Model based on Bliss independence has a R squared of 0.90, indicating that it is a good model for the combination of drug effects. g Scatter plot of the singlet viabilities, experimentally measured versus estimated. The vertical error bars indicate the 95% confidence interval. h Scatter plot of combinations viabilities (measured versus estimated) for cell line 501MEL at high dose, the assay with the lowest R\u00b2 in this dataset\nThe Bliss model of independence states that if two drugs have independent effects, then the viability of the combination should be equal to the product of the viability of the two single drugs: (1)Departure from this assumption will lead to a non-zero Bliss score defined as the difference between the left hand and the right-hand term of eq. (1): (2)Here, a positive Bliss score indicates that the observed combination viability Vij is lower than expected when the drug effects are independent (the product Vi\u2009\u2219\u2009Vj), therefore it indicates synergy. Conversely, a negative Bliss score indicates antagonism. While the Bliss score allows to easily rank synergies across tested combinations, the direct calculation of the Bliss score in an assay with no or few replicates poses several problems that we illustrate here using a large but sparse (dose-wise) combination dataset of drug pairs across a set of melanoma cell lines [6]:(i)\nThe single drug viabilities are used in many different combinations, and the error of measurement of single viabilities propagates to many Bliss scores because the error in measuring Vx propagates to the combination Bliss scores Sxj (for all 1\u2009<\u2009j\u2009<\u2009N); for instance, when a singlet viability is overestimated, this will lead to overestimated Bliss scores for all combinations involving that singlet (Fig. 2b, gray arrows show purple stripes of likely overestimated Bliss scores in our test dataset). Indeed, at the same positions in Fig. 2a, we can see stripes of low viabilities for these two drugs, but the viabilities of the singlets are near 1, as indicated by the color of the discs on the left side of the heat map. One possible interpretation is that these two drugs produced synergies with almost all the other 107 drugs we tested; the implicit assumption made by simply calculating a Bliss score from this data (Fig. 2b). A more parsimonious explanation for the high number of synergies for combinations involving these drugs is that the viability of the singlet, measured only once in a single well of the 1536 well plate, was overestimated due to experimental noise. Therefore, the stripe of synergies indicated by gray arrows in Fig. 2b are likely false positives with very large Bliss values, and risk occulting true positives.\n\u00a0(ii)\nWhen single drugs have little or no effect, observed singlet viabilities are often greater than one (over 100% viability) due to random error on measurement, and their product can produce high Bliss scores even if the combination\u2019s viability is greater than one. For instance, if two drugs have each a singlet viability of 1.2, and the combination viability is 1, the combination will have a large Bliss score of 1.22\u2009\u2212\u20091\u2009=\u20090.34 despite the fact that no substantial growth inhibition was seen with any of the 3 treatments. This problem is particularly visible in the top right corner of Fig. 2b (green arrow): when the singlets have high viabilities, the Bliss scores seem to be systematically high even though the combination had little effect (Fig. 2a top right corner); even more problematic for the interpretation of the full dataset, it seems that such Bliss score are among the highest in the full dataset as can be seen in the heat map. A simple approach is to cap all viability data at 1 but this is an ad-hoc solution that is inferior to approaches that could estimate the experimental noise and suppress it in statistically rooted manner.\n\u00a0(iii)\nif the experiment is noisy, any model will have a poor fit to the data: since the Bliss score is the deviation from the model of independence of drug effects, a noisy experiment will tend to yield over estimated Bliss scores overall: Since viability cannot be negative the data distribution is likely to be skewed towards positive synergy values (underestimating antagonistic interactions). Figure\u00a02h shows an experiment with the poorest fit at high dose; it is also the cell line that gave the highest number of combinations with Bliss scores greater than 0.3.\n\u00a0After applying the Bliss linear model to all the cell line in the dataset, we found that the model fit the combination data very well across cell lines (median R2 of 0.81, Fig.\u00a03a, b). In addition, we fitted models before and after median polish of the 1536 well plates: the median polish improved R2 values in most cases. Importantly, combinations were randomly assigned to the wells: neither the rows nor the columns of the 1536 well plates correspond to repeatedly the same drug. Therefore, there is no obvious reason for the median polish to increase R2 values, other than by removing experimental noise (Fig. 3a, b). Additionally, we compared the solved singlet viabilities to the measured singlet viabilities (throughout all singlet viability were left out of the linear modeling) we found excellent agreement between calculated and measured singlet values (median Pearson correlation of 0.82; Fig. 2g). We also computed the 95% confidence intervals around the solutions of the estimated singlets. Interestingly, for most cell lines, the confidence intervals are much smaller than the difference between the estimated singlet viability and the observed singlet viability from the screen (Fig. 2g, vertical bars), suggesting that we get more precise estimates of singlets viabilities from the linear system using the drug combinations than by using the observed viabilities in the single drug assay wells.\n\nFig. 3\nLinear modeling with bliss independence. a-b R2 values show the goodness of fit of the model for the high (a) and low (b) dose assays. Median R2 is 0.81, showing that the model fits well the majority of the data. Gray bars show R2 values when no median polish is performed in the pre-processing of the cell line plates, showing that median polish increases R2 values and probably reduce noise. c Pearson Correlation between low dose and high dose singlet viabilities across cell lines. Correlations are much higher when using the solved singlet viabilities than when using the viabilities measured on the plate. d Scatter plots of singlet viabilities between high and low dose, for measured singlets (left panels) and solved singlets (right panels). In these four examples, correlations went from negative to positive and significant. e-g R2 vs other measures on models at standard drug dose. e Negative correlation between the model R2\u2019s and the number of synergy found using an arbitrary cutoff (>\u20090.3) on Excess Over Bliss (showing high dose assays only). The cell line with the worst R2 also had the most synergic combinations (more than 2500 out of 5778), most of them are probably false positives. f The number of significant synergies does not correlate with the models R2. g The sample variance measured from the DMSO wells is a surrogate for the experimental noise. It correlates with low R2 for models with R2\u2009<\u20090.8; it suggests that very low R2 are mainly due to noise on measurement rather than an abundance of synergism or antagonism\nFor each drug, we then counted the number of synergies observed in each of its combinations (with 107 other drugs at either dose in 40 cell lines). We found a large range of synergy count among the 108 drugs, from 76 to 1065. Here also, we found good concordance between the assays at low drug dose and high drug dose: There was a positive correlation between the sensitizing potentials at both doses. For most of the drugs, high synergies occurrence at low dose is associated with high occurrence at high dose (Fig.\u00a04f). We then asked for each drug if the specific synergies matched between the high and the low drug dose. We found a significant overlap of synergies between the two doses for 77 drugs out of 108 (71%, fisher test p\u2009<\u20090.05, black dots in Fig. 4f). Note that such synergies found at two doses increase our confidence in the results, since synergy was observed in two independent assays; nevertheless, observing a synergy at only one dose does not dismiss the observation as spurious, since the synergy profile of each drug may differ between the high dose and the low dose for a variety of biochemical reasons (for example the low doses might be insufficiently inhibiting targets to yield effect or the high doses might have single agent effects too pronounced to allow for synergy observation in a cellular viability assay).\n\nFig. 4\nSynergy Square and histograms. a Black bars: Distribution of absolute synergy scores (i.e. the number of cell lines that showed synergy) for each tested drug pair. Gray bars: a randomization that conserved the number of synergies per cell line, but reassigned synergic drug pairs within a cell line. The observed distribution has much more 0 scores and high scores than the randomized one. b-e Randomization that conserves the sensitizing property of each drug. b A random absolute synergy score matrix that conserves the total synergy score per drug, in order to conserve each drug sensitizing properties in the randomization (generated with binomials distributions). c The observed absolute synergy score matrix. Drugs are ordered according to their sensitizing potential. d Comparison of the random and observed distributions (e) Number of synergy scores greater than 12, observed (vertical line) versus 1000 randomizations (histogram bars). f Sensitizing potential of each individual drug. For each drug, the number of synergies observed across all second drugs and cell lines, in the high dose assay versus the low dose assay. The number of synergies in common between the low dose and high dose assays was tested for each drug with the fisher test (black dots represent the 77 drugs with significant overlap)\nMost of the other drugs with a high number of synergies showed significant agreement between the high and low doses (Fig. 4f). Interestingly, ABT-263 (navitoclax) was seen as a top sensitizing drug. This is consistent with its targeting of anti-apoptotic proteins of the BCL2 family that is expected to lower the apoptotic threshold across cell lines and potentiate the pro-apoptotic effect of other drugs. We also found that the proteasome inhibitor bortezomib was a strong sensitizer, possibly due to the strong impact that proteasome inhibition has on many cellular processes. Several cytotoxic drugs (fludarabine, vincristine and docetaxel) are also among the top sensitizers. This might again be due to broad activity of these drugs across most cell lines inducing strong cellular stress and making cells more susceptible to many other additional stresses. Importantly, even with these broadly synergistic drugs, the pattern of synergy across partner drug (drug 2) was distinct indicating some level of specificity in sensitization. For example, there is little overlap among top synergy drug partners for the most sensitizing drugs (Fingolimod, Bortezomib, ABT-263, vincristine) (Fig.\u00a05a, Additional\u00a0file\u00a01: Table S1). In addition, we identify lapatinib as a sensitizer. In contrast to the other sensitizing drugs, this is a much more specific inhibitor that targets receptor tyrosine kinases, primarily of the EGFR family. However, we recently demonstrated that several synergistic events identified for combination including lapatinib are actually due to its capacity to inhibit multidrug resistance (drug pumps that expelled a range of compounds from the intracellular space). In fact, this activity can be a major confounding factor in analyzing synergies with this and likely other \u201cspecific\u201d drugs [6].\n\nFig. 5\nSnapshot of the Web Application and selected synergies. a Selected examples of synergies across some of the drugs: Absolute synergy (size of the dot) and specificity score (color code) are displayed. The top promiscuous drugs have a diverse profile of synergistic pairs and synergy pattern are different across different a selection of drugs synergies is. b The synergy score for all the drug pairs are displayed in a square heatmap. The lower triangle displays the absolute number of cell lines that displayed synergy for the drug pair, in a white to red color scale; the upper triangle displays the specificity score. When a drug pair is clicked, the corresponding viabilities for all cell lines is displayed in a dot plot. c Dot plot with the details on the synergy score. At the top the names of the drugs are shown, together with the absolute synergy score and the specificity score; then the detailed standard concentration results, and below the low concentration results, per cell line. The singlet viabilities for each drug estimated from the linear model are displayed in blue and green with standard error as a bar. The black dots show theoretical viability under assumption of independence of drug effect (no synergy). The red dots show the observed viability, with error bars as the standard deviation of the un-drugged wells. The error bars of the estimated singlet and the estimated combination under the independence assumption are the standard errors derived from the linear model (see methods). Hence, the distance between the black and red dots show the magnitude of the synergy (or antagonism). Significant synergies (p adjusted <\u20090.05) are shown with a black tick, and significant antagonism are shown with a pink tick\n\n (3)\nGiven or limited dose response coverage, we used Bliss independence to model synergy [9, 10] given by the Bliss score when Sij is null (no synergy):\n (4)\n\n\nAdditional file 1:\nTable of Synergy Scores. Table of absolute synergy scores and specificity scores for the 5778 combinations tested. Table S1. is generated in the source code script combos_script.R with the name combo_ranking_n.syn_score3.csv. (CSV 442\u2009kb)", "s12859-019-2594-y": "A large number of computational methods have been proposed for predicting protein functions. The underlying techniques adopted by most of these methods revolve around predicting the functions of an unannotated protein p from already annotated proteins that have similar characteristics as p. Recent Information Extraction methods take advantage of the huge growth of biomedical literature to predict protein functions. They extract biological molecule terms that directly describe protein functions from biomedical texts. However, they consider only explicitly mentioned terms that co-occur with proteins in texts. We observe that some important biological molecule terms pertaining functional categories may implicitly co-occur with proteins in texts. Therefore, the methods that rely solely on explicitly mentioned terms in texts may miss vital functional information implicitly mentioned in the texts.To overcome the limitations of methods that rely solely on explicitly mentioned terms in texts to predict protein functions, we propose in this paper an Information Extraction system called PL-PPF. The proposed system employs techniques for predicting the functions of proteins based on their co-occurrences with explicitly and implicitly mentioned biological molecule terms that pertain functional categories in biomedical literature. That is, PL-PPF employs a combination of statistical-based explicit term extraction techniques and logic-based implicit term extraction techniques. The statistical component of PL-PPF predicts some of the functions of a protein by extracting the explicitly mentioned functional terms that directly describe the functions of the protein from the biomedical texts associated with the protein. The logic-based component of PL-PPF predicts additional functions of the protein by inferring the functional terms that co-occur implicitly with the protein in the biomedical texts associated with it. First, the system employs its statistical-based component to extract the explicitly mentioned functional terms. Then, it employs its logic-based component to infer additional functions of the protein. Our hypothesis is that important biological molecule terms pertaining functional categories of proteins are likely to co-occur implicitly with the proteins in biomedical texts. We evaluated PL-PPF experimentally and compared it with five systems. Results revealed better prediction performance.The experimental results showed that PL-PPF outperformed the other five systems. This is an indication of the effectiveness and practical viability of PL-PPF\u2019s combination of explicit and implicit techniques. We also evaluated two versions of PL-PPF: one adopting the complete techniques (i.e., adopting both the implicit and explicit techniques) and the other adopting only the explicit terms co-occurrence extraction techniques (i.e., without the inference rules for predicate logic). The experimental results showed that the complete version outperformed significantly the other version. This is attributed to the effectiveness of the rules of predicate logic to infer functional terms that co-occur implicitly with proteins in biomedical texts. A demo application of PL-PPF can be accessed through the following link: http://ecesrvr.kustar.ac.ae:8080/plppf/Determining protein functions has been one of the central objectives for bioinformaticians, especially after the post-genomic era. This is because proteins have key roles in many biological processes. Identifying protein functions using experimental approaches is laborious and time consuming. Therefore, computational methods have been used extensively as alternatives. The underlying techniques adopted by most of these approaches revolve around computing protein functions from already annotated proteins. Most of them reference already annotated proteins using their structures [22], sequences [33], and/or interaction networks. The key limitation of these approaches is that they require highly reliable predictor algorithms. Recent computational methods exploit the huge growth of biomedical literature to predict protein functions from the information of already annotated proteins that appear within the literature. Some of them extract from the literature texts any information that describes proteins [12]. Others extract only information that describes the functions of proteins [2, 5, 7, 10, 28].We observe that some important biological molecule terms pertaining functional categories may implicitly co-occur with proteins in texts. Therefore, the methods that rely solely on explicitly mentioned terms in texts may miss vital functional information implicitly mentioned in the texts. Towards this, we propose in this paper an Information Extraction system called PL-PPF (Predicate Logic for Predicting Protein Functions) that employs techniques for predicting the functions of proteins based on their co-occurrences in texts with explicitly and implicitly mentioned biological molecule terms pertaining functional categories. PL-PPF infers the implicit terms using the rules of predicate logic. It does so by triggering protein specification rules recursively in the form of predicate logic\u2019s premises [14]. It extracts the explicit terms by employing Natural Language Processing (NLP) techniques that compute the semantic relationships among the biological terms in sentences.Using known protein and biological characteristics, PL-PPF composes rule-based protein specifications. These specifications are known protein characteristics in literature. PL-PPF composes these specifications in a pattern similar to predicate logic\u2019s premises [14]. It triggers them by applying the standard inference rules for predicate logic. It does so to deduce functional relationships between proteins. Ultimately, these deduced relationships enable PL-PPF to predict the functions of unannotated proteins. Let Pu be an unannotated protein. Let Lc be a list of known protein characteristics represented in the form of predicate logic\u2019s premises [14]. PL-PPF would first extract biological molecule terms related to Pu based on their co-occurrences in biomedical texts. It extracts the semantically related biological molecule terms to Pu in the sentences of the texts by employing linguistic computational techniques. It would then utilize these extracted terms as identifiers to serve as triggers for the appropriate premises from the list Lc using the standard rules of inferences [8, 16]. The conclusion of this process is a functional category term that co-occurs implicitly with Pu in the texts.Similar to our approach, a number of studies employed logic-based approaches as complementary to statistical approaches to perform some biological-related tasks. For example, [20] demonstrated that logic models can be used as complementary to statistical analysis models to identify fundamental properties of molecular networks and to perform biological inferences about the dynamics of intracellular molecular networks. As another example, [21] demonstrated that logic-based approaches are useful for improving static conceptual models in molecular biology. The paper demonstrated that adding logic-based approach can improve the Central Dogma information flow.Logic-based approaches have been successfully applied to solve complex problems in bioinformatics by viewing these problems as binary classification tasks. For example, [3] achieved acceptable results for predicting protein structures using constraint logic programming techniques. [4] presented a methodology that successfully predicted the tertiary structure of a protein using constraint logic programming. [17] used logic based multi-class classification method to accurately solve the problem of protein fold recognition. It accurately assigned protein domains to folds.Using known biological characteristics, PL-PPF composes rule-based protein specifications. It composes these specifications in a pattern similar to predicate logic\u2019s premises [14]. \u201cRepresenting protein specification rules in a pattern similar to predicate logic\u2019s premises\u201d section describes this process in detail.PL-PPF employs computational linguistic techniques to extract the biological molecule terms that are semantically related to an unannotated protein pu based on their explicit co-occurrences in texts. If an extracted term denotes a functional category f, PL-PPF will assign pu the function f. PL-PPF will also use the extracted term to serve as a given premise and apply it as a trigger identifier for the appropriate protein specification rules to identify additional functions of pu. \u201cExtracting biological molecule terms that cooccur explicitly with an unannotated protein in biomedical texts\u201d section describes this process in detail.PL-PPF will assign pu the functional terms that co-occur implicitly with pu in the texts by recursively triggering the appropriate premises constructed in step 1 and the given premises extracted in step 2 using the standard rules of inference for predicate logic. The conclusion will be a functional category that co-occurs implicitly with pu in the texts. \u201cInferring the functional terms that cooccur implicitly with an unannotated protein in texts using predicate logic\u201d section describes this process in detail.Premise R1 is constructed based on the following protein characteristics: (1) the folding of a protein takes place after a sequence of structural changes (the final stage of folding determines the structure of the protein) [5], and (2) the structure of a protein defines the function of the protein [5].Premises R2 and R3 are constructed based on the following protein characteristic: each protein\u2019s sequence is unique and defines the structure and function of the protein [1].Premise R4 is constructed based on the following protein characteristics: (1) the covalent bonds of a protein contribute to its structure [5], and (2) the raw sequence of a protein\u2019s amino acids determines its structure [1].Premise R5 is constructed based on the following protein characteristic: a protein\u2019s non-covalent interaction folding and dimensional structure can define the protein\u2019s biological function [5].Premises R6 is constructed based on the following protein characteristic: protein-protein interactions form complexes by interacting with one another [23].Premises R7 and R8 are constructed based on the following protein characteristics: (1) a complex assembly can result in a new function that neither protein can provide alone (the combined functionalities of the interacting proteins determine the new function) [23], and (2) the interacting proteins carry out their functions in the complex (the functions of the individual interacting proteins can be determined from the new complex assembly function) [23].Premise R9 is constructed based on the following protein characteristics: (1) proteins can be classified based on the similarities of their structural domains [1], (2) the structure of a protein reveals an insight into its function [5], and (3) the function of a protein p can be inferred from the functions of proteins that fall under the same structural classification as p [1].Premise R10 is constructed based on the following protein characteristics: (1) proteins can be classified based on the similarities of their amino acid sequences [5], and (2) the function of a protein p can be inferred from the structures of the proteins that fall under the same amino acid sequence classification as p [5].Premise R11 is constructed based on the following protein characteristic: the sequence of a protein\u2019s amino acids is inferred from the combination of the protein\u2019s covalent interactions with ligands and the protein\u2019s function [1].Premise R12 is constructed based on the following protein characteristic: non-covalent bonds between proteins during their transient interactions lead to Protein-Protein Interactions [18].Premise R13 is constructed based on the following protein characteristic: the structure of a protein can reveal an insight into its amino acid sequence [5].PL-PPF extracts the biological molecule terms that co-occur explicitly with an unannotated protein pu in the sentences of biomedical texts. If an extracted term denotes a functional category f, PL-PPF will assign pu the function f. PL-PPF will also use the extracted term to serve as a given premise and apply it as a trigger identifier for the appropriate protein specification rules to infer the functional category that co-occurs implicitly with pu in texts. The co-occurrence of a biological molecule term and pu in a sentence does not guarantee that this term and pu are associated. To be associated, the term and pu have to be semantically related in the sentence. We consider a term as semantically related to an unannotated protein, if their co-occurrence probability of being related is significantly larger than their co-occurrence probability of being unrelated in texts. PL-PPF computes the occurrence probabilities of terms using Z-score [32]. For two terms in texts associated with an unannotated protein to be semantically related, the co-occurrences of the same terms in the training dataset stored in PL-PPF\u2019s database should be considered semantically related.We use the term \u201ctraining dataset\u201d to differentiate between the following: (1) the set of biomedical texts stored in PL-PPF\u2019s database, and (2) the set of biomedical texts associated with an unannotated protein, whose functions need to be annotated. To differentiate between the two, we call the texts stored in PL-PPF\u2019s database a \u201ctraining dataset\u201d. In order for two molecule terms in texts associated with an unannotated protein to be semantically related, they have to be semantically related in the texts stored in the database (i.e., the training dataset).Based on linguistics, two nouns are considered related within a sentence, if they are connected by a pronoun (e.g., \u201cthat\u201d, \u201cwho\u201d, \u201cwhich\u201d) [19]. PL-PPF adopts a semantic rule based on the above observation for extracting semantically related biological molecule terms.Based on linguistics, two nouns are considered unrelated within a sentence, if they are connected by a preposition modifier (e.g., \u201cwhereas\u201d, \u201cbut\u201d, \u201cwhile\u201d) [13, 24]. PL-PPF adopts a semantic rule based on the above observation.We implemented PL-PPF in Java and used Prolog as the logic programming language. We ran it on Intel(R) Core(TM) i7 processor and a CPU that has frequency equals 2.70\u2009GHz. The machine has 16\u2009GB of RAM. We ran PL-PPF using Windows 10 Pro. We compared it experimentally with the following five systems: DeepGO [15], IFP_IFC [29], Text-KNN [31], Text-SVM [25], and GOstruct [9, 26]. DeepGO [15] uses deep learning to learn features from protein sequences for the purpose of predicting protein function. IFP_IFC is a system that we proposed previously for predicting the functions of unannotated proteins by employing random walks with restarts on a protein functional network. The nodes of the network denote the functional categories of proteins and the edges denote the interrelationships between them. Text-KNN and Text-SVM use characteristic terms, which are text features obtained from biomedical texts to represent proteins. The two systems assign an unannotated protein pu the functions of the set S of already annotated proteins, if pu and S have similar characteristic terms. The classifier employed by Text-KNN is based on k-nearest neighbour and the classifier employed by Text-SVM is based on support vector machine. In the framework of GOstruct, an unannotated protein pu is annotated with the functions of a Gene Ontology (GO) term, if this term co-occurs in close proximity with pu in biomedical texts.The complete list of specification rules used by PL-PPF in the experiments and the abbreviations of the terms included in the list can be accessed through the following two links, respectively:http://ecesrvr.kustar.ac.ae:8080/plppf/rules.pdf\nhttp://ecesrvr.kustar.ac.ae:8080/plppf/abbreviations.pdf\nWe also compared the systems using the 6086 SGD dataset [27]. The dataset is a complete information about the yeast proteins. The functions of these proteins have been experimentally determined by manual curation and verified using peer-reviewed process. We downloaded 46,227 PubMed texts associated with the SGD dataset based on their entries in [6].Cp: The number of correctly predicted functions for protein p.Np: The actual number of correct functions of protein p.Mp: The number of functions predicted for protein p by one of the systems.The protein centric maximum F-measure, which was used in evaluating the DeepGO method.The same GO dataset used in evaluating the DeepGO method (the dataset is shown in Additional file 1: Table S2 of [15]).As Figs. 1, 2, 3, 4, 5, 6 and 7 show, PL-PPF outperformed the other systems. This is an indication of the effectiveness and practical viability of PL-PPF\u2019s combination of explicit and implicit techniques (i.e., its techniques for inferring functional terms that co-occur implicitly with proteins using the rules of predicate logic as well as its techniques for extracting functional terms that co-occur explicitly with proteins). As the figures show also that the complete version of PL-PPF (i.e., which employs both of the explicit and implicit techniques) outperforms significantly the version of PL-PPF, which employs only the explicit techniques. This is attributed to the effectiveness of the rules of predicate logic in inferring the functional terms that co-occur implicitly with proteins in biomedical texts.As Fig. 7 shows, PL-PPF outperformed DeepGO in the GO Biological Process and Cellular Components subontologies. However, DeepGO outperformed PL-PPF in the Molecular Function subontology. Actually, we observed that PL-PPF performs better in the Biological Process dataset then the Molecular Function dataset in all conducted experiments including the ones described in \u201cAssessing the results returned by the systems through 5-fold cross validation\u201d and \u201cAssessing the results returned by the systems through cumulative-validation\u201d sections. We will investigate the root cause of this in a future work.As Figs. 5 and 6 show, the Recall and Precision values of the systems get better as the sizes of proteins, whose associate biomedical texts are used as a training dataset, increase. However, the Recall and Precision values of PL-PPF and IFP_IFC increase at higher rates. When the set of training texts is small, it would not have enough sentence structures. As a result, PL-PPF cannot accurately determine whether the sentences have solid relationships between their terms. Therefore, as the size of training biomedical texts gets larger, the z-score values computed by PL-PPF for determining semantically related terms become more accurate. This is advantageous for PL-PPF, since the size of biomedical texts associated with proteins in real-world increases significantly over time. As Figs.3 and 4 show, PL-PPF predicts the functions of GO annotation terms at lower hierarchical levels with better accuracy than higher-level ones.In general, we attribute the performance of PL-PPF over the other five systems to the fact that PL-PPF employs a combination of statistical and logic-based approaches while the other five systems employ only statistical-based approaches. That is, PL-PPF includes a combination of statistical-based explicit term extraction component and logic-based implicit term extraction component. Our hypothesis is that important biological molecule terms pertaining functional categories are likely to co-occur implicitly with proteins in biomedical texts.Some important biological molecule terms pertaining functional categories may implicitly co-occur with proteins in biomedical texts. Most current information extraction approaches do not take advantage of such implicitly inferred terms and focus solely on explicitly mentioned terms in texts. In this paper, we introduced an information extraction system called PL-PPF. The system predicts protein functions based on both explicitly and implicitly mentioned functional terms in biomedical texts. PL-PPF extracts explicitly mentioned functional terms in texts using computational linguistic techniques that identify semantically related terms in differently structured forms of sentences. It extracts implicitly mentioned functional terms by recursively triggering protein specification rules using the standard inference rules for predicate logic. We compared PL-PPF experimentally with the following five systems: DeepGO [15], IFP_IFC [29], Text-KNN [31], Text-SVM [25], and GOstruct [9, 26]. Results showed that PL-PPF outperformed the other systems in terms of inferring the functions of proteins from both the GO [11] and SGD [27] datasets. We also evaluated the impact of inference rules in inferring implicit functional terms by comparing two versions of PL-PPF: one adopts only the explicit techniques and the other is a complete version (i.e., adopts both of the explicit and implicit techniques). Results revealed that the complete version outperformed significantly the other version. This is attributed to the effectiveness of the rules of predicate logic in inferring implicitly mentioned functional terms in texts.Amino Acid Sequence of protein PxCovalent bond between Ligand y and protein PxFunction of protein PxFolding of protein PxLigand yNon-covalent bond between proteins Px and PyProtein Complex of Functions of proteins Px and PyProtein-Protein Interaction of proteins Px and PyStructure of protein PxThis research work was funded, in part, by ADEC Award for Research Excellence (A2RE 2016), Abu Dhabi, UAE; Grant No. 3108.The PL-PPF code is available at http://ecesrvr.kustar.ac.ae:8080/plppf/java.zipThe data analyzed during this study is included in its supplementary information file. The analyzed datasets were downloaded from the Gene Ontology and Saccharomyces Genome Database Websites, as follows:The GO ontology data:\nhttp://purl.obolibrary.org/obo/go/go-basic.obo\nThe GO annotations data: http://geneontology.org/page/download-go-annotationsThe yeast data:\nhttps://downloads.yeastgenome.org/curation/\nNot applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.PL-PPF infers the functions of an unannotated protein by going through the following sequential steps:1.\nUsing known biological characteristics, PL-PPF composes rule-based protein specifications. It composes these specifications in a pattern similar to predicate logic\u2019s premises [14]. \u201cRepresenting protein specification rules in a pattern similar to predicate logic\u2019s premises\u201d section describes this process in detail.\n\u00a02.\nPL-PPF employs computational linguistic techniques to extract the biological molecule terms that are semantically related to an unannotated protein pu based on their explicit co-occurrences in texts. If an extracted term denotes a functional category f, PL-PPF will assign pu the function f. PL-PPF will also use the extracted term to serve as a given premise and apply it as a trigger identifier for the appropriate protein specification rules to identify additional functions of pu. \u201cExtracting biological molecule terms that cooccur explicitly with an unannotated protein in biomedical texts\u201d section describes this process in detail.\n\u00a03.\nPL-PPF will assign pu the functional terms that co-occur implicitly with pu in the texts by recursively triggering the appropriate premises constructed in step 1 and the given premises extracted in step 2 using the standard rules of inference for predicate logic. The conclusion will be a functional category that co-occurs implicitly with pu in the texts. \u201cInferring the functional terms that cooccur implicitly with an unannotated protein in texts using predicate logic\u201d section describes this process in detail.\n\u00a0A predicate is a statement of one or more predicate variables. It can be transformed to a proposition by assigning values to the variables. These values determine whether the statements are true or false. The propositions are constructed by connecting the statements using logical connectives. PL-PPF composes protein specifications in a similar fashion. Using known protein and biological characteristics, PL-PPF composes the protein specifications from these known characteristics. It represents the specifications in a pattern similar to predicate logic\u2019s premises [14]. It uses these premises to find relations between an unannotated protein and protein functional categories. The specification rules can be updated periodically as new protein characteristics may be discovered. However, the update intervals should not be short, since new protein characteristics are discovered infrequently. We present in Table 1 a sample of protein specification rules in the form of predicate logic\u2019s premises. It includes only the rules used in the examples presented in the paper to illustrate the proposed concepts. We constructed the premises in Table 1 based on the following well-known protein characteristics:\nPremise R1 is constructed based on the following protein characteristics: (1) the folding of a protein takes place after a sequence of structural changes (the final stage of folding determines the structure of the protein) [5], and (2) the structure of a protein defines the function of the protein [5].\n\nPremises R2 and R3 are constructed based on the following protein characteristic: each protein\u2019s sequence is unique and defines the structure and function of the protein [1].\n\nPremise R4 is constructed based on the following protein characteristics: (1) the covalent bonds of a protein contribute to its structure [5], and (2) the raw sequence of a protein\u2019s amino acids determines its structure [1].\n\nPremise R5 is constructed based on the following protein characteristic: a protein\u2019s non-covalent interaction folding and dimensional structure can define the protein\u2019s biological function [5].\n\nPremises R6 is constructed based on the following protein characteristic: protein-protein interactions form complexes by interacting with one another [23].\n\nPremises R7 and R8 are constructed based on the following protein characteristics: (1) a complex assembly can result in a new function that neither protein can provide alone (the combined functionalities of the interacting proteins determine the new function) [23], and (2) the interacting proteins carry out their functions in the complex (the functions of the individual interacting proteins can be determined from the new complex assembly function) [23].\n\nPremise R9 is constructed based on the following protein characteristics: (1) proteins can be classified based on the similarities of their structural domains [1], (2) the structure of a protein reveals an insight into its function [5], and (3) the function of a protein p can be inferred from the functions of proteins that fall under the same structural classification as p [1].\n\nPremise R10 is constructed based on the following protein characteristics: (1) proteins can be classified based on the similarities of their amino acid sequences [5], and (2) the function of a protein p can be inferred from the structures of the proteins that fall under the same amino acid sequence classification as p [5].\n\nPremise R11 is constructed based on the following protein characteristic: the sequence of a protein\u2019s amino acids is inferred from the combination of the protein\u2019s covalent interactions with ligands and the protein\u2019s function [1].\n\nPremise R12 is constructed based on the following protein characteristic: non-covalent bonds between proteins during their transient interactions lead to Protein-Protein Interactions [18].\n\nPremise R13 is constructed based on the following protein characteristic: the structure of a protein can reveal an insight into its amino acid sequence [5].\nTable 1\nA sample of known protein characteristics represented in a form similar to predicate logic\u2019s premises and used as specification rules. The abbreviations in Table 3 are used in the formation of these premises. Ri denotes premise number i. The following Logic Symbols are used: \u201c\u2227\u201d for Conjunction; \u201c\u2228\u201d for Logical Disjunction; \u201c\u2192\u201d for implies\nWe present below two of the key computational linguistic techniques adopted by PL-PPF to extract the molecule terms that are semantically related to an unannotated protein based on their explicit co-occurrences in the sentences:\nBased on linguistics, two nouns are considered related within a sentence, if they are connected by a pronoun (e.g., \u201cthat\u201d, \u201cwho\u201d, \u201cwhich\u201d) [19]. PL-PPF adopts a semantic rule based on the above observation for extracting semantically related biological molecule terms.\n\nBased on linguistics, two nouns are considered unrelated within a sentence, if they are connected by a preposition modifier (e.g., \u201cwhereas\u201d, \u201cbut\u201d, \u201cwhile\u201d) [13, 24]. PL-PPF adopts a semantic rule based on the above observation.\nPL-PPF computes the functions of an unannotated protein p implicitly using the following: (1) the protein specification rules (i.e., premises) described in \u201cRepresenting protein specification rules in a pattern similar to predicate logic\u2019s premises\u201d section , (2) the biological molecule terms (i.e., given premises) that co-occur explicitly with p in biomedical literature and described in \u201cExtracting biological molecule terms that cooccur explicitly with an unannotated protein in biomedical texts\u201d section , and (3) the standard inference rules for predicate logic. PL-PPF can infer the functions of p by recursively triggering the protein specification rules using the premises (i.e., extracted terms) and the standard inference rules for predicate logic. At each recursion, an inference rule is triggered and applied to the premises that have been proven previously. This will lead to a newly proven premise. The final conclusion will be a protein function, which will be considered as the function of p. The conclusion is valid, if it has been deducted from all previous premises [30]. Table 2 presents the standard inference rules for predicate logic.Table 2\nThe standard inference rules for predicate logic\nWe now present case studies in Examples 1 to 4 to show the effectiveness of the deductive inferencing methodology presented in this section. The examples use various biological molecule terms as given premises for inferring the functions of unannotated proteins.Table 3\nNotations and abbreviations of the terms used in the formation of the premises presented in Table 1\nConsider that PL-PPF extracted the following terms based on their co-occurrences with an unannotated protein Pu in biomedical texts after applying the techniques presented in \u201cExtracting biological molecule terms that cooccur explicitly with an unannotated protein in biomedical texts\u201d section: FD(Px) and ST(Px) (recall Table 3). Using inference rules, we show how the co-occurrences of FD(Px) and ST(Px) in texts can be indicative of an implicit mentioning of the function of Px (i.e., F(Px)). Therefore, the co-occurrences of FD(Px), ST(Px), and Pu can be indicative of an implicit co-occurrences of F(Px) and Pu. Accordingly, the functions of Pu is likely to be similar to F(Px). Table 4 shows the inference rules, which conclude that the given premises FD(Px) and ST(Px) are indicative of F(Px).Table 4\nInferring the function of protein Pu described in example 1\nConsider that PL-PPF extracted the following terms based on their explicit co-occurrences with an unannotated protein Pu in biomedical texts: AAS(Px) and AAS(Py) (recall Table 3). Using inference rules, we show how the co-occurrences of AAS(Px) and AAS(Py) in texts can be indicative of implicit mentioning of the functions of Px and Py (i.e., F(Px) and F(Py)). Therefore, the co-occurrences of AAS(Px), AAS(Py), and Pu can be indicatives of implicit co-occurrences of F(Px), F(Py), and Pu. Accordingly, the functions of Pu is likely to be similar to F(Px) and F(Py). Table 5 shows the inference rules, which conclude that the given premises AAS(Px) and AAS(Py) are indicative of F(Px) and F(Py).Table 5\nInferring the function of protein Pu described in example 2\nConsider that PL-PPF extracted the following term based on its explicit co-occurrences with an unannotated protein Pu in biomedical texts: ST(Px) (recall Table 3). Using inference rules, we show how the co-occurrences of ST(Px) in texts can be indicative of implicit mentioning of the function of Px (i.e., F(Px)). Therefore, the co-occurrences of ST(Px) and Pu can be indicatives of implicit co-occurrences of F(Px) and Pu. Accordingly, the functions of Pu is likely to be similar to F(Px). Table 6 shows the inference rules, which conclude that the given premise ST(Px) is indicative of F(Px).Table 6\nInferring the function of protein Pu described in example 3\nConsider that PL-PPF extracted the following terms based on their explicit co-occurrences with an unannotated protein Pu in biomedical texts: NCBND(Px, Py) and F(Px) (recall Table 3). Using inference rules, we show how the co-occurrences of NCBND(Px, Py) and F(Px) in texts can be indicative of implicit mentioning of the function of Py (i.e., F(Py)). Therefore, the co-occurrences of NCBND(Px, Py), F(Px), and Pu can be indicative of implicit co-occurrences of F(Py), and Pu. Accordingly, the functions of Pu is likely to be similar to F(Py). Table 7 shows the inference rules, which conclude that the given premises NCBND(Px, Py) and F(Px) are indicative of F(Py).Table 7\nInferring the function of protein Pu described in example 4\nWe compared the systems using GO dataset [11], which contains GO terms as well as proteins annotated with their functions. We extracted a fragment from the biological process ontology that has 70 GO terms. We also extracted a fragment from the molecular function ontology that has 30 GO terms. We downloaded the GO dataset from [11]. The number of downloaded proteins (which are annotated with the functions of the selected terms) is shown in Table 8. We downloaded the PubMed texts associated with the selected proteins based on their entries in [6]. The number of downloaded texts was 577,486. PL-PPF will use these 577,486 texts as a training dataset for extracting the semantically related GO terms to the selected proteins. We considered a term t to be semantically related to an unannotated protein pu, if the co-occurrence probability of t and pu using Z-score [32] is greater than \u201c-1.96\u201d standard deviation (with 95% confidence level).Table 8\nNumber of GO terms and proteins downloaded for the experiments\n\na We selected for the evaluations only proteins that satisfy the following: (1) associated with at least one PubMed publication based on their entries in UniProtKB [6], and (2) have experimental evidence code: IC, IDA, IPI, IEP, EXP, TAS, IMP, IGI, or IC.\nWe divided each of the GO and SGD datasets to five sets. The systems were assessed five times. At each time, a different set of each of the GO and SGD datasets was used for testing and the remaining four sets were used to train the systems. We considered the testing proteins as unannotated and assessed the systems for predicting their functions accurately. We evaluated two versions of PL-PPF: one adopts all the techniques described in this paper and the other adopts only the explicit terms co-occurrence extraction techniques (i.e., without the inference rules described in \u201cInferring the functional terms that cooccur implicitly with an unannotated protein in texts using predicate logic\u201d section). This will enable us to determine the impact of the inference rules in inferring implicit terms co-occurrences. We assessed the prediction accuracy of each system for identifying the functions of each unannotated protein p using the following standard quality metrics shown in Eqs. 1, 2 and 3: (1) (2) (3)\nCp: The number of correctly predicted functions for protein p.\n\nNp: The actual number of correct functions of protein p.\n\nMp: The number of functions predicted for protein p by one of the systems.\nFigures 1 and 2 show the results achieved by each system using the GO dataset and SGD datasets respectively. Table 9 shows the number of valid and invalid co-occurrences identified by PL-PPF in the GO and SDG datasets.Table 9\nNumber and percentage of valid and invalid co-occurrences identified by PL-PPF in the GO and SDG datasets\n\n\nFig. 1\nThe systems\u2019 performances for predicting GO functions after applying 5-fold cross validation\n\n\nFig. 2\nThe systems\u2019 performances for predicting SGD functions after applying 5-fold cross validation\nWe also assessed each system for accurately inferring the functions of each GO term at different hierarchical levels (depths) of the GO ontology. The size of proteins annotated with the functional category of a GO annotation term decreases as its hierarchical level increases. We aim at investigating whether the accuracy of a system for predicting the functional categories of GO annotation terms gets better as the sizes of these terms increases. We randomly divided the proteins annotated with each functional category c into two sets. We considered the proteins in the first set as unannotated, whose functions need to be detected. We considered the biomedical texts associated with the proteins in the second set as a training dataset. We computed the performance of each system for predicting the functions of c at different hierarchical levels. Figures 3 and 4 show the results achieved by each system.\n\nFig. 3\nThe Recalls of the systems for predicting the functional categories of the set of GO terms positioned at the same hierarchical level of the GO ontology\n\n\nFig. 4\nThe Precisions of the systems for predicting the functional categories of the set of GO terms positioned at the same hierarchical level of the GO ontology\nWe ran each system ten times against the GO dataset. The number of proteins, whose associate biomedical texts are used as a training dataset, keeps accumulating at each run. At each run, we randomly selected 1000 Biological Process testing proteins and 500 Molecular Function testing proteins as unannotated and assessed the systems for predicting their functions. The first run was performed using: (1) 52,386 Biological Process proteins and 11,576 Molecular Function proteins, whose associate biomedical texts are used as a training dataset, and (2) 1000 Biological Process proteins and 500 Molecular Function proteins, whose functions are considered unannotated. At each run, thereafter, the set of proteins, whose associate biomedical texts are used as a training dataset, includes also the Biological Process and Molecular Function proteins, whose functions were annotated in the prior run. Figures 5 and 6 show the results achieved by each system.\n\nFig. 5\nThe Recalls of the systems for predicting the functional categories of GO terms using a cumulative set of proteins, whose associate biomedical texts are used as a training dataset\n\n\nFig. 6\nThe Precisions of the systems for predicting the functional categories of GO terms using a cumulative set of proteins, whose associate biomedical texts are used as a training dataset\nWe compared PL-PPF with DeepGO [15] using protein centric maximum F-measure. DeepGO uses deep learning to learn features from protein sequences for the purpose of predicting protein function. It uses the dependencies between GO Classes to construct the learning model. We followed the same experimental setting used for evaluating the DeepGO method as described in [15]. We also compared the two systems using the same dataset described in [15]. Specifically, we compared the two systems using the following:(1).\nThe protein centric maximum F-measure, which was used in evaluating the DeepGO method.\n\u00a0(2).\nThe same GO dataset used in evaluating the DeepGO method (the dataset is shown in Additional file 1: Table S2 of [15]).\n\u00a0Figure 7 shows the protein centric maximum F-measure of PL-PPF and DeepGO for predicting the functional categories of the GO dataset described in [15].\n\nFig. 7\nThe protein centric maximum F-measure of PL-PPF and DeepGO [15] for predicting the functional categories of the GO dataset described in [15]\nTable 10 shows the percentages of valid explicit and implicit terms that PL-PPF identified in the datasets used in the experiments. For each of the GO and SDG datasets used in the experiments, Table 10 presents the percentages of terms in the Biological and Molecular Function ontologies identified by PL-PPF. As the table shows, the percentages of implicit terms that PL-PPF identified are considerable.Table 10\nThe percentages of valid explicit and implicit terms that PL-PPF identified in the datasets. For each of the GO and SDG datasets used in the experiments, the table presents the percentages of valid terms in the Biological Process and Molecular Function Ontologies identified by PL-PPF\n\n\nAdditional file 1:\nThe data presented in Tables S1 and S2 is the Biological Process and Molecular Function GO annotation terms used in the experiments as well as the randomly selected sets of training and testing proteins annotated with the functions of these GO terms. (DOCX 56 kb)", "s12859-019-2635-6": "Accurate identification of perturbed signaling pathways based on differentially expressed genes between sample groups is one of the key factors in the understanding of diseases and druggable targets. Most pathway analysis methods prioritize impacted signaling pathways by incorporating pathway topology using simple graph-based models. Despite their relative success, these models are limited in describing all types of dependencies and interactions that exist in biological pathways.In this work, we propose a new approach based on the formal modeling of signaling pathways. Signaling pathways are formally modeled, and then model checking tools are applied to find the likelihood of perturbation for each pathway in a given condition. By adopting formal methods, various complex interactions among biological parts are modeled, which can contribute to reducing the false-positive rate of the proposed approach. We have developed a tool named Formal model checking based pathway analysis (FoPA) based on this approach. FoPA is compared with three well-known pathway analysis methods: PADOG, CePa, and SPIA on the benchmark of 36 GEO datasets from various diseases by applying the target pathway technique. This validation technique eliminates the need for possibly biased human assessments of results. In the cases that, there is no apriori knowledge of all relevant pathways, simulated false inputs (permuted class labels and decoy pathways) are chosen as a set of negative controls to test the false positive rate of the methods. Finally, to further evaluate the efficiency of FoPA, it is applied to a list of autism-related genes.The results obtained by the target pathway technique demonstrate that FoPA is able to prioritize target pathways as well as PADOG but better than CePa and SPIA. Also, the false-positive rate of finding significant pathways using FoPA is lower than other compared methods. Also, FoPA can detect more consistent relevant pathways than other methods. The results of FoPA on autism-related genes highlight the role of \u201cRenin-angiotensin system\u201d pathway. This pathway has been supposed to have a pivotal role in some neurodegenerative diseases, while little attention has been paid to its impact on autism development so far.Analysis of gene expression experiments comparing two groups of samples (e.g., normal and diseased), typically results in long lists of differentially expressed genes (DEGs). These long lists of genes are often hard to be interpreted by researchers. As a result, some methods have been developed to transform the gene expression data into meaningful sets. An example is to identify the set of genes that function in the same pathway which is commonly referred to as pathway analysis. This analysis is appealing to researchers for two reasons: first, grouping thousands of genes by the pathways in which they exist and involve, reduces the complexity to some hundred pathways; second, it facilitates identifying gene signaling networks relevant to a given condition which can help in understanding the mechanisms of diseases [1, 2], develop better drug production [3\u20135], personalize drug regimens [5, 6], etc.The pathway analysis methods usually use two types of data as inputs: the experimental data, like gene expressions obtained when comparing two conditions and the pathway knowledge, which was previously known and stored in pathway databases. There exist several pathway databases providing collections of pathways for various organisms, most of which are drawn manually and updated regularly. Examples of these databases include KEGG [7], BioCarta/NCI-PID [8], PANTHER [9] and Reactome [10]. Pathway analysis tools use one or more pathway database(s) as their input and identify the pathways that are most relevant to a given condition. For more information regarding the pros and cons of various pathway analysis methods, please refer to the review published by Khatri et al. [11].The pathway analysis methods are classified into two groups according to their strategy for incorporating the pathway data into their analysis: The first group considers pathways as simple gene lists [12\u201318] and the second group incorporates pathway topology in the analysis [19\u201326]. The former is usually referred to as \u2018gene set based\u2019 approach, and the latter is referred to as \u2018Pathway Topology-based\u2019(PT-based) approach. PT-based approach adds pathway topology in the analysis for utilizing the correlation between pathway components. The first proposed PT-based method was named Pathway-Express, as part of the Onto-tools suite [19]. Following that, some PT-based methods have been proposed. A comparison of PT-based methods is made by Mitrea et al. [27] with respect to their inputs, output, and analysis strategies.Most PT-based methods [20, 24] model the biological pathways as simple graphs. They model genes as nodes and interactions among them as directed edges between nodes. This kind of modeling has some limitations: First, simple graphs are limited in describing all types of relations among genes involved in the same interaction. As some examples: (a) If a protein has some activators and inhibitors, an inhibitor may prevent the activation of the protein by each of its activators. Methods that use graphs to model signaling pathways use +\u20091 weight edges for activation interactions and\u2009\u2212\u20091 weight edges for inhibition interactions, which does not accurately model the reality. (b) The condition where some genes together activate a gene; second, assume that a pathway is activated through a single receptor. If that particular receptor is not produced, the pathway will be probably completely shut off [20]. This problem is not addressed correctly by the graph modeling of signaling pathways; third, a simple graph is unable to model the concurrent and stochastic behavior of biological pathways.Due to the similarity between biological systems and distributed systems studied in computer science, modeling techniques developed in formal methods can be applied to biological systems as well [28]. Formal methods are techniques for specification, verification, and analysis of systems. Systems are described rigorously by formal languages that help to reduce any ambiguity in the system specification. Once a model is constructed, it can be translated into a computer program for simulating the system under specification. This program can be used for reasoning and analyzing the system, predict the behavior of the system with some initial conditions, validating new experimental result, and identifying the inputs or parameters of the system enforcing a desired behavior [29]. Regev et.al. [30] were the first to propose considering signaling pathways as distributed computer systems. Since then, there has been a successful development in using formal methods in analyzing signaling pathways [31\u201333]. However, the objective of them is to model specific pathways to describe and analyses their dynamics rather than finding the most impacted signaling pathways in a given condition, the primary objective of this study. In this study, signaling pathways are formally modeled initially, and then model checking is used to find the likelihood of perturbation for each pathway in a given condition. FoPA tool is implemented based on this approach. Model checking is an automatic verification technique for finite state concurrent systems that helps to check whether a system model meets specified properties, by exhaustively exploring all possible executions of the system. In addition to the widespread application of this technique for ascertaining the correctness of distributed systems in computer science, it has recorded a remarkable success in analyzing biological signaling pathways [34, 35]. To the best of the authors\u2019 knowledge, the proposed approach is the first attempt to use model checking to identify the pathways that are significantly affected in a given condition.In this section, first, some related basic concepts and primary definitions are briefly explained and then proposed approach is described in more details.Probabilistic model checking is a variant of model checking used for analyzing systems that exhibit probabilistic behavior. A probabilistic model checker requires (a) a formal description of the system (formulated in some precise mathematical language), and (b) the specifications of one or more desired properties of that system in temporal logic (e.g., CTL or LTL). A model is typically a state-transition structure in which each state represents a configuration, and the transitions represent the evolution of the system from one configuration to another. In probabilistic model checking, the models are probabilistic (typically variant of Markov chains), in the sense that they are augmented with a probability of making a transition between states. As an endpoint, a probabilistic model checker returns \u201cyes\u201d or \u201cno\u201d indicating whether or not each property is satisfied, or the probability of some properties of the model, based on a systematic and exhaustive exploration of the model.PRISM [36, 37] is a probabilistic model checker used for formal modeling and verification of quantitative properties of systems that exhibit random or probabilistic behavior. It can be used for analyzing different types of probabilistic models: continuous-time Markov chains (CTMC), discrete-time Markov chains (DTMC) and Markov decision processes (MDP). Models must be specified with the PRISM language, a simple language, based on the Reactive Modules formalism [38]. Properties to be verified against these models are expressed in probabilistic extensions of temporal logic.The symbol a is an action label used for synchronization. If a transition does not have to synchronize with other transitions, then no action label needs to be provided for that. The guard is a predicate over all the variables in the model. When the guard is true, the model is updated according to the transitions and their probabilities described in the updates. The transitions are specified by giving the new values of the variables in the module, possibly as a function of other variables. The primed variable is used to represent the new values for the variables [39].The P operator in the PRISM property specification language is used to reason about the probability of an event\u2019s occurrence. For computing the actual probability that some behavior of a model is observed, PRISM allows the P operator to take the following form: P\u2009=? [pathprop]. pathprop is a formula that evaluates to either true or false for a single path in a model that describes the desired behavior [40].After that, the Markova chain model is given to score calculator which allocates a score to each pathway by executing its model with the help of a model checking tool. A Model checking tool receives a model of the system and checks whether this model satisfies given properties expressed in logical formulas. Therefore, in our application, the properties should be defined in a fashion that if they are satisfied with the model, the model could be considered related to the condition. An example of such properties is to check that whether a high-level process (e.g., apoptosis) in the given signaling pathway model is activated differentially when the model is initialized with the given differential expression of genes. The idea behind this property is that the signal transduction is a process that ultimately results in a cellular response.The example property explained above is expressed by PRISM notation in Fig.\u00a01, which means the probability of the apoptosis response being active eventually in the future. The probabilistic model checker is employed to check this property against the model. The probability returned by the probabilistic model checker is used to allocate a score to the pathway. The higher the score, the higher the relevancy of the pathway to the given phenotype (A toy example and more definitions and proofs are provided in Additional file 1).KEGG signaling pathways consists of proteins/genes (throughout this paper, gene is used instead of protein and means protein-coding gene), small molecules and their interaction which includes but not limited to activation, inhibition, phosphorylation, dephosphorylation, and expression. They are represented by KGML format which is an XML representation of KEGG pathway maps. Model builder converts the KGML files into a formal model that can be executed by the PRISM tool.Before building a PRISM model, some editing steps in KGML representation of signaling pathways have to be taken. First, in cases that several nodes are annotated with the same gene symbol, they are merged into a node, sharing all incoming and outgoing edges of the original nodes. Next, nodes representing small molecules and other non-gene parts are removed in a fashion that the parents and children of such a node stay connected.In the following, modeling of interaction and inhibition interactions are described, where the rest of the interactions are modeled similarly.In an activation interaction (A\u2009\u2192\u2009B), an active gene A will activate gene B. The command for this interaction is expressed as command (1) in Table 1. In this command, A and B indicate the variables for modeling genes A and B. If A is active, (i.e., it is in states 3 or 4) and B is expressed either differentially (i.e., it is in state 2) or not (i.e., it is in state 1), then B will be active with the probability probactive, while with the probability 1-prob be not active. The gene B moves to state 3 if neither A (the activator gene) nor B (The activated gene) belongs to the differentially expressed genes and it moves to state 4 if either A or B or both belong to differentially expressed genes.The inhibition interaction (A \u22a3 B) indicates that A inhibits the activation of gene B, that is, if A is active, B will not be activated. This interaction is modeled with commands (2) in Table 1. The first command indicates that if gene B is expressed and gene A is not activated then the gene B will be activated with probability probinhibit1. If gene B belongs to differentially expressed genes, then it moves to state 4, otherwise moves to state 3. The second command indicates that if both genes A and B are active, A cause the inactivity of gene B with probability probinhibit2.In addition to commands for modeling each interaction, different commands are defined to initialize each variable. The measurement errors of data are considered in these commands. The command (9) in Table 1 shows these initializations wherein A represents a desired gene.The model built by the Model builder is parametric. These parameters are probactive (the probability of activation commands), probinhibit1 and probinhibit2 (the probability of inhibition commands),\u00a0probinit (the probability of activating a gene, which is not activated nor inhibited by other genes in the pathway),\u00a0prob1 and\u00a0prob2 (the probability of initializing the variables representing gens) which are estimated by Parameter computation.The diff(A,B) term in the Eqs. (1\u20133) is formulated in Eq. (4). The reason behind the ratios in this equation is that two class of genes is defined in FoPA: DEG (differentially expressed gene) and notDEG (not differentially expressed gene). So, there are three types of relationships considering these classes: (a) notDEG \u2013 notDEG, (b) notDEG \u2013 DEG, and (c) DEG \u2013 DEG. It is expected that when there are more DEG-DEG relations in a pathway compared to another pathway, the first one is more relevant to the condition under study. For example, let\u2019s suppose that two pathways have the same number of DEG genes with different numbers of DEG-DEG relations. In the absence of other factors, the pathway with more DEG-DEG relations seems to be more relevant to the condition under study. Hence, a higher weight is assigned to DEG-DEG relations than DEG-notDEG relations, and likewise, DEG-notDEG relations are assigned higher weight than notDEG-notDEG relations. The relative values 1, 2, and 3 are chosen to weight these relations.The\u00a0probinit parameter which is used in modeling the activation of gene A that is not activated nor inhibited by other genes in the pathway (Command (8) in Table 1) is set equal to P(A).To estimate \u03b1, let\u2019s suppose that, a cut off equal to v is chosen for FDR adjusted p-values for discovering the differentially expressed genes. It means that there is a v% chance that we make the wrong decision. In other words, the gene discovered as differentially expressed is not differentially expressed with the probability v%. Accordingly, \u03b1 is chosen equal to the cut-off value v.Pathway scores are intended to provide the amount of change incurred by the pathway between two phenotypes (e.g., normal and diseased). However, the amount of change can take place by chance. Consequently, an assessment of the significance of the measured changes is required.To obtain the significance of the measured change the null distribution must first be estimated. The null hypothesis here is that the differential expression of the genes does not associate with the condition under study. Consequently, for constructing null distribution, the pathway scores for the situations where the random number of DE genes are scattered randomly in the pathway are of interest.Thus, the null distribution is constructed by permuting the label of the normal and disease samples. This procedure generates samples under the assumption that no particular association between the gene differential expressions and phenotype exist. Class label permutations allowed to maintain gene-gene relations but remove the association between differential expressions of genes and the condition under study.All 36 datasets in the mentioned benchmark of the target pathway technique are available from the Gene Expression Omnibus (GEO) (details for each dataset are given in Additional\u00a0file\u00a02). These datasets are collected and normalized as \u2018KEGGdzPathwaysGEO\u2019 [42] and \u2018KEGGandMetacoreDzPathwaysGEO\u2019 [43] R packages. For all, a moderated t-test between disease and normal groups is performed by using the R limma package [44], followed by selecting genes with FDR adjacent p-values [45] less than 0.05 as differential.Assessing the correctness of any pathway analysis method in real experiments is a challenging task because a real gold standard has been not proposed yet. Lack of a definitive answer concerning the involvement of a given pathway in a given condition makes it impossible to calculate exact values for sensitivity, specificity, ROCs. Under such circumstances, it is best to compare the results of the desired pathway analysis method with other available and well-known methods.Among available TP-based methods, the ones with available R scripts or packages for downloading are of concern in this study. These methods are compared by Bayerlova et al. [46]. In this comparison, centrality based pathway analysis (CePa-GSA) [24] indicates better results, therefore, FoPA is compared with it. Moreover, signaling pathway impact analysis (SPIA) [20] is chosen for comparison, because, it is the first introduced PT-based method and almost, all methods compare their results with it. Furthermore, some gene set based methods are compared by Tarca et al. [47] indicating that the pathway level analysis of gene expression (Plage) [14], Globaltest [12] and pathway analysis with down-weighting overlapping genes (PADOG) [16] outperform their counterparts. Because PADOG is newer and ranks the target pathways better than the other two, it is chosen for comparison here. In the following, these methods are described briefly.CePa_GSA incorporates network centralities to weight gene-level statistics, and then these statistics transforms into pathway level statistics. For gene level statistics the absolute value of t-statistic is used as default, and the default function for computing pathway level statistics is mean function.SPIA is combining two scores. For computing the first one, it is assumed the same as the simple ORA methods, that the number of DEGs in a given pathway follows the hypergeometric distribution. To obtain the second, so-called perturbation score, the pathway topology information is incorporated into the analysis. First, to each gene in a pathway a perturbation factor is assigned which is the logarithm of the fold-change (logFC) of this gene and the sum of perturbation factors of its direct upstream genes normalized by the number of all its downstream genes. The terms of the sum are weighted by the type of interaction between genes: 1 for activation and\u2009\u2212\u20091 for inhibition. Next, the accumulated perturbation of each gene is computed by the difference between the perturbation factor of that gene and its observed logFC. Finally, the accumulated perturbations of the pathway\u2019s genes are aggregated in total pathway accumulated perturbation. The two scores are then combined into a global score by using Fisher\u2019s product test.Gene set scores in PADOG are computed by first calculating moderated t-scores for all the genes, and then integrating the weighted moderated t-scores in a global gene set score. The weight of a gene in a gene set is down-weighted if it is involved in multiple gene sets. Thus, the higher weights are given to gene set specific genes, prioritizing their effect in the scoring.Most of the pathway analysis methods usually select a few real datasets for comparing their methods with others and then interpret the results either with the help of a life scientist or by searching the published literature. But since a large number of pathways are implicated directly or indirectly in any biological condition, authors may select specific literature as supporting evidence. Thus, this type of validation may lead to biased results.A better assessment approach must eliminate human bias and be performed on a large number of datasets and conditions. The validation approach introduced by Tarca et al. [47] is reproducible, based on multiple datasets, and does not require an expert human evaluation of the results.In this approach, multiple microarray datasets are used as a benchmark. Every dataset in this benchmark represents a particular disease coming from different tissues and laboratories. Each dataset has been linked to a defined pathway from the KEGG database which is considered to be the target pathway for that dataset. For example, a dataset comparing normal and cancerous colon would have \u2018colorectal cancer\u2019 as its target pathway. It is expected from any pathway analysis method to identify the colorectal pathway as affected and rank it close to the top. Methods are compared based on their performance in ranking the target pathways.Based on the above explanations, the sensitivity of a method is defined as the median p-value of the target pathways over benchmark datasets (a lower p-value indicates higher sensitivity). The prioritization of a method is the medians of the rank percentage of the target pathways over benchmark datasets.The disadvantage of the target pathway technique is focusing on only one pathway for each dataset, whereas the behavior of a biological system may be governed by more than one pathway in a given condition. Because in reality, there is no a priori knowledge of all relevant pathways, the simulated false inputs (permuted class labels and decoy pathways) are chosen as a set of negative controls.In the first, 50 trials are used wherein the class labels (e.g., normal, disease) of the actual samples are randomly permuted before the analysis. The percentage mean of the significant pathway subject to this null hypothesis is expressed as the false positive rate of the method. By using this null hypothesis, the expression levels are dissociated from the studied phenotypes while the gene-gene correlations are preserved. In the second technique, the simulated decoy pathways are chosen as a set of negative controls. Decoy pathways are generated using KEGG pathways. A decoy pathway maintains the structure of the KEGG pathway that is made from, with the difference that its genes are substituted with random genes from the set of all genes. Compared methods are run on both KEGG real and decoy pathways to check their ability to distinguish decoy from real pathways. The ROC curves are created by plotting the true-positive rate (the rate of the real pathways) against the false-positive rate (the rate of the decoy pathways) at various threshold levels. The area under these curves is defined as a measure of how methods can well distinguish between the decoy and real pathways.Dong et al. [17] assumed that a successful method should produce consistent results for independent datasets under similar studying conditions. They select three independent datasets and performed enrichment analysis for them and then counted the number of overlapping gene sets that are significant in at least two datasets at a given rank threshold. Here, the same strategy is followed to test the performance of the FoPA under similar conditions. The analysis is performed for five independent colorectal cancer datasets (GSE4107, GSE8671, GSE9348, GSE23878, GSE4183). At a given rank threshold (e.g., top 10, 20, \u2026, 50 significant pathways) and for each pair of datasets, the number of overlapping pathways that are identified relevant is counted. This value demonstrates the consistency between the results of the method for these five datasets.Because of tumor heterogeneity nature of cancers including colorectal cancer, another test for consistency analysis is performed. In this test, instead of choosing independence datasets, a dataset with N sample is selected. This dataset is randomly resampled to obtain datasets of size n\u2009<\u2009N. The analysis is then performed on these new resampled datasets. Since these datasets are chosen from the same experiment, mehods should produce consistent results on them.As an application of FoPA, it is applied to two real data samples. The first one is the colorectal cancer dataset used in the target pathway technique (GSE8671) and the second one is a list of autism-related genes identified by Rubies et al. [48].The p-values produced for each pathway by a pathway analysis method must be uniformly distributed in the interval [0,1] when the null hypothesis is true [49]. If the p-values are not uniformly distributed, the result of the pathway analysis method may be biased. For example, pathways that have p-values biased towards zero may often be falsely identified as significant.In [49] an approach for constructing an empirical null distribution for analysing the systematic bias of the methods is proposed. In this approach, the expression data related to the control samples of some independent Alzheimer\u2019s disease experiments are used. Half of these samples are randomly labeled as disease, and the rest are labeled as normal. This procedure is repeated many times to generate different groups of control and disease samples. Groups with fewer or more disease samples (e.g., 10 diseases and 20 normal or 20 diseases and 10 normal) are also generated to eliminate the effect of sample size. Then, the p-values of the KEGG signaling pathways are calculated for each group. These p-values should be uniformly distributed.Currently, available pathway databases are not complete and may be noisy. To inspect how incompleteness and noise affect FoPA\u2019s performance, they are mimicked by randomly removing or rewiring a portion (e.g., 10, 20, 30,40, and 50%) of interactions in a pathway.By interaction removal, an interaction between genes is removed and by interaction rewiring the endpoint of an interaction is set uniformly to a new gene from the pathway. The edge removal and edge rewriting will not remove genes from the pathways but will make changes in the gene-gene interactions.At each portion of edge removal or rewriting for each pathway, the procedure is repeated 100 times. The similarity measure for each pathway is computed as the number of generated pathways identified related or unrelated as the main pathway divided by the number of genrated pathways.The mean and mean reciprocal ranks of the target pathways are almost equal in PADOG, and FoPA suggested that on average FoPA ranks the target pathways as well as PADOG ranks them. This Wilcoxon signed rank test which is done on the rank and p-value of PADOG and FoPA also confirms this outcome and shows that there is no statistically significant difference between the distribution of ranks and p-values of FoPA and PADOG. Thus, it can be concluded that PADOG and FOPA are able to prioritize target pathways with high sensitivity when compared with CePa and SPIA.Two experiments are used to measure the false positive rate of the methods. In the first, 50 trials of the original datasets with randomly permutated class labels (e.g., normal, disease) are used. The percentage mean of the significant pathway subject to these random datasets is considered as the false positive rate of the methods. In the second, the simulated decoy pathways generated by connecting random genes using the same network structure as the KEGG pathways, are chosen as a set of negative controls. Compared methods are run on both KEGG real and decoy pathways to check their ability to distinguish decoy from real pathways.In Table 3, the \u2018colorectal cancer\u2019 and \u2018Renal cell carcinoma\u2019 are overlapped respectively among the significant pathways found by PADOG in four and three of the colorectal cancer datasets. However, the null distribution analysis performed in [49] has shown that p-values produced by PADOG for the \u2018colorectal cancer\u2019 and the \u2018Renal cell carcinoma\u2019 pathways are biased towards zero, that makes these pathways detected as significant regardless of input dataset.FoPA found the \u2018Vasopressin-regulated water reabsorption\u2019 and the \u2018Aldosterone-regulated sodium reabsorption\u2019 pathways overlapped between the significant pathways in four colorectal datasets.The presence of vasopressin receptors was reported in transformed epithelial cells, as well as in a wide panel of human tumor cell lines [50]. In addition, the expression of vasopressin receptor is confirmed in commercially available colon tumor samples [51]. Moreover, desmopressin, a synthetic analogue of vasopressin, has shown significant antitumor activity in preclinical murine models of colorectal cancer [52]. Therefore, finding the \u2018Vasopressin-regulated water reabsorption\u2019 pathway as an overlapped pathway in colorectal cancer datasets is in line with other findings.We refer to the research done by Guo et al. [53] to indicate that the second overlapped pathway, \u2018Aldosterone-regulated sodium reabsorption\u2019, is also consistent with the other findings. In this research, some colorectal datasets are integrated to elucidate the potential key candidate genes and pathways in CRC. The \u2018Aldosterone-regulated sodium reabsorption pathway\u2019 is among the candidate pathways that have been identified as common in CRC.From the five colorectal cancer dataset above, GSE8671 is chosen, since it has more samples than the others. This dataset is randomly resampled to obtain sub-datasets of size 8, 16, and 32. This procedure is repeated 50 times to create different groups of samples. The result of the four methods for these datasets is assessed for consistency of results. At each given rank threshold the average number of overlapped pathways are shown in Fig. 8b. As it clear, the average number of overlapped pathways identified by FoPA is more than that of other methods in almost all rank thresholds.According to the analysis accomplished by FoPA, six pathways (p-value <\u20090.05) are recognized as relevant to colorectal adenoma (GSE8671). The first one is the \u2018non-small cell lung cancer signaling pathway\u2019. This is consistent with a recent study [57] shown that the majority of genes for colon and lung cancer susceptibility are linked pair-wise and are likely identical or related.The second identified one is the \u2018Intestinal immune network for IgA production\u2019. The differentially expressed genes of the present dataset show that the expression of the majority of genes in the \u2018intestinal immune network for IgA production\u2019 pathway is lower than that in the normal mucosa. These include a series of human leukocyte antigen (HLA) class II genes (HLA-DOA, DPA1, DPB1, DQA1, DQA2, DQB1, DMB, DRA, DRB1, DRB3, DRB4, and DRB5). These genes encode major histocompatibility complex class II molecules in antigen presenting cells (B lymphocytes, dendritic cells, and macrophages), which are essential for the proliferation and differentiation of B cells [58]. Since IgA-secreting cells contribute to reducing inflammatory response which is a strong risk factor for the development of gastrointestinal adenocarcinomas, it is likely that the impairment of IgA production may drive further inflammatory responses and promote tumor growth. This is consistent with prior studies that showed the influence of IgA-secreting cells and B cells to colon tumors progression [59, 60].The third identified pathway is \u2018Aldosterone-regulated sodium reabsorption\u2019. In this pathway, Aldosterone binds MR (Mineralocorticoid Receptor), which translocate into the nucleus and regulates gene transcription. A recent study [61] have demonstrated that decreased MR expression can contribute to angiogenesis and poor patient survival in colorectal malignancies and they show MR activation in the presence of a physiological amount of aldosterone exerts a negative role on angiogenesis.The fourth pathway is \u2018Tight Junction\u2019(TJ) pathway. Claudin family proteins consisting of at least 24 members are essential for the formation of TJs and have a significant effect on the biological behavior of tumor progression. Previous studies have demonstrated the several claudin (claudin-1 [62, 63], claudin-3 [64, 65], claudin-4 [64], claudin-7 [65]) aberrant expression patterns in colorectal cancer. Among them, claudin-1 and claudin-2 overexpression are identified in the present dataset.The fifth pathway is \u2018p53 signaling pathway\u2019 where its dysfunction is highly prevalent in most cancers [66].Autism exome sequencing study: Rubeis et al. [48] have analyzed exome sequencing of autism patients and healthy people and identified 22 autism-related genes. Here, these genes are considered as differentially expressed genes and the mutation rate of them as the probability of each gene instead of t score in differentially expressed genes analysis.It has been reported that atypical processing of odor and taste stimuli is presented in autism spectrum disorders (ASD) [67, 68]. A study [69] examined the relationship between sensory responsiveness and social severity in children with high functioning ASD. Analyses revealed scores of oral sensory, olfactory, and touch as the strongest predictors of greater social impairment in autism.Asthma is another identified KEGG pathway. Recently, researchers at Sydney University\u2019s Brain and Mind Centre have published a study that shows a relationship between a mother\u2019s active immune response during pregnancy to allergies and asthma and severe social impairment symptoms in children with autism [70].GABAergic synapse may be one of the important relevant pathways to ASD. Several lines of evidence suggest that an impairment of GABAergic transmission contributes to the development of ASDs. GABAergic signaling dysfunction early in development leads to a severe excitatory/inhibitory unbalance in neuronal circuits, a condition that may account for some of the behavioral deficits observed in ASD patients [71].The last pathway in the list is the Renin-angiotensin system (RAS). This pathway has been hypothesized to have a pivotal role in some neurodegenerative diseases, such as Parkinson, Alzheimer, Huntington and Multiple Sclerosis (MS) [72].Angiotensin-converting enzyme (ACE) is the essential enzyme in this pathway which plays a major role in the degeneration of a family of neurotransmitters in the central nervous system (CNS). The implication of neurotransmitters in psychiatric disorders is supported by their function in the regulation of emotions, cognition, behavior, and memory which are disrupted in autism [73].This analysis highlights the role of Renin-angiotensin system and GABAergic synapse pathways in ASD. It can be concluded that there may exist relations between GABAergic synapse pathway and RAS in the development of autism. Further genetic studies can support this finding.In this study, a new pathway analyzing approach is introduced, that uses formal methods to rank pathways according to their relevance to a given clinical condition (e.g., disease). To the best of the authors\u2019 knowledge, it is the first attempt in using the formal methods in solving such a problem. Formal modeling has many advantages over the modeling by graphs. It helps researchers to express any relations among biological components involved in an interaction. This helps to create a reliable model of signaling pathways that can be effective in reducing the false results in pathway analysis studies. The proposed tool named FoPA is constructed based on this new approach. We have compared FoPA with three other analysis methods, two topology-based (CePa, SPIA) and one gene set-based (PADOG). These methods are chosen regarding their performance in previous comparisons.Some techniques are used to evaluate the FoPA\u2019s performance and compare it with other methods. We have assessed the results considering both rankings and P-values of the target pathways. The results indicate that PADOG and FoPA are able to prioritize target pathways with high sensitivity when compared with CePa and SPIA. However, considering the target pathway technique performance is not enough. Thus, the simulated false inputs (permuted class labels and decoy pathways) are created as a set of negative controls to measure the false-positive rate of the methods. The number of significant pathways identified by giving permuted class labels to FoPA is less than the other three methods; that is, FoPA differentiates significantly between actual and random clinical data. Moreover, the area under the curve ROC is greater in FoPA compared with PADOG and CePa, which indicates that FoPA excludes more decoy pathways from real ones.The results of the methods on independent colorectal cancer datasets indicate that, FoPA identifies more overlaps than PADOG and SPIA at almost all rank thresholds and its finding is in line with other researches. Although CePa has found more overlaps in some rank thresholds, these overlaps appear to have false pathways. The consistency analysis is also performed on a group of dependent datasets (datasets made by resampling). The average overlapped pathways found by FoPA on these datasets is more than that of other methods.We also have demonstrated that there is no systematic bias in FoPA that makes some pathways detected as significant regardless of the input differentially expressed genes.As an application of FoPA, we apply it to a list of autism-related genes and show that FoPA can discover pathways relevant to autism. This analysis highlights the role of Renin-angiotensin system and GABAergic synapse pathways in ASD.These lines of evidence well demonstrate FoPA\u2019s advantage over the other methods. One of the disadvantages of FoPA may be its high running time compared with available statistical methods. Though this running time is tolerable, it will be decreased through the improvement made in our modeling and also in formal verification techniques.The other disadvantage of FoPA is that it treats pathways as independent entities, and gives more focus on pathway-specific genes rather than overlapped genes among pathways. While some of these overlapped genes will lead to \u201ccrosstalk\u201d phenomenon that could influence other pathways. As a result, considering cross-talk genes and inter-pathway relations may lead to better performance of FoPA.It is worth mentioning that FoPA is the first attempt of using formal methods in pathway analysis. So, by adding other details in model specification and considering some other aspects such as pathway\u2019s cross-talk, the result of the experiments would be improved significantly.Area under curveCentrality based pathway analysisContinuous-time Markov chainDifferentially expressed geneDiscrete-time Markov chainsFormal model based checking pathway analysisMarkov decision processesPathway analysis with down-weighting overlapping genesSignaling pathway impact analysisNot applicableThe FoPA tool is implemented as an open-source python package which is available from github at https://github.com/fmansoori/FoPA.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.A model described in the PRISM language consist of a set of modules that can interact with each other. The state of each module is being represented by the value of a set of finite-ranging variables. The global state of the whole model is determined by the local state of all modules. The behavior of each module is described by a set of commands of the form:To understand the proposed approach, let\u2019s formulate the query solved by the approach. Given that we have that two lists of genes R and R\u2019 associated with the desired phenotype (i.e., normal and diseased) and a list of pathways (i.e.., all signaling pathways of KEGG) the query is to infer which one of the pathways are more related to the given phenotype. Figure\u00a01 shows the proposed approach whose goal is to solve the query formulated above. The proposed approach requires a formal description of the behavior of the signaling pathways (formulated in some formal languages: i.e.., Petri net or PRISM modeling language). The differential expression of genes between the conditions under study are used to estimate the parameters of the model or define the initial configuration. Once the model is specified by the proper language, it should be converted into discrete time or continuous time Markova chain model which is usually done by the chosen model checking tool.\n\nFig. 1\nArchitecture of the Model checking based approach: Model checking based approach requires a formal description of the behavior of the signaling pathways. The differential expression of genes between the conditions under study are used to estimate the parameters of the model or define the initial configuration. Once the model is specified by the proper formal language, it should be converted into discrete time or continues time Markova chains model which is usually done by the chosen model checking tool. After that, the model is given to score calculator which allocates a score to each pathway with the help of a model checking tool. For example, Score computation requests the model checking tool to compute the possibility of a cellular response activation\nA pathway analysis tool named FoPA is introduced here, Fig.\u00a02, using the approach proposed above. Gene expressions of the desired condition and its matched controls are converted to a list of differential gene expression, which is fed into FoPA as input along with the signaling pathways of KEGG. The output is a list of signaling pathways sorted according to their relevance to the desired condition.\n\nFig. 2\nGeneral overview of the FoPA: A list of differentially expressed genes and the signaling pathways of KEGG are fed to FoPA as inputs, and the output is a list of signaling pathways sorted according to their relevance to the differential genes\nFoPA, Fig.\u00a03, consists of four parts: Parameter computation, Model builder, Score computation, and Significance assessment. The Model builder constructs a PRISM model for each pathway. The model parameters (i.e., the probability of interactions and the initial state of the model) are estimated by Parameter computation using the KEGG pathways and gene expressions data. Score computation defines the appropriate properties to be checked by PRISM [37]. By computing the probability of these properties, a score is allocated to each pathway. This score is intended to reflect the relevancy of the pathway to the condition under study. However, this score can take place just by chance. Thus, an assessment of the significance of the measured score is required, which is done by Significance assessment.\n\nFig. 3\nDifferent Parts of the FoPA: Model builder build a PRISM model for each pathway. The parameters of the model are estimated by Parameter computation using the KEGG pathways and gene expressions data. The Score computation defines the appropriate properties to check by PRISM. By computing the probability of these properties, a score is allocated to each pathway. An assessment of the significance of the measured score is done by Significance assessment\nFor modeling the KEGG pathway by PRISM language, a variable is assigned to each gene. This variable indicates the state of the gene which can take on six values where three of them correspond to no expression, expression, and differentially expression of the gene relative to the control expression level (e.g., as measured in normal tissue) and encoded as 0, 1, and 2, respectively. The value \u2212\u20091 for the variable representing a gene indicates that the variable is not initiated in the model yet. The values 3 and 4 belong to the activated states of the genes. When a gene is activated, it will move from one of its states 1 or 2 to the states 3 or 4. It will move to state 4 if it is differentially activated. A gene is differentially activated if it is a member of the differentially expressed genes and is activated, or it is activated by one of the members of differentially activated genes. Otherwise, it is not differentially activated and will move to state 3. By this type of modeling, the amount that a pathway is affected by the differentially expressed genes is considered. Interactions in KEGG pathways are modeled with PRISM commands where the details of them are tabulated in Table\u00a01.Table 1\nPRISM commands modeled signaling pathway\n\nActivation, Phosphorylation activation, Dephosphorylation activation, and Indirect effect are all different types of activation in which gene A activates the gene B. Thus, they model the same as the Activation relation. Likewise, the Inhibition, Phosphorylation inhibition, and Dephosphorylation inhibition are all different types of inhibition in which gene A prevents the activation of gene B and they model the same as the Inhibition relations. The probs are the parameters for the commands that are replaced with the appropriate values when the model is constructed. A and B in PRISM command are variables indicating the states of the genes A and B respectively\nTo estimate the probability of commands (A-B) pieces of evidence are combined which are as follows: (1) (2) (3)In diff(A,B), the value of \u03b1 can be arbitrarily selected from the interval [0,\\( \\frac{1}{3} \\)]. This interval has been chosen so that the value of diff(A,B) does not exceed 1. Since the pathway\u2019s score are used to compare the pathways, selecting an arbitrary value for \u03b1 will not affect the results, because all pathway\u2019s score are changed by a factor of \u03b1. In FoPA, the value of \u03b1 has been selected equal to \\( \\frac{1}{6}. \\) (4)By the second term in Eqs. (1, 3), P(A), the amount of change of the gene A between two conditions of interest is of concern by using the moderated t-score [41] of this gene as follows: (5)where T(A) is the moderated t-score of the gene A and the weight, Fn(A), is the function of the frequency of the gene A in the set of all pathways. The weight is defined such that reduced the contribution of the overlapping genes. The idea supporting this weighting is that whenever a differentially expressed gene appears in fewer pathways, it is assumed that particular gene reveals the evidence that those pathways are affected by the given condition. Therefore, the frequently appearing genes are assigned with a low weight close to the 0.0, while pathway-specific genes are assigned with a high weight close to the 1.0. Similar to what is done in Traca et al. method [16] the weight Fn(A) is defined as the normalized frequency of the gene A across all KEGG pathways in the scale of (0,1) as follows: (6)where f(A) is the frequency of gene A, min(f) is the minimum frequency of genes and max(f) is the maximum frequency of genes in the set of all pathways.The third term in Eqs. (1) and(3), is the probability of interactions (A\u2009\u2192\u2009B) and (A\u2009\u22a3\u2009B). To estimate these probabilities, the rational assumption is that the more the number of pathways in which A and B interact with each other than the number of pathways in which they have not interact with each other, the more the likely the interaction of A and B. Therefore, each pairwise interaction in the set of allpathways is checked against all the pathways in KEGG database. To estimate the probability of (A\u2009\u2192\u2009B) interaction, the number of pathways, where there exist A\u2009\u2192\u2009B are counted. The number of pathways where both A and B exist but have no activation association is between are also counted. Likewise, the activation interaction is replaced by inhibition to estimate the probability of inhibition interactions. Eqs. (7, 8) reveal how the probability of activations and inhibitions interactions are estimated, respectively. (7) (8)To estimate the prob1, prob2 parameters available in the Command (9) in Table 1, suppose, where the error for determining differentially expressed genes is \u03b1, according to which the equation set (9) is introduced as follows: (9)Score computation defines PRISM properties to check against the formal models of pathways. These properties are sought to find the possibility that the final effector genes (final genes that trigger cell responses) are differentially activated; that is, they are in state 4. This property is defined in PRISM language as: (10)where F is a temporal operator means eventually in the future, and g is a final effector gene. These probabilities are computed for every final effector genes of the pathway path, and their sum is used to assign a score to each pathway as follows: (11)Thus to assess the significance of a pathway score, the sample labels are swapped Nperm times and the score is recalculated for these new samples. Finally, the significant of pathway score is obtained as follows: (12)where I(.) is an indicator function, path\u2009_\u2009scorepermis the score of the pathway for each permutation, path\u2009_\u2009scorereal\u2009_\u2009sample\u00a0is the score of the pathway for the main data and Nperm is the number of permutations.The median of p-values and the median of the ranks of the target pathways over benchmark datasets are defines as sensitivity and prioritization of the methods. The results considering both the rankings and P-values of the target pathways associated with each dataset, Figs.\u00a04 and 5, are assessed (for additional details see Additional\u00a0file\u00a03).\n\nFig. 4\nComparing ranks of target pathways: Each box contains 36 data points representing the rank (%) of the target pathway in each method when using as input an independent dataset. Methods are ranked from best to worst according to the median rank\n\n\nFig. 5\nComparing p-values of target pathways: Each box contains 36 data points representing the p-values of the target pathway in each method when using as input an independent dataset. Methods are ranked from best to worst according to the median p-value\nThe summary of results for the four different methods based on the panel of 36 datasets is tabulated in Table\u00a02. Out of the four compared methods, CePa ranks the 1st regarding sensitivity, while it ranks the 3rd regarding prioritization; PADOG ranks the 2nd regarding sensitivity, and the 1st regarding prioritization and FoPA ranks the 3rd regarding sensitivity and the 2nd regarding prioritization.Table 2\nComparison between 4 methods regarding prioritization, sensitivity and the reciprocal ranks of the target pathways\n\nThe table shows the statistics computed from the p-values and ranks of the 36 target pathways identified by each method. The mean reciprocal rank is the average of the reciprocal ranks of the target pathways computed by the equation \\( \\frac{1}{N}{\\sum}_{i=1}^N\\frac{1}{{\\mathit{\\operatorname{rank}}}_i} \\). The results of comparing the ranks of each method against FoPA (chosen as reference), using a paired Wilcoxon test are also included. The best value for each measure is shown in bold\nThe false-positive rate of all methods for different significance threshold produced by analyzing 50 permuted versions of original datasets is shown in Fig.\u00a06. The percentage of all pathways found significant at different significance thresholds is reported for each method with a vertical bar which indicates that the FoPA false-positive rate is less than that of other methods.\n\nFig. 6\nComparing false-positive rates produced by four methods: The percentage of all pathways found significant at different significance thresholds is reported for each method with a vertical bar (the scale is logarithmic). The horizontal lines indicate the expected number of false positives at each threshold. Methods are ranked from best to worst according to their false positive rates\nThe results of three methods FoPA, CePa and PADOG in ranking decoy and real pathways are shown as ROC curves in Fig. 7. Two datasets (GSE6956C, GSE18842) are chosen out of 36 datasets to feed into the methods as inputs. No matter which datasets are selected, the decoy pathways should not appear in high ranks of the results for the input dataset. The results here have illustrated that the area under the curves ROC in FoPA is greater for both datasets. That is FoPA outperforms PADOG and CePa in distinguishing real from decoy pathways.\n\nFig. 7\nDistinguishing real and decoy pathways with FoPA, PADOG and CePa: Each line shows the receiver operator characteristic for distinguishing real and decoy pathways. Two datasets prostate cancer (GSE6956C) and Non-small cell lung cancer (GSE18842) are fed into the methods as desired conditions\nAn analysis is performed for five independent colorectal cancer datasets (GSE4107, GSE8671, GSE9348, GSE23878, GSE4183). The results of methods for these datasets are assessed at multiple rank thresholds. At a given rank threshold (e.g., top 10, 20, \u2026, 60 significant pathways) the overlapping pathways between the results of each pair of datasets are counted which is shown in Fig.\u00a08a. In this experiment, FoPA identifies more overlaps than PADOG and SPIA at almost all rank thresholds. CePa recognizes more overlaps in ranks 10, 20, 30, 40. However, it seems the recognized list by CePa as shown in Table\u00a03, contains false-positive (The complete results can be found in Additional files 4 and 5).\n\nFig. 8\nOverlaps among relevant pathways identified by different methods at the different rank threshold: a Scatter plots of the number of overlapped pathways identified in five colorectal cancer datasets. b Scatter plot of the number of overlapped pathways identified in resampled datasets of a colorectal cancer dataset (GSE8671) at the different rank threshold\nTable 3\nOverlapped pathways in the top 10 significant pathways found by each method in five colorectal cancer datasets (GSE4107, GSE8671, GSE9348, GSE23878, GSE4183)\nAs an example, the results of FoPA are analyzed on one of the datasets used in the target pathway technique and shown it is consistent with other available studies. This dataset compares 32 pairs of samples collected from colorectal adenomas with those of normal mucosa from the same individuals [54] using Affymetrix HG-U133 Plus 2.0. Microarray platform. This dataset is available via Gene Expression Omnibus (ID\u2009=\u2009GSE8671) (For details about the differentially expressed genes in this dataset, refer to Additional\u00a0file\u00a06). The KIA1199 gene is reported as the most overexpressed gene in this study. There is an increasing body of evidence that suggests the involvement of this gene in cancer progression, metastasis and poor prognosis of patients with colorectal cancer [55]. Cancer data analysis indicates that the expression of KIAA1199 and \u2018Wnt-signaling pathway\u2019 genes are correlated [56]. Thus, \u2018Wnt signaling pathway\u2019 is likely to be relevant to the condition under study in this dataset. Four pathway analysis methods are compared regarding their ability to identify the \u2018Wnt signaling pathway\u2019 as relevant to the present dataset. The results are tabulated in Table\u00a04 (For complete results refer to Additional file 5). As illustrated, FoPA identifies the \u2018Wnt signaling pathway as significant\u2019 (p-value <\u20090.05) and with a lower rank than the other compared methods.Table 4\nThe top 15 pathway retrieved by FoPA, PADOG, CePa and SPIA for the colorectal cancer (GSE8671) dataset\n\n'Wnt signaling pathway' and 'colorectal cancer' pathways shown in bold are expected to be impacted in GSE8671 dataset. These pathways are among the top-ranked pathways found by FoPA\nThe results of applying FoPA to autism-related genes are shown in Table\u00a05 (More results are given in Additional\u00a0file\u00a07). FoPA does not find any significant pathway (p-value <\u20090.05). However, five high ranked pathways worth to be considered as the likely related pathways to autism. Through reviews of literature, pieces of evidence are provided showing these pathways may be related to autism.Table 5\nresults of applying FoPA to autism-related genes\nTo analysis, the possible bias of the FoPA, a total of 1000 resampled datasets from 41 control samples of 4 Alzheimer\u2019s datasets (GSE5281_EC, GSE5281_HIP, GSE5281_VCX, and GSE16759) is generated. Some of them (18) are randomly labeled as disease and the remaining marked as normal samples. This procedure is repeated 500 times to create different groups of 18 disease and 17 control samples. To eliminate the effect of the group size 100 datasets consisting of 10 control and 10 diseases, 200 datasets consisting of 10 control and 20 diseases and 200 datasets consisting of 20 control and 10 diseases samples are also generated. The p-values of KEGG signaling pathways for each of the datasets are calculated using FoPA. The results, Fig.\u00a09, indicate that the distributions of p-values cumulated from all KEGG signaling pathways are a bit biased toward zero. However, it\u2019s not so much that it can affect the performance of the FoPA. The top 16 most biased pathways sorted by their distributions means have also shown in Fig. 9. It is showed that the distributions of p-values produced by FoPA are reasonably uniform for each of these pathways, while the results of the analysis in [49] have shown that PADOG and SPIA are biased towards generating lower p-values for some of the signaling pathways.\n\nFig. 9\nNull distributions of p-values produced by FoPA: The large panel on the left (A0), displays the distribution of p-values cumulated from all KEGG signaling pathways. The smaller panels on the right (A1-A16) display the p-values distributions of individual pathways, which are extreme cases sorted based on the distribution\u2019s mean\nPathway database incompleteness and noise are mimicked by randomly removing or rewiring a portion (e.g., 10, 20, 30, 40, and 50%) of the interactions in a pathway. At each portion of edge removal or rewriting for each pathway, the experiment is repeated 100 times. The similarity measure for each pathway is computed as the number of the generated pathways identified the same as the main pathway (both relevant or both not relevant) to the condition under study. The average of these measures for each pathway at each portion is illustrated in Fig.\u00a010. The results have shown that the average similarity drop with an increase of perturbations. Interaction rewriting has a more significant effect on FoPA than removal does.\n\nFig. 10\nThe impact of perturbation on the performance of FoPA: each time a portion of interaction in a pathway are randomly removed or rewired, FoPA is re-applied to infer is the generated pathway is identified the same as the main pathway (both relevant or both not relevant). The similarity measure is the number of the generated pathways identified the same as the main pathway\n\n\nAdditional file1:\nThe file contains the additional details on the following: i) formal definition of Markov chains ii) probability measure of Markov chains iii) reachability probabilities iv) a toy example showing how the model checking based approach works. IV) Results of FoPA on different disease datasets. (DOCX 3053 kb)\n\n\n\nAdditional file 2:\nThe 36 datasets used as a benchmark in the target pathway technique. (XLSX 11 kb)\n\n\n\nAdditional file 3:\nThe results of the target pathway technique (ranks and P-values of the target pathways associated with each benchmark dataset) for each of four methods (FoPA, PADOG, CePa, and SPIA). (XLSX 15 kb)\n\n\n\nAdditional file 4:\nThe results of four methods (FoPA, PADOG, CePa, and SPIA) on four colorectal cancer datasets (GSE4107, GSE9348, GSE4183, GSE23878). (XLSX 120 kb)\n\n\n\nAdditional file 5:\nThe results of four methods (FoPA, PADOG, CePa, and SPIA) on GSE8671 dataset. (XLSX 37 kb)\n\n\n\nAdditional file 6:\nList of the differentially expressed genes identified for GSE8671 dataset. (XLSX 1282 kb)\n\n\n\nAdditional file 7:\nThe results of FoPA on autism genes. (XLSX 416 kb)", "s12859-018-2449-y": "Molecular docking studies on protein-peptide interactions are a challenging and time-consuming task because peptides are generally more flexible than proteins and tend to adopt numerous conformations. There are several benchmarking studies on protein-protein, protein-ligand and nucleic acid-ligand docking interactions. However, a series of docking methods is not rigorously validated for protein-peptide complexes in the literature. Considering the importance and wide application of peptide docking, we describe benchmarking of 6 docking methods on 133 protein-peptide complexes having peptide length between 9 to 15 residues. The performance of docking methods was evaluated using CAPRI parameters like FNAT, I-RMSD, L-RMSD.Firstly, we performed blind docking and evaluate the performance of the top docking pose of each method. It was observed that FRODOCK performed better than other methods with average L-RMSD of 12.46\u00a0\u00c5. The performance of all methods improved significantly for their best docking pose and achieved highest average L-RMSD of 3.72\u00a0\u00c5 in case of FRODOCK. Similarly, we performed re-docking and evaluated the performance of the top and best docking pose of each method. We achieved the best performance in case of ZDOCK with average L-RMSD 8.60\u00a0\u00c5 and 2.88\u00a0\u00c5 for the top and best docking pose respectively. Methods were also evaluated on 40 protein-peptide complexes used in the previous benchmarking study, where peptide have length up to 5 residues. In case of best docking pose, we achieved the highest average L-RMSD of 4.45\u00a0\u00c5 and 2.09\u00a0\u00c5 for the blind docking using FRODOCK and re-docking using AutoDock Vina respectively.The study shows that FRODOCK performed best in case of blind docking and ZDOCK in case of re-docking. There is a need to improve the ranking of docking pose generated by different methods, as the present ranking scheme is not satisfactory. To facilitate the scientific community for calculating CAPRI parameters between native and docked complexes, we developed a web-based service named PPDbench (http://webs.iiitd.edu.in/raghava/ppdbench/).Protein-peptide interactions are essential in various biological processes involving signaling, cellular localization, immune system, and apoptotic pathways. Such interactions serve as structural components in approximately 40% of all macromolecular interactions [1, 2]. Peptides can be used to prevent diseases involving malfunctioning of proteins due to undesirable protein-protein interactions [3, 4]. Many databases and algorithms have been developed in the past specifically in the field of peptide-based therapeutics [5\u201315]. There are more than 200 therapeutics peptides, approved by FDA for the treatment of various diseases [16, 17]. Peptides are more flexible than proteins and tend to adopt numerous conformations. Thus, modeling protein-peptide interactions is a challenging and time-consuming task [18].Numerous docking methods have been developed in the past for structural determination of protein-peptide complexes. Broadly, these methods can be classified into the following 3 categories; i) protein-peptide docking, ii) protein-protein docking and iii) protein-small molecule docking. Protein-peptide docking methods [19\u201328] have been specifically developed to dock peptide on protein like pepATTRACT, FlexPepDock, HADDOCK, PEP-SiteFinder, etc. Though protein-protein docking methods [28\u201339] have been developed for docking two proteins; some of these methods, for example, ZDOCK, Hex can also be used to dock peptide on a protein. Similarly, some of the software developed for docking small-molecules on a protein [40\u201348] can also be used to dock peptide on a protein, for example, AutoDock and AutoDock Vina. In summary, a wide range of docking methods have been developed in past that can be used directly or indirectly for docking peptide on a protein.Prediction of peptide interaction with receptor protein is highly desirable to design peptide-based therapeutics. However, utility of any prediction is entirely dependent on the accuracy of the prediction. All the above docking methods can be used to predict the interaction between protein and peptides, thus evaluating the performance of these methods is essential to understand their pros and cons. Also, benchmarking is required to develop highly accurate docking methods that can overcome the limitation of the existing methods. There are a number of benchmarking studies on protein-protein [49, 50], protein-ligand [51, 52] and nucleic acid-ligand [53] docking interactions. Comparatively, limited attempts had been made to benchmark docking method on protein-peptide complexes. Spoel and coworkers evaluated the capability of AutoDock for docking studies of a set of 8 protein-peptide complexes without having the prior knowledge of the binding site [54]. Rentzsch and Renard assessed the performance of AutoDock Vina on a meta-data set of 47 protein-peptide complexes [55]. Recently, Hauser and Windshugel developed a LEADS-PEP dataset to evaluate the performance of peptide docking methods [56]. The major limitation of existing benchmarking studies is that they evaluated only a limited number of docking methods, as well as dataset used for evaluation contain small number of protein-peptide complexes. In addition, there is no platform or web server where users can benchmark or evaluate the performance of their newly developed docking method.Docking method requires structural coordinates of\u00a0the protein and peptide for docking peptide on the protein. As we are providing structure coordinates of both peptide and protein from the original complex, it means we are giving actual docking pose to the\u00a0docking software. This docking pose information may affect the performance of a docking method. In order to avoid biasness in the evaluation, we shifted Cartesian coordinate of the structure without changing the structure of the peptide, i.e. dihedral angles of both original and modified (shifted Cartesian coordinates) peptide remains the same (see Material and Methods). We compute the backbone RMSD (B-RMSD) between the actual and modified structure to verify that shifting of coordinates does not affect peptide structure too much. It was observed that B-RMSD value ranges from 0.067 to 0.827 for 133 peptides with 123 peptides showing value \u22640.5\u00a0\u00c5. In order to understand the shifting of peptide structure from the original position, we compute the distance between actual and modified peptide. As shown in Additional file 1: S2, the modified peptide shifted/moved drastically from its original position. It means modified peptide does not maintain original docking position so it will not affect the performance of docking methods in blind docking.Ideally top pose assigned by a docking method should have the best performance, but in reality, it is not always correct. Thus, the docking method faces two major challenges, (i) how to generate the best docking poses and (ii) ranking these docking poses based on their performance. In order to rank docking poses, the different method uses different scoring functions. Thus, the scoring function plays a crucial role in ranking docking poses particularly in the identification of top/best pose out of many poses generated by a method. We tested the scoring function of all the 6 docking methods on the PPDbench dataset using blind docking. We analysed top 3, 5, 10 and 20 poses generated by the different software and computed performance of best poses (Table 2). The sequential improvement was observed with the increase of the number of selected poses in all the docking methods. However, Hex, ZDOCK, PatchDock and ATTRACT docking methods show a higher deviation in the results as clear from Table 2. The scoring function of FRODOCK seems better for docking studies of peptides compared to other docking methods. Percent of success rate in reproducing the docked poses within 2.0\u00a0\u00c5\u00a0L-RMSD with the original pose is also presented in Table 2. ZDOCK show the success rate of 32.33% on considering the top pose whereas FRODOCK shows a success rate of 39.09% Docking method pepATTRACT performed worst among all the docking methods. It takes the sequence of the peptide instead structure of the peptide. The method itself predicts the structure of the peptide from its sequence, and it is possible that predicted structure of the peptide is not correct.In order to understand the limits of blind docking, for each complex we analyse top 20 docking poses generated by different methods. We identify best docking pose for a given peptide-complex generated by any method and compute the performance of best pose. This process is repeated for all complexes in PPDbench dataset, and average performance is computed. We achieved an average performance of 92.92% in term of FNAT, which is better than the performance achieved by any individual method. Similarly, we calculated the performance in term of L-RMSD and achieved an average L-RMSD value of 1.55\u00a0\u00c5. This analysis shows that the combination of all 6 docking methods can dock almost all the peptides to their proteins with reasonably high accuracy (Additional file 1: S9-S10).In order to facilitate the scientific community, we developed a web server or portal called PPDbench for benchmarking docking methods. This web server consists of following modules: (i) Single: This module is developed to provide a tool to calculate FNAT, I-RMSD and L-RMSD values for a single docked protein-peptide complex. (ii) Batch: This module is designed to calculate the above parameters for more than one complex. The user needs to submit the original and their respective docked poses separately in zip format for batch mode. A complete dataset used in this study with all the receptors, ligands and their respective complexes are available in the web server for downloading. PPDbench web service is freely accessible at http://webs.iiitd.edu.in/raghava/ppdbench/.In this study, we selected 133 protein-peptide complexes to rigorously validate the applicability of 6 widely used docking methods for studying protein-peptide interactions. We generated 20 docked poses for each protein-peptide complex using different docking methods to evaluate their performance. The study shows that in the case of blind docking FRODOCK performed best among all methods whereas, in the case of re-docking, ZDOCK performed best. One of the possible explanation for this result is that ZDOCK is a FFT based docking algorithm and its scoring functions comprise of pairwise shape complementarity with desolvation and electrostatics. When binding site information is not present the method performs a complete search over the protein surface for locating the ligand binding site. However, when the binding site information is known, it restricts the search to the complementarity region and hence probability of\u00a0obtaining near-native pose is much higher. This could be one of the reasons why ZDOCK performs better in re-docking in comparison to blind docking. However, FRODOCK is an initial stage rigid body docking algorithm which optimizes different interactions such as van der Waals interactions, electrostatic interaction and desolvation potentials using a new fast rotational algorithm based on spherical harmonics (SH). In the previous studies, SH has been shown to enhance the docking efficiency [65, 71] and inspired by these studies, FRODOCK implemented the same in their algorithm for enhancing the docking efficiency. This approach increased the searching by accelerating the 3 rotational degrees of freedom. This novel approach could be one of the main reason for the better performance of FRODOCK over the other methods in case of blind docking. Also, the procedural differences (such as the use of search constraints, the definition of interface residues in the evaluation, statistical differences in the number of runs and different sampling sizes) and different approximations probably facilitates FRODOCK to provide better results compared to ZDOCK in blind docking [35]. Therefore, based on our study we proposed FRODOCK for performing blind docking and ZDOCK for re-docking.It was observed that most of the docking method fails to rank their docking pose successfully, as the performance of their best pose is much better than the top pose (Table 2). Thus, there is a need to develop new scoring functions which can rank docking poses with high precision. We combine docking pose generated by different methods for a protein-peptide complex and compute performance of best pose. It has been observed that the performance of best pose obtained from different methods collectively is much better than the performance of pose generated by any individual method. This observation suggests the need for a universal scoring function that can rank pose generated by any method. Our study also suggests the utilization of high-resolution protein-peptide complex structures for benchmarking. In order to facilitate scientific community, we develop a web-based platform that provides benchmarking dataset as well as tools to evaluate the performance of docking poses.We created a dataset of 133 protein-peptide complexes by combining peptiDB dataset and ACCLUSTER dataset. The peptiDB [72] and ACCLUSTER [73] datasets consist of 103 and 251 protein-peptide complexes respectively. We removed all those complexes having more than 1 protein chain or where the length of the peptide was less than 9 or more than 15 residues. We also removed complexes containing any modified residue and complexes corresponding to obsolete PDB entry. After applying the above filters, we were left with 44 protein-peptide complexes from the peptiDB dataset and 115 from ACCLUSTER dataset. We combined both the datasets and selected unique protein-peptide complexes since there were some common PDB IDs in both the datasets. Finally, we were left with unique 133 protein-peptide complexes, and this dataset is referred as PPDbench or main dataset. The detail information of all the selected complexes is given in Table 1. We processed the proteins and peptide chains before docking. All heteroatoms (for example metal ions, water molecules) were removed, if an atom has alternative locations (coordinates) then the only \u2018A\u2019 coordinates were considered, and rest were removed, missing atoms (if any) in the PDB file were modeled and completed using Modeller software [74]. We also calculated redundancy among the 133 proteins using CD-HIT software [75]. The redundancy was calculated at 40% sequence similarity at default parameters (\u2212c\u2009=\u20090.4, \u2212n\u2009=\u20093, and -M\u2009=\u2009400) since it is a well-established standard approach [76, 77].Complexes present in the PPDbench or main dataset contain long peptides with the number of residues between 9 to 15 residues, in contrast to previous studies where small peptides were used. To provide a comprehensive picture, we also evaluated the performance of the above docking methods on protein-peptide complexes used in previous studies. The first dataset is of 47 protein-peptide complexes having peptide length up to 5 residues, used in a study by Rentzsch and Renard. In this study, authors evaluated the performance of AutoDock Vina on a meta-data set of 47 protein-peptide complexes based on existing 11 publications, with peptide length up to 5 residues. Re-docking and Semi-blind docking were performed with the maximum number of poses set to 20, energy range to 10 and exhaustiveness level-up to 1024. At the end of the study, authors concluded that increased sampling made the result more reproducible and improved the primary rigid docking result however no change in the case of flexible docking was observed. Also, there was no correlation present in between the ranked pose and the native pose. The overall performance of AutoDock Vina performance was case dependent and poor for peptides with more than 4 residues [55]. However, in our study, we used only 40 complexes out of 47 which are suitable for docking (reason for excluding 7 complexes is explained in the results section). We referred this dataset as Vina dataset.The other dataset was the LEADS-PEP dataset, used for benchmarking different docking methods in a study by Hauser and Windshugel [56]. The dataset comprises 53 protein-peptide complexes with peptide length 3\u201312. Authors evaluated the performance of four docking methods namely AutoDock, AutoDock Vina, Surflex and GOLD with different scoring functions. The result showed that all the software were able to reproduce conformations of the peptides up to 4 residues. However, performance declined with increasing peptide length. Author also concluded that implementing the scoring function, the performance of the methods improved in identifying near-native pose. Overall the performance of GOLD:ASP in combination with CS rescoring was the method of choice in this study.In order to evaluate blind docking ability of a docking method, one should not provide any information related to peptide binding site. The Cartesian coordinate of peptide structure already has information of binding site since we have taken the peptide from the protein-peptide complex. Thus, it is important to change the Cartesian coordinates of a peptide without changing the structure of the peptides. In this study, we converted the Cartesian coordinates to Internal coordinates and back from Internal to Cartesian coordinates using the following approach. The Cartesian coordinates of 133 peptides were converted to internal coordinates using \u201cdihed.pl\u201d, a perl script of MMTSB toolkit [78]. All dihedral angles of ligand were calculated and using these dihedral angles, the structure of the ligands was reconstructed using \u201ctleap\u201d module of AMBER [79]. In this way, original information of peptide coordinates is lost in the new structure.Re-docking is preferred over blind docking if one knows the binding site of peptide/ligand on protein/receptor [80]. We also performed re-docking to generate docking pose using different methods on the PPDbench dataset of 133 protein-peptide complexes. In order to perform re-docking, we obtained information about all interacting residues, which were in contact with any of the peptide heavy atom within range of 5\u00a0\u00c5 using the script from pdbtools [81]. This binding site information was provided to all docking methods for performing re-docking. In the case of re-docking, we used original peptides instead of a modified structure with shifted coordinates.Detail description of all the 6 docking methods along with the parameters used for running the experiment is given below.The method is based on the randomized search algorithm. It performs a systematic docking based on energy minimization of the protein in the translational and rotational degree of freedom. This docking approach adopts the protocol where each amino acid of a protein is represented by up to three pseudo-atoms. This reduction of protein helps in faster energy minimization and also helps in finding docking energy minima on the protein surfaces. Scoring function used in this program is Lennard-Jones type effective potentials and electrostatics. It is relatively simple and distinguishes between hydrophobic and hydrophilic side chains. Here, residue-residue potential has been used which ranks amount of surface complementarity, hydrophilic or hydrophobic nature of contracting protein regions during docking experiment. This docking approach was built to provide fast docking method that can account for side-chain conformational flexibility. The program is written in Python and C++ and is part of the object-oriented PTools library. This library consists of several routines to manipulate the structure of proteins, to prepare and perform docking and to analyze the results obtained after docking.For blind docking, we uploaded the receptor and ligand file onto the \u201cPartners\u201d tab of the server with Generate Harmonic Mode and RMSD calculation option off. No files were loaded in the \u201cSampling\u201d tab. In \u201cEnergy and Interaction\u201d tab, grid-accelerated docking option was checked but not the iATTRACT refinement [82]. The number of poses was set to 20 in \u201cAnalysis\u201d tab and lastly in computation 1 processor core was used. In ATTRACT we can provide grid size but in our case software itself calculated it. After providing this information, we download the ready-to-use script. The script was further run on the local machinery. ATTRACT performs 1000 minimization steps on each starting structure by default since the grid option was provided. No provision of re-docking is available in the ATTRACT, and hence re-docking study was not performed.Hex 8.0.0 is another popular and widely used method for protein-protein docking. This program uses Spherical Polar Fourier (SPF) correlations rather than Fast Fourier Transform (FFT) based search. In SPF search, 5 rotational and 1 translational degree of freedom is present which reduces the execution time to few minutes whereas in FFT based search 3 rotational and 3 translational degrees of freedom is there. This SPF algorithm of Hex has been validated in CAPRI blind docking experiment [34]. Hex uses the strategy of densely sampling the search space and then cluster the solutions showing similar orientation. In its scoring scheme, Hex calculates shape complementarity excluded volume with an optimal in vacuo electrostatic contribution. The Hex docking algorithm has also been implemented in the form of a web server known as HexServer. This server takes PDB files as an input and provides high-quality docking predictions for further refinement. In recep.mac file (which is a macros file) we set the docking receptor samples to 492, docking ligand sample to 492, docking alpha samples to 128, receptor range angle to 30, Ligand range angle to 30, twist range angle to 30, R12 range as 31, R12 step to 0.75, grid size to 0.6, docking main scan to 16 and docking main search to 25. These values are macros value, and we have taken it from the Hex manual pdf. These values were common for both blind and re-docking. The only difference was that in blind docking we used \u2018nopos\u2019 option in the script file, whereas during re-docking we used \u2018pos\u2019 option because this option position ligand near receptor during docking.PatchDock 1.0 is a molecular docking method, which is based on shape complementarity theory. The algorithm of PatchDock is inspired by the technique of object recognition and image segmentation. Surfaces of the two given molecules are divided into different patches on the basis of their shape. These patches are matched with the corresponding generated patterns. Once the identification of patches is completed, they are mapped using the shape-matching algorithm. The patches identified retained the \u201chot spot\u201d residues. For surface-patch mapping, PatchDock implies hybrid of the Geometric Hashing and Pose-Clustering machine algorithms. Complexes are ranked according to their geometric shape complementarity score. PatchDock algorithm is available as a web server for docking. Here in this study, option \u201cdrug\u201d was given as complex type during blind docking with the default clustering RMSD parameter of 4\u00a0\u00c5. The \u2018drug\u2019 option is given when docking is performed for the small molecules like peptide, drug, etc. However, in the case of re-docking, we provided additional information about residues involved in receptor active site which was used during docking.ZDOCK3.0.2 is one of the widely used protein-protein docking method developed at Weng lab and is based on rigid-body Fast Fourier Transform docking algorithm. The scoring function of ZDOCK is a combination of pair-wise shape complementarity (PSC) with desolvation (DE) and electrostatics (ELEC) where desolvation is the main component of the ZDOCK\u2019s competitive function. The algorithm performs the global search over the protein rotational and translational space in the absence of binding site information. Chen and Weng benchmarked these scoring functions individually and in combination and showed that the overall combination of PSC\u2009+\u2009DE\u2009+\u2009ELEC performed best and is responsible for the better performance of ZDOCK. ZDOCK web server has also been developed for the docking purpose to help researchers.In case of blind docking, receptor and ligand file were processed using \u201cmark_sur\u201d and \u201cuniCHARMM\u201d binary files as mentioned in the README file of the software. ZDOCK generate the grid size according to the size of the protein. The default spacing between the grid cells was constant of 1.2\u00a0\u00c5, and by default, the receptor was fixed during docking with the initial rotation of the ligand with Euler angle 0. However, during the re-docking study, we blocked those residues which were not present in the receptor active site using perl script \u2018block.pl\u2019 given in the software along with the above-mentioned default parameters.pepATTRACT is a recently developed docking approach specifically for docking peptide and protein. This program is a part of ATTRACT and is quite flexible in nature. pepATTRACT takes the sequence of the peptide as an input and generates its own peptide models. pepATTRACT performs rapid coarse-grained ab-initio global docking onto protein surfaces. This docking method selects top models on the basis of ATTRACT ranking score and performs atomistic refinement using iATTRACT [82]. After the refinement, top 1000 models are selected and refined using molecular dynamics simulations with AMBER14. Finally, models are clustered using the fraction of common residue contacts and are ranked on the basis of the average energy value of the top 4 ranking members.In case of blind docking, receptor and ligand file were uploaded onto the \u201cPartners\u201d tab of the server with RMSD calculation option off. In the case of pepATTRACT, no \u201cSampling\u201d tab is there. In \u201cEnergy and Interaction\u201d tab, grid-accelerated docking and iATTRACT refinement options were checked. Number of poses were set to 20 in \u201cAnalysis\u201d tab and lastly in computation 1 processor core was used. It performs 1000 minimization steps on each starting structure by default since grid option was provided. After providing this information, we download the ready-to-use script with the above-mentioned information. The script was further run on the local machinery. In case of re-docking, in addition to the above-mentioned parameters, we provide the list of active residues directly involved in the interaction in the Protein section of Partner tab.FRODOCK is one of the popular and widely used servers for protein-protein docking. It was ranked 4th among the 18 docking/scoring function tested. Earlier version of FRODOCK was based on the principle of 3D grid-based potentials with spherical harmonics (SH) properties. However, recently developed version FRODOCK 2.0 includes an extra knowledge-based potential, which helps in improving docking success rate more significantly. It combines 3 binding energy van der Waals, desolvation, and electrostatics interaction and optimizes it by using new fast rotational docking algorithm based on spherical harmonics which is coupled with systematic translational search. The scoring function was improved by adding complementary coarse-grained knowledge-based protein-protein docking potential [83]. FRODOCK allows the user to predict protein-protein complexes using unbound components in a few minutes. Scoring function in the server has been optimized for 3 different types of interactions, i.e. enzyme-substrate, antigen-antibody and others.Web server was used in the case of FRODOCK for performing docking studies. Files were uploaded onto the server with the type of interaction \u201cUnknown\u201d which is the default case. Web server methodology section tells that in case of FRODOCK electrostatic contribution is calculated in the range of \u00b110 Kcal/mol e. buried surface area of receptor and ligand is calculated using a generic probe of radius 1.7\u00a0\u00c5 at the grid points near to the corresponding surface. In the case of FRODOCK, there is no provision of re-docking. Therefore, only blind docking study was performed using this method.In order to measure the performance of docking poses generated by the different method, we used parameters namely FNAT, I-RMSD, and L-RMSD adopted in worldwide competition CAPRI (Critical Assessment of PRedicted Interactions). Mendez et al. proposed \u201cA pair of residues on different sides of the interface was considered to be in contact if any of their atoms were within 5\u00c5\u201d [84, 85]. FNAT is the fraction of correct (native) residue-residue contact in predicted complex divided by residue\u2013residue contacts in the original complex. We computed L-RMSD and I-RMSD for measuring the overall geometric fit between the original complex and predicted complex tertiary structure. L-RMSD is the backbone root-mean-square deviation of the ligands in the original and predicted complexes based on superpositioning of backbone atoms. I-RMSD is the root-mean-square deviation of the backbone atoms of the interface (contact) residues in the original and predicted complexes. In our study, Ptools [86] (an open source molecular docking library) is used for calculating FNAT and I-RMSD values and PyMol for calculating L-RMSD values.We divided our dataset into two categories based on the resolution of the structure. In the first group, we put all those structures whose resolution was in between 1 and 2\u00a0\u00c5 (89 complexes out of 133 complexes), and the other group consists of structures whose resolution was in between 2 and 3\u00a0\u00c5 (44 complexes out of 133 complexes). We evaluated the performance of docking methods each group of complexes. We calculated the number of rotatable bonds present in the peptides using PADEL software [87]. All 133 complexes were divided into three groups based on the number of rotatable bonds present in the peptides (i) peptides having a number of rotatable bonds in between 0 and 40 (ii) peptides having a number of rotatable bonds in between 41 and 60 and (iii) peptides having a number of rotatable bonds above 60.Similarly, we group complexes based secondary structure of peptides. We assign secondary structure of all peptides in complexes using DSSP software [88, 89]. We made two groups of protein-peptide complexes (i) regular secondary structure and (ii) coil. Regular secondary structure was assigned to that complex whose sum of helix and sheet content is 60% or more. Rest of the complexes were assigned to coil category. We also analysed the class of 133 complexes present in PPDbench dataset to observe the class preference of different docking methods. For this, we considered blind docking result obtained for the top pose. Method which showed the lowest L-RMSD value for the complex was considered best for that particular complex.Critical Assessment of PRedicted InteractionsInterface RMSDLigand RMSDRoot Mean Square DeviationSpherical HarmonicsPublications cost were funded by J.C. Bose National Fellowship, Department of Science and Technology (DST), Government of India.Dataset is freely available and can be downloaded from the webserver.This article has been published as part of BMC Bioinformatics Volume 19 Supplement 13, 2018: 17th International Conference on Bioinformatics (InCoB 2018): bioinformatics. The full contents of the supplement are available online at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-19-supplement-13.This study does not require any ethical clearance or any consent to participate.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.In order to facilitate scientific community and to complement previous benchmarking studies, we made a systematic attempt to benchmark docking methods on a large set of protein-peptide complexes. The main aim of this study is to evaluate the performance of major docking methods, as well as evaluation of scoring function used by docking methods. We also perform a wide range of analysis to understand the impact of the absence of the binding site information, type of secondary structure, and other molecular properties on the performance of docking method. It is not practically possible to evaluate all docking methods. Thus we select those methods, which broadly satisfy following conditions; i) available free for public use, ii) standalone version is available, iii) widely used by scientific community and iv) performed well in the Critical Assessment of PRediction of Interactions (CAPRI) competition. CAPRI is an international competition which provides a framework for evaluating protein-protein interactions/docking and refinement methods by blind testing on the set of unpublished targets [30, 36, 57\u201361]. Finally we select following 6 software for benchmarking; ZDOCK 3.0.2, FRODOCK 2.0, Hex 8.0.0, PatchDock 1.0, ATTRACT and pepATTRACT. ZDOCK 3.0.2 is a rigid body docking method based on the Fast Fourier Transform algorithm, and its scoring function is a combination of pairwise shape complementarity, desolvation and electrostatic energy [62\u201364]. ATTRACT is a flexible protein-protein docking method based on randomized search algorithm and employs Lennard-Jones potential and electrostatic energy as a scoring function [30, 39]. Hex 8.0.0 is another popular method which uses Spherical Polar Fourier (SPF) correlations algorithm rather than Fast Fourier Transform (FFT) based search [65, 66]. FRODOCK 2.0 is a rigid body docking algorithm and is based on the principle of 3D grid-based potentials with knowledge-based potential and spherical harmonics (SH) properties which help in improving docking success rate more significantly [35, 36]. pepATTRACT is a flexible protein-peptide docking algorithm which performs a rapid coarse-grained global search on the protein surface and model peptide simultaneously during docking [67]. PatchDock 1.0 is also a rigid body docking software which considers surface variability or flexibility implicitly marked through liberal intermolecular penetration. Its scoring function is based on geometry fit and atomic desolvation energy [47]. The standalone versions of ZDOCK 3.0.2, Hex 8.0.0 and PatchDock 1.0 docking methods are available for use on the local machine. FRODOCK 2.0 provides an online web service where a user can upload their docking partner for docking. ATTRACT, and pepATTRACT provides a ready-to-use script, which can be downloaded from their web-based service for performing docking of each complex. The main dataset created and used in this study comprises of highly annotated 133 protein-peptide complexes; Table 1 shows detailed information about these complexes. Also, methods were evaluated on datasets used in previous studies.Table 1\nThe list of all the considered complexes along with their PDB-IDs\nIn this study, a dataset called PPDbench has been used to evaluate the performance of 6 docking methods. This dataset contains 133 non-redundant complexes of protein-peptides at 40%, it means no two proteins have more than 40% sequence similarity. We removed redundancy using commonly used software CD-HIT; detail procedure is given in the Material and Method section. In total, 117 clusters were obtained out of which similarity among the proteins was present only in 12 clusters. Detail information of the clustering result is provided in Additional\u00a0file\u00a01: S1. We used CAPRI parameters (e.g., FNAT, I-RMSD, and L-RMSD) and the following 3 steps for evaluating the performance of the methods. In the first step, the structure of a protein-peptide complex is obtained from PPDbench. In the second step, docking methods use structures of protein and peptide to predict the structure of the protein-peptide complex. Finally, the performance of docking methods is determined by comparing the predicted and actual structure of a protein-peptide complex. Overview of the PPDbench algorithm has been shown in Figure\u00a01.\n\nFig. 1\nSchematic representation of the workflow of PPDbench algorithm\nBlind docking of two structures is one of the major challenges in the field of docking. We used default parameters for blind docking and generated 20 docking poses for each complex. In order to compute the performance of a method on a protein-peptide complex, we compared its top 20 poses one by one with original docking pose. The average performance of different methods on the PPDbench dataset is shown in Table 2. The pose which is ranked first by the respective scoring function of method is termed as \u201cTop pose\u201d while the pose for which we obtain the lowest L-RMSD value among all the generated poses is termed as \u201cBest pose\u201d. In the case of top docking pose, FRODOCK performs better than any other method and achieve average L-RMSD of 12.46\u00a0\u00c5. It was noted that the performance of different methods in term of I-RMSD and FNAT also follow the same trend. The performance of different methods on individual complexes is shown in Additional file 1: S3-S5. The performance of best docking poses out of top 20 poses generated by various methods, also shows the similar trend; FRODOCK with average L-RMSD of 3.72\u00a0\u00c5 performed better than other methods.Table 2\nThe performance of best docking pose generated by different docking methods using blind docking on the PPDbench dataset\n\naNumber indicate number of top docking poses generated by method\nRe-docking is preferred over blind docking if one knows the binding site of peptide/ligand on protein/receptor. Binding site information reduces searching space drastically; thus, re-docking is fast and more precise. In order to evaluate re-docking ability of methods, we perform re-docking on the PPDbench dataset. In this study, we used default parameters for re-docking using instructions provided by different docking method (See Material and Methods Section for detail). It is important to note that we used the original structure of the peptide in the case of re-docking instead of the modified structure since we are already providing information of binding site in the case of re-docking. We were unable to perform the re-docking experiment using FRODOCK, as there is no provision for re-docking in this software. Similarly, ATTRACT also didn\u2019t perform re-docking, as the server does not take the input of active residue information. Thus, we performed re-docking only using 4 methods (ZDOCK, Hex, PatchDock, and pepATTRACT) and generated top 20 docking poses for each complex and compute the performance of the methods. We observed that the performance of all the docking methods improved during the re-docking study (Table 3). ZDOCK performed better than other docking methods for the top pose as well as for the best pose; followed by Hex. In the case of ZDOCK, L-RMSD improved from 15.85\u00a0\u00c5 to 8.60\u00a0\u00c5 for top docking pose and from 7.53\u00a0\u00c5 to 2.88\u00a0\u00c5 for the best pose. FNAT, L-RMSD and I-RMD values for all the individual complexes are given separately in Additional file 1: S6-S8.Table 3\nThe performance best docking pose generated by different docking methods using re-docking on the PPDbench dataset\n\naNumber indicate number of top docking poses generated by method\nFigure\u00a02(a-b) shows the deviation in the success rate with the increase in the L-RMSD values. The success rate is calculated as the percentile of the success in obtaining the docked poses within the specified L-RMSD values. In our study, we observed that the success rate is not much affected by a slight increase in the L-RMSD for most of the considered docking methods. Considering the top 20 solutions, FRODOCK reproduced around 82.00% complexes within 4.0\u00a0\u00c5\u00a0L-RMSD. Thus, the performance of FRODOCK is much better as compared to other 5 docking methods. Figure 2(a-b) also shows the complete failure of docking of all the docking methods (more than 30\u00a0\u00c5\u00a0L-RMSD) for some of the complexes. It is clear from Additional file 1 S11, deviation in the success rate with the increase in the I-RMSD values follow the same trend as the deviation in the success rate with the increase in L-RSMD values. Any successful scoring function should either produce the top score pose as the best pose or should show the minimum deviation between these two poses. In order to understand the difference between top pose and best pose generated by each method, we calculate the percent of complexes having L-RMSD (difference in best and top pose) in a different range (Table 4). In most of the cases (~\u200956.00%), top and best pose generated by FRODOCK have no difference with L-RMSD 0.00. As shown in Table 4, the scoring function of FRODOCK, ATTRACT and ZDOCK are better than other methods.\n\nFig. 2\nL-RMSD values obtained for all the docking methods on the PPDbench dataset for the (a) Top poses and (b) Best pose\nTable 4\nPercentile of success rate where \u2018Top pose\u2019 and \u2018Best pose\u2019 are same or within the range of some specified differences\nIdeally, docking pose generated by a method and ranking of docking poses should be same every time for a given protein-peptide complex. In order to check reproducibility, we generate docking pose using blind docking for each complex two times called \u201cFirst docking\u201d and \u201cSecond Docking\u201d for each docking method. Then we compute the performance of each method for \u201cFirst Docking\u201d and \u201cSecond Docking\u201d as well as the difference in the performance. As shown in Table 5, for most of the methods, the difference was either zero or negligible. It means the results of docking methods are reproducible (Table 5).Table 5\nThe performance of best docking pose generated by different methods and difference in performance of docking poses generated in two events on the PPDbench dataset\n\naNumber indicate number of top docking poses generated by method\nThe quality of structure of a complex depends on the resolution of the crystal structure. The quality of benchmarking depends on the quality of the complex structure as it is used as a standard of truth or reference for measuring quality docking pose. Thus, we divide protein-peptide complexes into two groups having the resolution between 1 and 2\u00a0\u00c5 and between 2 and 3\u00a0\u00c5. Then we compute performance in term of L-RMSD of all methods using blind docking for above two group of complexes. In case of top pose, the performance of all method except FRODOCK improves for complexes having resolution 1\u20132\u00a0\u00c5 (Figure\u00a03(a)). In case of best pose, the performance of all methods except Hex improves for complexes having resolution 1\u20132\u00a0\u00c5 than complexes having resolution 2\u20133\u00a0\u00c5 (Figure 3(b)). This is expected as the quality of evaluation also depend on the resolution of complex structures. Average value of FNAT and I-RMSD was also analysed for the top pose (Additional\u00a0file\u00a02:(a-b)) and best pose (Additional\u00a0file\u00a03:(a-b)).\n\nFig. 3\nThe performance of different docking methods on the PPDbench dataset with resolution 1\u20132\u00a0\u00c5 and 2\u20133\u00a0\u00c5 for (a) Top pose and (b) Best pose based on average L-RMSD value\nWe group protein-peptide complexes based on rotatable bonds in the peptide to understand the effect of rotatable bonds on quality of docking pose generated by different methods (Tables 6, 7, 8). Considering average L-RMSD, we found the different behaviour of software in docking results with respect to the number of rotatable bonds. When the\u00a0top pose was considered, FRODOCK and ATTRACT were found to perform better for complexes either having a smaller number of rotatable bonds or larger bonds while other software showed a decline in the performance with the increasing number of rotatable bonds. In case of best pose, the situation was a little different. ZDOCK and FRODOCK were found to perform better than other software (Tables 6, 7, 8).Table 6\nThe performance of best docking poses having different number of rotatable bonds in term of FNAT; poses generated by different methods on the PPDbench dataset\n\naNumber indicate number of top docking poses generated by method\nTable 7\nThe performance of best docking poses having different number of rotatable bonds in term of L-RMSD; poses generated by different methods on the PPDbench dataset\n\naNumber indicate number of top docking poses generated by method\nTable 8\nThe performance of best docking poses having different number of rotatable bonds in term of I-RMSD; poses generated by different methods on the PPDbench dataset\n\naNumber indicate number of top docking poses generated by method\nWe divide protein-peptide complexes into two groups based on the secondary structure of the\u00a0peptides. The first group contains peptides dominated by regular secondary structure (e.g., Helix, Sheet). The second group is dominated by the peptide structure comprising coils. We compute performance of\u00a0the docking methods using blind docking on both groups of complexes separately. The performance in term of L-RMSD for two group is shown in Figure\u00a04. In case of top pose, all methods except Hex performed better on protein-peptide complex dominated by the coil in comparison (Figure 4(a)). A similar trend was observed for best docking pose, where most of the method perform better for peptides dominated by the coils (Figure 4(b)). Average value of FNAT and I-RMSD was also analysed for the top pose (Additional\u00a0file\u00a04(a-b)) and best pose (Additional\u00a0file\u00a05(a-b)). This analysis indicates that docking peptides having regular secondary structure is more difficult than peptides having no regular secondary structure. One of the possible explanation for this result could be the flexible nature and high degree of freedom of coiled peptides in comparison to helical peptides which is more rigid and possess a lesser degree of freedom. Coiled peptide might adapt better conformational change during docking to find the near-native pose. Also, in previous studies, it has shown that the formation of coiled-coil in peptide help in better docking with the receptor molecule [68, 69]. Similarly, in another study, it was shown that the docking method performed better on coiled peptides [70].\n\nFig. 4\nThe performance of different docking methods on the PPDbench dataset with different secondary structure for (a) Top pose and (b) Best pose based on average L-RMSD value\nWhen we analysed the classification of the PPDbench dataset (Table 1), we found that most of the complexes belong to the enzymatic class, for example, hydrolase, ligase, oxidoreductase, transferase (around 27%), transcription (~\u200917%) or to signaling proteins (~\u200912%). Rest of the complexes were present in other classes like structural proteins, membrane proteins, binding proteins, etc. Overall, a good amount of diversity was present in the dataset. Based on the blind docking result and considering the top pose with lowest L-RMSD value, we analysed the group of complexes preferred by each software (Table 9). FRODOCK performed best on 42 complexes out of 133 which belongs majorly to Transcription and Signaling protein class. ATTRACT achieved 2nd position by performing best on 35 complexes where most of them belong to the Enzymatic class, Immune system class or Signaling protein class. ZDOCK performed best on 30 complexes, and during analysis, we didn\u2019t find any specific class which is preferred over another. Likewise, PatchDock performed best on 15 complexes in total, and the class diversity was a mix. However, it didn\u2019t cover complexes belonging to Signaling proteins or Structural proteins class. Hex showed lowest L-RMSD only for 8 complexes, belonging to class Signaling proteins or Transcription and lastly, pepATTRACT was found best only on 3 complexes.Table 9\nClustering result of docking method showing the PDB IDs for which it performed best during blind docking when top pose was considered\n\naNumber of complexes for which software performed best\nRentzsch and Renard evaluated the performance of AutoDock Vina on a dataset of 47 protein-peptide complexes, where the length of peptide varies from 2 to 5. Rentzsch and Renard also generated 20 docking poses and compute performance of top pose as well as the performance of best docking pose. We also evaluated our methods on this dataset; unfortunately, few methods fail on certain PDB IDs, or there were certain issues due to which we didn\u2019t involve them in our study. For example, pepATTRACT failed on PDB files\u00a01PAU, 8GCH, 1JQ9 and 5SGA, ATTRACT failed on IDs 1PAU and 1BE9, FRODOCK failed on 1PAU and 5SGA. After analysing these files, we found that one potential reason for the failure of different docking methods is the presence of non-natural residues (example ACE, ACY) in the peptide of these complexes. Likewise, we didn\u2019t get any contacts for the IDs 1BHX and 3TPI as per our criteria (mentioned in Material and Methods) during re-docking studies. Therefore, we excluded these 7 IDs and proceeded with 40 peptide-protein complexes instead of 47 for benchmarking methods used in this study. We referred this dataset (40 protein-peptide complex structures) as Vina dataset. We performed blind docking using 7 docking methods including AutoDock Vina and evaluate the performance of all methods (Table 10). In the case of top docking pose, PatchDock got the average L-RMSD value of 12.21\u00a0\u00c5; performing better than other methods. Similarly, FRODOCK achieved an average L-RMSD value of 4.45\u00a0\u00c5 in case of best docking pose; performing better than other methods. We also evaluate the re-docking ability of four docking methods only as some of the methods do not have provision for re-docking. In the case of re-docking, we have not evaluated the re-docking ability of AutoDock Vina instead we have taken performance reported by authors for these complexes. As shown in Table 10, AutoDock Vina performed better than other methods. The performance of different docking methods in detail is described in Additional file 1: S12-S13.Table 10\nThe performance of different docking methods on Vina dataset (40 protein-peptide complexes) in terms of average L-RMSD\n\naNumber indicate number of top docking poses generated by method; #NA re-docking provision is not available\nRecently, Hauser and Windshugel used LEADS-PEP dataset for benchmarked re-docking ability of four different protein-ligand docking software AutoDock, AutoDock Vina, Surflex, and GOLD. We compare three datasets (LEADS-PEP dataset, with our dataset of 133 complexes and 40 complexes of dataset created by Rentzsch and Renard) and found 10 common protein-peptide complexes (8 complexes from the PPDbench dataset and 2 complexes from Vina dataset). On these 10 common complexes, we evaluate the performance of top and best docking pose generated by docking methods used in our study. We have taken the performance of other methods as reported by their authors (Table 11). Z-DOCK achieved an average L-RMSD value of 4.87\u00a0\u00c5 and 2.14\u00a0\u00c5 for the top and best docking pose respectively, which is better than other methods.Table 11\nThe performance of different docking methods on common 10-protein-peptide complexes, in term of their re-docking ability\n\naASP, CP, CS and GS are different scoring functions\n\n\nAdditional file 1:\nS1. Sequence similarity at 40% between the proteins of PPDbench dataset using CD-HIT software. S2. Distance between the original peptides and the peptides with changed coordinates. S3 (a-f). FNAT values of 133 complexes obtained for all 6 docking methods by blind docking. S4 (a-f). L-RMSD values of 133 complexes obtained for all 6 docking methods by blind docking. S5 (a-f). I-RMSD values of 133 complexes obtained for all 6 docking methods by blind docking. S6 (a-d). FNAT values of 133 complexes obtained for 4 docking methods by re-docking. S7 (a-d). L-RMSD values of 133 complexes obtained for 4 docking methods by re-docking. S8 (a-d). I-RMSD values of 133 complexes obtained for 4 docking methods by re-docking. S9 (a-c). FNAT, L-RMSD and I-RMSD values for the top poses of all the considered docking methods obtained by blind docking. S10 (a-c). FNAT, L-RMSD and I-RMSD values for the best poses of all the considered docking methods obtained by blind docking. S11. Deviation in the success rate with the increase in the I-RMSD values obtained by blind docking. S12. (a-g). L-RMSD values of 40 complexes obtained for 7 docking methods by blind docking. S13 (a-d). L-RMSD values of 40 complexes obtained for docking methods which perform re-docking. (DOCX 1777\u00a0kb)\n\n\n\nAdditional file 2:\nPerformance of different docking method on the PPDbench dataset with resolution 1\u20132\u00a0\u00c5 and 2\u20133\u00a0\u00c5 for top pose based on average (a) FNAT value and (b) I-RMSD respectively. (JPG 349\u00a0kb)\n\n\n\nAdditional file 3:\nPerformance of different docking method on the PPDbench dataset with resolution 1\u20132\u00a0\u00c5 and 2\u20133\u00a0\u00c5 for best pose based on average (a) FNAT value and (b) I-RMSD respectively. (JPG 352\u00a0kb)\n\n\n\nAdditional file 4:\nPerformance of different docking methods on the PPDbench dataset with the different secondary structure for top pose based on (a) FNAT and (b) I-RMSD value respectively. (JPG 362\u00a0kb)\n\n\n\nAdditional file 5:\nPerformance of different docking methods on the PPDbench dataset with the different secondary structure for best pose based on (a) FNAT and (b) I-RMSD value respectively. (JPG 274\u00a0kb)", "s12859-019-2603-1": "Tandem repeat sequences are common in the genomes of many organisms and are known to cause important phenomena such as gene silencing and rapid morphological changes. Due to the presence of multiple copies of the same pattern in tandem repeats and their high variability, they contain a wealth of information about the mutations that have led to their formation. The ability to extract this information can enhance our understanding of evolutionary mechanisms.We present a stochastic model for the formation of tandem repeats via tandem duplication and substitution mutations. Based on the analysis of this model, we develop a method for estimating the relative mutation rates of duplications and substitutions, as well as the total number of mutations, in the history of a tandem repeat sequence. We validate our estimation method via Monte Carlo simulation and show that it outperforms the state-of-the-art algorithm for discovering the duplication history. We also apply our method to tandem repeat sequences in the human genome, where it demonstrates the different behaviors of micro- and mini-satellites and can be used to compare mutation rates across chromosomes. It is observed that chromosomes that exhibit the highest mutation activity in tandem repeat regions are the same as those thought to have the highest overall mutation rates. However, unlike previous works that rely on comparing human and chimpanzee genomes to measure mutation rates, the proposed method allows us to find chromosomes with the highest mutation activity based on a single genome, in essence by comparing (approximate) copies of the pattern in tandem repeats.The prevalence of tandem repeats in most organisms and the efficiency of the proposed method enable studying various aspects of the formation of tandem repeats and the surrounding sequences in a wide range of settings.The implementation of the estimation method is available at http://ips.lab.virginia.edu/smtr.Tandem repeats, which form about 3% of the human genome [1], are segments of DNA that primarily consist of repeats of a certain pattern. The number of copies in tandem repeats is highly variable and is prone to change due to tandem duplication mutations. Furthermore, tandem repeats are subject to point mutations [2]. The variability of tandem repeats enables them to be used for population genetics [3] and forensics [4]. Tandem repeats may cause expansion diseases, gene silencing [5], and rapid morphological variation [6].A mechanisms suggested for the formation of tandem repeat sequences, especially those of shorter lengths, is slipped-strand mispairing [7], also known as replication slippage [8]. This mechanism refers to the misalignment of the template and the nascent strand during DNA replication. It is thought that the presence of near-identical sequences increases the probability of misalignment [7].In this work, we present and analyze a model of the evolution of tandem repeat sequences via tandem duplication and substitution mutations. The starting point is a short sequence which we refer to as the seed. At each mutation step, either a tandem duplication or a substitution mutation occurs, each with a given probability. Here, a tandem duplication refers to a type of duplication in which a newly created copy of a segment of the sequence (the template) is inserted into the same sequence immediately after the template. Thus, the model is appropriate for studying slippage-driven repeats but not designed to represent repeats resulting from other processes, such as recombination [9]. The length of the seed, also referred to as pattern length, may be from one to hundreds of nucleotides. However, generally only repeats with short pattern lengths, e.g. 1\u201310 nt, are associated with polymerase slippage [7]. In the model, tandem duplications of different lengths do not necessarily have the same probability. We show analytically that certain statistical features of the sequence converge as the number of mutations increases. This in turn allows us to i) predict the behavior of the sequence after a large number of mutations if we have the parameters of the model, or ii) estimate the parameters of the model given the sequence after a large number of mutations. In other words, given a sequence that is the result of the aforementioned process, we can estimate conditional mutation probabilities without any other information or comparison with homologous sequences from other organisms.We study two cases in the evolution of tandem repeats. First, we consider the case in which substitution mutations do not occur and the only type of mutation is tandem duplication. We show that in this case, while the prediction of evolutionary behavior is easy, estimation of model parameters, including the probabilities of tandem duplications of given lengths, is difficult. This is because as the number of mutations increases, the sequence demonstrates periodic behavior, lacking features that can be leveraged for estimation. Perhaps surprisingly, the period of this sequence is not necessarily the most common or the shortest possible tandem duplication length.We then consider the more interesting case in which both tandem duplication and substitution mutations occur. In this case, substitutions disrupt the periodic pattern that would arise from tandem duplications. As a result, after a large number of mutations, the resulting sequence is more complex and informative, allowing us to estimate the model parameters. Specifically, from such a sequence, we can estimate the probability of a substitution in each step, as well as the probabilities of tandem duplications of different lengths. Furthermore, we can estimate the total number of mutations that gave rise to the sequence under study. We apply this method to the tandem repeats in the human genome, which enables us to investigate the prevalence of substitutions in repeats of different lengths and to compare the average number of mutations among chromosomes. We show that two classes of tandem repeats are observed based on their mutation profiles and that this classification is compatible with the mini- and micro-satellite classification based on pattern length. Furthermore, our analysis illustrates that the average number of mutations in some chromosomes are higher than others. Interestingly, this agrees with another measure of mutation activity, i.e., comparison with the chimpanzee genome: The chromosomes with higher mutation counts in repeated regions are the same as the ones that have diverged most from chimpanzee chromosomes.Our results demonstrate that the proposed estimation method can be used to study various aspects of tandem repeat sequences, such as the effects of different factors on mutation rates, at a large scale. Such studies will be helpful for understanding what factors affect the occurrences of diseases that result from tandem repeats, such as repeat expansion diseases [5]. More accurate estimates of the number of mutations will also enable a better characterization of the relationship between cancer and repeat instability [10]. Classification of tandem repeats based on mutation profile is informative for understanding the differences between the underlying mutation mechanisms. Furthermore, such a classification will lead to more accurate choices of distance metrics between sequences with similar mutation profiles, based on how likely each mutation type is. These metrics can then be used to obtain improved phylogenetic trees using tandem repeat sequences.This paper presents an explicit stochastic model for the evolution of tandem repeat sequences. Conventional models of sequence evolution, such as those of Jukes and Cantor [11], Kimura [12], and Felsenstein [13], focus on substitution mutations and are not applicable to more complex mutations such as insertions, deletions, and duplications. While the study of more complex models has proved challenging [14], they have been studied by some authors, including [14\u201316] for deletions and insertions, and [17\u201319] for tandem duplications.In previous work on modeling tandem duplication and substitution mutations, it is often assumed that in each step, the length of the sequence grows by at most one repeat unit, which simplifies the analysis; see, e.g., [18] and references therein. Our model however allows duplications of lengths longer than one repeat unit at a time. Note that models that do not allow longer duplications may underestimate the probability of substitution and overestimate the probability of tandem duplication since more duplication events are needed to account for the observed copy number. Models proposed by [17, 19] include duplications of lengths longer than one repeat unit. But these works only consider perfect tandem repeats, in which all copies are identical. Imperfect tandem repeats, however, are common in genomic data. Furthermore, unlike [17, 18] that use Markov chains and branching processes for modeling, our analysis is based on stochastic approximation, which enables the description of new aspects of the problem. In particular, we see that the observed period in a tandem repeat sequence is not necessarily the most common duplication length (Theorem 1) and that the presence of substitutions allows the estimation of mutation probabilities (9). Finally, these papers are not concerned with recovering the duplication history, which is a focus of the current paper. Stochastic analysis has also been used by [20, 21], to study latent periodicity in genomic sequences. There, the goal is to utilize statistical analysis to improve upon the purely spectral methods of period detection for both genomic and non-genomic data, rather than the estimation of the duplication history.Recovering the duplication history has been studied by [22\u201324], which take a combinatorial approach to solving the problem. Via simulation, we show that the method proposed in this paper outperforms the state-of-the-art method, called DTSCORE [24]. Estimation of the duplication history using a stochastic model, to the best of our knowledge, has not appeared in the literature before.We will first present an overview of our method. Our approach relies on designing a stochastic model for the evolution of tandem repeats in the presence of tandem duplication and substitution mutations. Assuming the parameters of the model (the conditional probabilities of duplication and substitution mutations) are known, we study the asymptotic behavior of tandem repeat sequences. This analysis is based on the autocorrelation function since this feature well represents the (approximate) periodicity that results from duplication and substitution mutations. We determine the limit set of the autocorrelation function as a function of model parameters. We will then address the inverse problem of estimating the parameters given a sequence, assuming that its autocorrelation is close to the limit. This in turn enables us to estimate the counts of mutations of different types in the history of the sequence.In this section, we first present the stochastic model and a general framework for analyzing the evolution of sequences under duplication and substitution mutations using stochastic approximation. Stochastic approximation relates the behavior of a discrete system to an ordinary differential equation (ODE) [25], which is often more tractable. The use of stochastic approximation for the analysis of stochastic mutation models was originally proposed by [26] to study the evolution of the frequencies of k-mers in a simplified model of interspersed duplication. After setting up the model and the preliminaries, we study the behavior of the autocorrelation function in systems with tandem duplication and substitution.Let s be a circular sequence over some alphabet \\(\\mathcal A\\) that \u201cevolves\u201d over time. The process starts with s(0), called the seed, and in each step, s(i) is obtained from s(i\u22121) through a random mutation. The reason that we choose s to be a circular string, and not a linear one, is to avoid the technical difficulties of dealing with its boundaries. If the mutation occurring at time i is a substitution, its position is chosen at random among all symbols of s(i). That symbol is then changed randomly to one of the other symbols of \\(\\mathcal {A}\\). If the mutation is a tandem duplication of length \u2113, a substring of length \u2113 is chosen uniformly at random, duplicated, and inserted in tandem. We use q0 to denote the probability that the mutation in any given step is a substitution and q\u2113, \u2113>0, to denote that it is a tandem duplication of length \u2113. We assume that there exists K such that q\u2113=0 for all \u2113>K. Finally, we let q=(q0,q1,\u2026,qK) where \\(\\sum _{i=0}^{K}q_{i}=1\\). Note that q represents conditional mutation probabilities given that a mutation occurs and not the mutation probabilities per generation. In our notation s(i) is the instance of s at time i. However, if it causes no ambiguity, we may use s instead of s(i). We use Li to denote the length of s(i).For an ordered set U, let \\(\\boldsymbol {R}_{n}=\\left (R_{n}^{u}\\right)_{u\\in U}\\) be a vector representing the number of appearances of objects u\u2208U in the sequence s at time n and let \\(\\boldsymbol {\\rho }_{n}=\\frac {\\boldsymbol {R}_{n}}{L_{n}}\\) be the normalized version of Rn. For example, U can be the set of all strings over \\(\\mathcal {A}\\) with length at most three. Our goal is to find out how \u03c1n changes with n by finding a differential equation whose solution approximates \u03c1n.Define \\(\\mathcal {F}_{n}\\) to be the filtration generated by the random variables {\u03c1n,Ln}. Furthermore, let \\(\\mathbb {E}_{\\ell }\\left [ \\cdot \\right ]\\) denote the expected value conditioned on the fact that the length of the duplicated substring is \u2113 and let \\(\\boldsymbol {\\delta }_{\\ell }=\\mathbb {E}_{\\ell }\\left [\\boldsymbol {R}_{n+1}|\\mathcal {F}_{n}\\right ]-\\boldsymbol {R}_{n}\\). Recall that q0 is the probability of a substitution and qi,0<i\u2264K is the probability of the event that a sequence of length \u2113=i is duplicated.where h\u2113(\u03c1)=\u03b4\u2113(\u03c1)\u2212\u2113\u03c1 and \\(\\boldsymbol {h}(\\boldsymbol {\\rho })={\\sum }_{\\ell =0}^{K}q_{\\ell }\\boldsymbol {h}_{\\ell }(\\boldsymbol {\\rho })\\), and where \\(\\boldsymbol {M}_{n+1}=\\boldsymbol {R}_{n+1}-\\mathbb {E}\\left [\\boldsymbol {R}_{n+1}|\\mathcal {F}_{n}\\right ]\\) is a bounded martingale difference sequence.Let \\(R_{n}^{r}\\) denote the autocorrelation of function after n mutations starting from the seed sequence and let \\(\\rho _{n}^{r}=\\frac {R_{n}^{r}}{L_{n}}\\). To express autocorrelation as a vector, let \\(\\boldsymbol {R}_{n}=\\left (R_{n}^{0},R_{n}^{1},\\dotsc,R_{n}^{m-1}\\right)\\) and \\(\\boldsymbol {\\rho }_{n}=\\frac {\\boldsymbol {R}_{n}}{L_{n}}\\), for a constant m. Note that \\(R_{n}^{0}=L_{n}\\) and \\(\\rho _{n}^{0}=1\\).for 0<r\u2264m\u22121. We thus see that the set of equations governing \u03c1 are linear.It can then be shown that \u03c1n converges almost surely to the null space of A.In the following sections, we consider the null space of A in two cases. First, we assume q0=0, that is, there are no substitutions. Next we study the case with positive probability of substitutions, i.e., q0>0.In this section, we consider the case in which the only type of occurring mutations is tandem duplication. We show that in this case the null space of A is simple.Suppose q0=0. Let P={i:i>0,qi>0} and \\(d=\\gcd P\\). The normalized autocorrelation \\(\\boldsymbol {\\rho }_{n}=\\left (\\rho _{n}^{0},\\dotsc,\\rho _{n}^{m-1}\\right)\\) converges almost surely to a vector \\(\\boldsymbol {\\rho }_{\\infty }=\\left (\\rho _{\\infty }^{0},\\dotsc,\\rho _{\\infty }^{m-1}\\right)\\), where \\({\\rho }_{\\infty }^{j}\\) is periodic in j with period d, \\({\\rho }_{\\infty }^{j}=1\\) if j\u22610 (mod d), and \\({\\rho }_{\\infty }^{j}={\\rho }_{\\infty }^{d-j}\\). In particular, every pair of symbols at distance d in s(n) are, with high probability, the same.The theorem implies that regardless of the seed, after many duplications, the sequence becomes almost periodic with period d. The periodicity is expected since no substitutions occur. However, the period is not the dominant or the shortest duplication length, but rather it is the gcd of all lengths i for which the probability of duplication qi is positive. For example, if duplications of lengths 4 and 6 occur, the sequence becomes approximately periodic with period 2. Since given P, d does not depend on the values of the qi, observing d does not provide enough information for estimating q and thus, in this case, we are not able to solve the inverse problem. Nevertheless, the study of this case lays the foundation for the more complex case in which substitutions are present and where we are able to solve the inverse problem.To prove Theorem 1, we need the following lemma whose proof is given in Additional file\u00a01.Since \u03c1\u221e is in the null space of A, where the null space of A is given by Lemma 1, \u03c1\u221e is a linear combination of the vectors S(d). Furthermore, by definition we know that \\(\\rho _{\\infty }^{0}=1\\). In the basis of S(d) given in Lemma 1, the only vector that has a nonzero element in the 0th coordinate is v0. So the coefficient of v0 in the linear combination describing \u03c1\u221e is 1 and thus \\(\\rho _{\\infty }^{j}=1\\) if j\u22610 (mod d). We hence have Theorem 1. \u25a1We now consider both tandem duplication and substitution mutations and describe how the parameters of the model, as well as the number of mutations of each type, may be estimated. Note that while the parameters of the model are unknown, we have access to the sequence s(n) for some n.The following lemma (see Additional file\u00a01 for proof) states that the autocorrelation function converges to a single point when both duplication and substitution mutations are present. This fact will facilitate the design of the estimator.Let q0>0, P={i>0:qi>0}, \\(d=\\gcd P\\), and let A be the matrix of Eq. (6). We have Null (A)=Span(v), where v=(v0,\u2026,vm\u22121)T is a vector satisfying v0=1 and \\(v_{j}=\\frac {1}{4}\\) for j\u22620 (mod d).For example, for \\(d\\,=\\,3, \\boldsymbol {v}\\,=\\,\\left (1,\\frac {1}{4}, \\frac {1}{4},v_{3}, \\frac {1}{4},\\frac {1}{4},v_{6},\\frac {1}{4},\\dotsc \\right)^{T}.\\)From the lemma, it follows that there is only one valid solution to the equation A\u03c1\u221e=0 which satisfies \\({\\rho }_{\\infty }^{0}=1\\). This unique point is the limit of the autocorrelation function.We have thus shown that if we know q, we can determine \u03c1\u221e. We now turn to the estimation problem, which is the inverse of determining \u03c1\u221e using q. In other words, we are given a sequence whose autocorrelation we can compute and our goal is to determine q.where we assume the length of the seed s(0) is equal to the pattern length. The estimator based on the proposed Stochastic Model of Tandem Repeats and defined by Eqs. (9) and (10) is referred to as SMTR.In tandem repeat sequences observed in genomes duplication events have lengths that are multiples of a certain value, leading to a pattern of that length appearing many times. We refer to this length as the pattern length and to the number of times that the pattern appears as the copy number. While in general SMTR does not need to know the pattern length d, if it is known, we set qi=0 for i\u22620(mod d). Furthermore, from Lemma 2, we know \\({\\rho }_{\\infty }^{r}=1/4\\) for r\u22620(mod d). Replacing these values in Eq. (10) allows us to solve it by keeping only rows and columns of C\u2032 whose indices are multiples of d.We note that in Eq. (10), if \\(\\hat {q}_{0}\\) is close to 1, then the estimate \\(\\hat {n}\\) for n may be very large. It is reasonable to expect that \\(\\hat {n}\\) is not larger than the length of the sequence. Thus, we add the constraint \\(\\left (d,2d,\\dotsc,m^{\\prime } d\\right)\\left (q_{d},q_{2d},\\dotsc,q_{m^{\\prime } d}\\right)^{T}\\ge 1\\) to Eq. (9), where d is the pattern length. This ensures that on average each mutation contributes at least 1 to the length of the sequence. Furthermore, since our method relies on asymptotic approximation, for short sequences, specifically those with copy number \u22643, we provide an alternative heuristic estimation algorithm, which is described in Additional file\u00a01.In this section, we use simulation to evaluate the performance of SMTR by comparing its estimates of the model parameters with the true values. We also compare SMTR to DTSCORE introduced by [27], which was shown to outperform similar methods [24]. Further, we apply SMTR to tandem repeats in the human genome to study variation across chromosomes and pattern lengths.In the results that follow, we set the computation parameters as follows. First, we find \u03c1=(\u03c1r) for \\(r=0,1,\\dotsc,\\left \\lfloor \\frac {|s|}{2}\\right \\rfloor \\). This ensures that each value of the autocorrelation function is the average of at least |s|/2 values. Furthermore, we let \\(m^{\\prime }=m^{\\prime \\prime }=\\min \\left (\\max (10d,5r^{*}),\\left \\lfloor \\frac {|s|}{2}\\right \\rfloor \\right)\\), where r\u2217= arg maxr\u03c1r. The max here is intended to ensure that m\u2032 is large enough, while the min ensures that all needed values of \u03c1 are available. Finally, while the estimation method is geared towards tandem repeats with substitution mutations, our inspection of the results shows that for perfect tandem repeats, the algorithm returns probability near zero for substitution mutations, as expected, and nearly uniform probability for different duplication lengths. Thus, in the results that follow, we apply it to tandem repeats regardless of the apparent presence of substitution mutations.We now turn to evaluating the performance of SMTR through simulation and also compare it with DTSCORE [27]. We show that SMTR provides more accurate estimates and is significantly faster compared to DTSCORE.We find the errors for two different cases: for a pair of given values for n and q, we estimate \\(\\hat {n}\\) and \\(\\hat {\\mathbf {q}}\\) based on 1) a single sequence and 2) ns sequences all generated with parameters q and n. In the latter case, estimates are obtained for each sequence individually and then averaged. The multiple-sample case is intended to show that performance improves, as expected, with more data. Due to the large number of tandem repeat sequences in many genomes, it is reasonable to expect that for a set of factors affecting duplication probabilities, e.g., GC content and pattern length, a given set of values for these factors is likely to arise multiple times. When studying the effects of such factors on mutation rates, we may expect a similar performance improvement by averaging the estimates among all instances with the same set of values for the factors.From Fig.\u00a02a, it is clear that SMTR estimates q\u2032 with significantly higher accuracy than DTSCORE. Furthermore, if multiple samples from the same distribution are available, the improvement for SMTR is larger than for DTSCORE. Finally, the execution time of SMTR is faster than DTSCORE. In particular, for n=120, on average, SMTR needs no more than 0.015 s to compute the estimate for each tandem repeat sequence, while DTSCORE needs 15 s, 3 orders of magnitude longer. As a result, SMTR will scale better when analyzing a large number of tandem repeats, for example, all repeats in a given chromosome or genome.While we have shown the improved accuracy and efficiency of SMTR compared to DTSCORE, we note that combinatorial methods such as DTSCORE are more generic in the sense that they do not rely on a stochastic model of the generation of tandem repeats. On the other hand, DTSCORE is more restrictive in the sense that it assumes duplications occur at the predefined boundaries of tandem repeat blocks (copies of the pattern). Blocks are meaningful if each copy is a gene, but in general, they are logical constructs rather than biological entities. Finally, it is worth noting that both DTSCORE and SMTR are designed for the analysis of repeats resulting from polymerase slippage and not recombination events.We now apply SMTR to tandem repeats in the human genome to estimate the number of substitution and tandem duplication mutations for each. We use these estimates to explore the variation of mutation rates for minisatellite and microsatellites and across chromosomes. Most of the results provided in this section rely on estimating the number of substitutions in tandem repeat sequences. We note that the DTSCORE algorithm only provides estimates for duplication events. Furthermore, due to its efficiency, SMTR is more appropriate for large-scale data analysis.We use the Tandem Repeats Database (TRDB) [28], which provides the set of tandem repeats in each chromosome, as identified by the Tandem Repeat Finder (TRF) algorithm, and related information such as the length of the repeat unit and indel (insertion/deletion) percentage. As a preprocessing step, among overlapping repeats, we keep only one. We also remove repeats with unknown (N) bases and those with copy number less than 2. Finally, we discard repeats whose indel percentage is nonzero, as our model does not include insertion and deletion mutations. We note however that the indel percentage is an approximate value for the number of apparent insertions and deletions. Excluding repeats with non-zero indel percentages does not guarantee that there will be no insertions or deletions in the remaining repeats. Another limitation is that our method assumes substitutions are unbiased, and so it cannot take into account different transition and transversion probabilities, or the effect of GC content. As an example of the preprocessing step, the number of repeat sequences in chromosome 1 reduces from 93,626 to 38,628 as a result of preprocessing.We now turn our attention to evaluating the variation of mutation rates across chromosomes. Through comparison with the chimpanzee genome [29\u201331], it is known that mutation rates vary across chromosomes. To see whether this variation can also be observed in repeated regions, we study the number of mutations in tandem repeat sequences across chromosomes. Since our model represents replication slippage, we only consider tandem repeats with short patterns. Specifically, for tandem repeats with pattern length \u226410, we estimate the number of substitution and duplication mutations. As a measure of mutation activity, we find the average of the ratio of the the number of substitutions to the length of the tandem repeat sequence for each chromosome (Fig.\u00a03b). The top five chromosomes that have the highest substitution rates are Y, 21, 22, 19, and 16. Based on comparison with the chimpanzee genome [31], the five chromosomes with highest mutation activity are Y, 21, 19, 22, and 16. Thus the top five chromosomes are the same based on the two approaches (p-value=0.00002). We repeated this analysis for repeats with maximum pattern lengths of 8, 9, 11, and 12, and in all cases, at least four of the top five matched the result from comparison with chimpanzee [31].We also considered the average number of mutations per tandem repeat for each chromosome (Fig.\u00a03c). On average, tandem repeats in chromosome 21 have a higher number of mutations than other autosomes. The average number of duplication mutations is estimated to be higher in chromosome 21 than in the Y chromosome. The higher number of mutations in chromosome 21 compared to other autosomes is also observed if we set the upper bound on the length of the patterns at 8, 9, 11, and 12.Figure\u00a01 demonstrates that compared to DTSCORE, the proposed method, SMTR, is both more accurate and faster. The efficiency of SMTR allows it to be applied at the genome scale. Such large-scale analyses enable statistically studying hypotheses about the formation of tandem repeats.We studied the relationship between the length of the pattern in a tandem repeat and number of substitution and duplication mutations. A clear difference emerges between minisatellites and microsatellites, as shown in Fig.\u00a03a. The different mutation profiles suggest that these two types of tandem repeats may result from different mutation mechanisms. This is compatible with previous findings, where polymerase slippage is thought to give rise to microsatellites while unequal recombination is believed to cause the heterogeneity observed in minisatellites [32]. Our method is only designed to model slippage and not recombination. The fact that it generally estimates the number of substitutions to be higher for minisatellites than microsatellites can be the result of higher raw heterogeneity that is observed in microsatellites and/or caused by model mismatch. The results of this analysis suggests that it is possible to design statistical tests to decide the origin of tandem repeat sequences, as a means of classifying them, rather than relying on classification merely based on pattern length.Figure\u00a03b presents the normalized number of substitutions in tandem repeats, averaged for each chromosome. As discussed, the five chromosomes with the highest rates in Fig.\u00a03b are the same as the five chromosomes with the highest mutation rates, as obtained by [31] based on comparison with chimpanzee genome. This suggests a strong relationship between substitutions in repeated regions and overall mutation activity in chromosomes. On the other hand, the results are not exactly aligned. For example, while chromosome X has the smallest divergence from chimpanzee, it does not have the smallest normalized number of substitutions. Overall, our results suggest estimation of mutation activity based on tandem repeats can be a powerful tool in studying mutations since unlike existing methods it relies on a single genome rather than on comparison of genomes from different species.In Fig.\u00a03c, the reason that tandem repeats in chromosome 21 exhibit a higher number of mutations is unknown to us but it is interesting to note that individuals with trisomy 21 can survive into adulthood, which suggests that mutations in chromosome 21 are relatively better tolerated. It is also observed that 3 of the 5 chromosomes with the highest total number of mutations in microsatellites, Y, 21, and 22, match the result from [31]. This further suggests a higher mutation activity in these chromosomes. However, care should be taken in interpreting results about mutation counts that are not normalized by the length of the sequence. The opportunity for mutation increases with length and copy number. In particular, increased copy number may increase the probability of misalignment during replication [33]. Another factor that can affect the number of mutations in a complex manner is the interplay between substitution mutations and tandem duplication mutations: if many substitutions occur, the copies become more heterogeneous, which may decrease the possibility of misalignment. This interaction is not taken into account in our model and left to future work.In this paper, we introduced a new stochastic model for tandem duplication and substitution mutations, and analyzed it via stochastic approximation. In particular, we fully characterized the limit set of the stochastic process described by the model. In addition to enabling us to predict the behavior of a sequence that undergoes tandem duplication and substitution mutations, this characterization allowed us to derive a minimization problem whose solutions are estimates of the conditional mutation probabilities for tandem duplication and substitution. We showed further that it is possible to estimate the total number of mutations. Finally, we evaluated the estimation method via simulation by generating random sequences and comparing the estimated probabilities with the true values and also applied it to the human genome, where it demonstrated the differing behavior of micro- and mini-satellites as well as the variability of mutation activity across chromosomes.Advantages of our method include its scalability and the fact that it relies on a single sequence to infer occurrences of mutations. While with this method, we can learn only about mutations in tandem repeat regions, our results show that the findings may be applicable to surrounding regions and can be of use in forming hypotheses about mutation activity, for example, about factors that increase or decrease activity.There still exist many open problems in stochastic modeling and estimation for tandem repeats. For example, the model presented here does not take into account deletions nor the fact that the level of heterogeneity may affect the probability of tandem duplication. Neither does the model consider bias in substitution mutations. For example, it cannot reflect different transversion and transition probabilities. Incorporating such biases will make the method more appropriate, for instance, for GC rich repeats. Further, we only analyzed it in the asymptotic regime and left finite-time behavior to future work. Finite-time analysis will enable us to analytically quantify the accuracy of the presented estimation method as a function of the number n of mutations and to devise improved estimation algorithms. Finally, further work is needed to accurately model mutations other than DNA slippage that cause duplication, especially those that lead to minisatellite repeats.Ordinary differential equationNormalized root mean square errorThis research was supported by National Science Foundation grants CCF-1317694 and CCF-1755773, and by a United States \u2013 Israel Binational Science Foundation (BSF) grant no. 2017652.The datasets analyzed in the current study are available from the Tandem Repeat Database (https://tandem.bu.edu/cgi-bin/trdb/trdb.exe) [28] under organism: Homo sapiens HG38.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.To understand how \u03c1n varies, our starting point is the difference sequence \u03c1n+1\u2212\u03c1n. Similar to [26] and as described in the Additional file\u00a01 for completeness, it can be shown that \n (1)This system can be analyzed through stochastic approximation ([25], Theorem 2), by relating the discrete system describing \u03c1n to a continuous system. In particular, the sequence \u03c1n converges almost surely to a compact connected internally chain transitive invariant set of the ODE \n (2)While different properties of the sequence can be analyzed via the aforementioned method, for our purpose, the autocorrelation of the sequence is the most suitable, as it captures the degree of repetitiveness of sequences arising from tandem duplication. The autocorrelation function Rr of a sequence \\(s=s_{1}\\dotsm s_{\\left |s\\right |},\\,s_{i}\\in \\mathcal {A}\\), at lag r, is defined as \n where indices of s are computed modulo |s| and \u3008\u03b1,\u03b2\u3009=1 if \u03b1=\u03b2 and \u3008\u03b1,\u03b2\u3009=0 otherwise.To find the ODE of Eq. (2), we need to find \\(\\boldsymbol {h}_{\\ell }(\\boldsymbol {\\rho })= \\left ({h}_{\\ell }^{0}(\\boldsymbol {\\rho }),\\dotsc,{h}_{\\ell }^{m-1}(\\boldsymbol {\\rho })\\right)\\). As shown in Additional file\u00a01, \n (3)From Eq. (2), we have \n (4)For m\u2265K, we can write Eq. (4) as \n (5)where A is the m\u00d7m matrix whose rows and columns are indexed by {0,1,\u2026,m\u22121} and its elements are given as \n (6)As discussed in Additional file\u00a01, \u03c1t converges to some \u03c1\u221e satisfying \n (7)Let q0=0, P={i>0:qi>0}, and \\(d=\\gcd P\\). Furthermore, let S(t)=Span{v0,\u2026,v\u230at/2\u230b}, where vi=(vi,0,\u2026,vi,m\u22121)T, with \n We have Null (A)=S(d).Note that we can rewrite the equation A\u03c1\u221e=0, where A is the matrix given in Eq. (6), as \n (8)where q = (q0,q1,\u2026,qm)T and \\(\\tilde {\\boldsymbol {\\rho }_{\\infty }}=\\left ({\\rho }_{\\infty }^{1}, 2 {\\rho }_{\\infty }^{2},\\dotsc, (m-1){\\rho }_{\\infty }^{m-1}\\right)^{T}\\), and where C=(Cri) is a (m\u22121)\u00d7(m+1) matrix whose elements are \n where r\u2208{1,\u2026,m\u22121} and i\u2208{0,1,\u2026,m}.Given \u03c1\u221e, we can solve Eq. (8) for q. Since we only know the sequence after a finite time n, we approximate \u03c1\u221e by \\(\\boldsymbol {\\rho }_{n} =\\left (\\rho _{n}^{0},\\dotsc,\\rho _{n}^{m-1}\\right)\\) computed from s(n). In our model, there exists K such that qi=0 for i>K. However, the value of K is unknown to us. We thus choose some m\u2032 and assume that qi=0 for i>m\u2032. The value of m\u2032 can be chosen for example based on our knowledge of the underlying biological processes, such as slipped-strand mispairings [7], that lead to tandem repeats. Furthermore, the value of m\u2032 should be chosen large enough so that m\u2032\u2265K with a high degree of confidence. Note that there are m\u2032+1 unknown quantities, namely, the elements \\(q_{0},\\dotsc,q_{m^{\\prime }}\\) of q. Another parameter is the number of equations used to estimate q, denoted m\u2032\u2032, which should be chosen close to m\u2032. Having chosen m\u2032,m\u2032\u2032, we can write Eq. (8) as \\(C^{\\prime } \\mathbf {q}=\\tilde {\\boldsymbol {\\rho }_{n}}\\), where \\(\\mathbf {q}=\\left (q_{0},q_{1},\\dotsc,q_{m^{\\prime }}\\right)^{T}\\) and \\(\\tilde {\\boldsymbol {\\rho }_{n}}=\\left ({\\rho }_{n}^{1},2{\\rho }_{n}^{2},\\dotsc,m^{\\prime \\prime }{\\rho }_{n}^{m^{\\prime \\prime }}\\right)^{T}\\), and where C\u2032 is the matrix containing the first m\u2032\u2032 rows and the first m\u2032+1 columns of C, computed using \u03c1n instead of \u03c1\u221e. Now to obtain an estimate of q we can solve the least-square curve fitting problem \n (9)The solution \\(\\hat {\\mathbf {q}}\\) of this problem contains an estimate of the substitution probability q0 and the probabilities q\u2113 of duplications of lengths \u2113. Noting that the expected length added to the sequence by each mutation is \\({\\sum }_{i=1}^{m^{\\prime }}i\\hat {q}_{i}\\), we estimate the total number n of mutations that have occurred as \n (10)In our simulation set up, we first generate a random seed s(0) of a random length d that then undergoes n random substitutions and tandem duplications, where the probabilities of these events are given by q, itself randomly generated. The resulting sequence s(n) and the pattern length d are then passed to the SMTR estimator, which of course does not know s(0), n, or q. We evaluate the performance by finding the L2 error in estimating \\(\\hat {\\mathbf {q}}\\), \\(\\|\\hat {\\mathbf {q}}-{\\mathbf {q}}\\|_{2}\\), averaged across N experiments for each value of n. We also find the normalized root mean square (NRMS) error in estimating n. For a given value of n, NRMS Error is defined as \n where N is the number of experiments with n mutations and \\(\\hat {n}_{i}\\) is the estimate for n in the ith experiment.More detail on the simulation setup is given in Additional file\u00a01. The results are given in Fig.\u00a01 where n ranges from 10 to 500, with step size equal to 10. For each value of n, the experiment is performed N=500 times, and in each of these N trials, estimates are obtained based on a single sequence and based on ns=5 sequences drawn for the same seed and q. We observe that as n increases, the errors sharply decrease. For a single sequence and a small number of mutations, the estimation algorithm relies on a very limited amount of data. As the number n of mutations increases, the sequence becomes longer, providing more data in the form of the autocorrelation function and asymptotic approximations become more accurate. It is also observed that with more samples for the same set of parameters, more accurate estimates are obtained.\n\n\nFig. 1\nErrors of the estimate \\(\\hat {\\mathbf {q}}\\) of q, (a), and the estimate \\(\\hat {n}\\) of n, (b)\n\nWe now compare the performance of SMTR with DTSCORE [27]. DTSCORE is a distance-based algorithm designed to find the duplication history in the form of a tree, thus providing estimates for the counts of duplications of various lengths. In [24], it was shown that DTSCORE performs better than other algorithms for identifying the duplication tree, including TRHIST [22] and WINDOWS [23]. Due to the slower speed of DTSCORE (the worst-case time complexity is O(L4), where L is the copy number), we restrict the range of the number of mutations n to {10,20,\u2026,120} and also reduce N=200 but maintain ns=5. As the distance measure, we use Jukes-Cantor\u2019s [11], which is compatible with our sequence generation method. The comparison is given in Figure\u00a02. Since from DTSCORE, we can only derive estimates for the counts of duplications but not substitutions, we compare the accuracy of estimating \\(\\mathbf {q}^{\\prime }=\\left (q^{\\prime }_{1},q^{\\prime }_{2},\\dotsc \\right)\\) where \\(q^{\\prime }_{i}\\) for i\u22651 is defined as \\(q^{\\prime }_{i}=\\frac {q_{i}}{1-q_{0}}\\ \\cdot \\)\n\n\nFig. 2\nComparison of SMTR Estimation (SM) and DTSCORE (DT): Error of \\(\\hat {\\mathbf {q}}^{\\prime }\\), (a), and the average execution time for an instance of the problem on an Intel Core i7\u20137700 CPU with 16 GB of RAM, (b)\n\nWe applied the SMTR algorithm to tandem repeats in each chromosome to study the role of tandem duplication and substitution mutations in their formation. The results for chromosome X are given in Fig.\u00a03a. Each point in this plot corresponds to a tandem repeat sequence. The position of each point is determined by the estimated number of tandem duplications and substitutions that occurred to create the sequence. It can be observed that tandem repeat sequences can roughly be divided into two clusters with different behaviors: one dominated by tandem duplication mutations and the other by substitution mutations. This difference in behavior matches well with the classification of tandem repeats as microsatellites and minisatellites, with pattern lengths of 1\u201310 and 11\u2013100 bases, respectively. Other chromosomes exhibit behavior similar to chromosome X illustrated here. Among all chromosomes, the minimum Kendall tau correlation coefficient between the rankings of repeats based on length of the pattern and based on the fraction of mutations that are substitutions was 0.5160. Given the large number of tandem repeats in each chromosome, such high correlation coefficients lead to p-values that are practically zero (as computed with MATLAB).\n\n\nFig. 3\nThe mutation profile of tandem repeats in chromosome X (a) and mutation variation across chromosomes in microsatellites (pattern length \u226410): Mean of the ratio of the number of substitutions to the length of the tandem repeat sequence (b) and mean of the total number of mutations per tandem repeat sequence for each chromosome (c)\n\n\n\nAdditional file 1\nSupplementary Material. Section 1 (\u00a7SM.1) Proof of (1). \u00a7SM.2: Proof of (3). \u00a7SM.3: Proof of (7). \u00a7SM.4: Proof of Lemma 1. \u00a7SM.5: Proof of Lemma 2. \u00a7SM.6: Estimation for copy number \u22643. \u00a7SM.7: Simulation Setup. (PDF 244 kb)", "s12859-019-2658-z": "It is possible to predict whether a tuberculosis (TB) patient will fail to respond to specific antibiotics by sequencing the genome of the infecting Mycobacterium tuberculosis (Mtb) and observing whether the pathogen carries specific mutations at drug-resistance sites. This advancement has led to the collation of TB databases such as PATRIC and ReSeqTB that possess both whole genome sequences and drug resistance phenotypes of infecting Mtb isolates. Bioinformatics tools have also been developed to predict drug resistance from whole genome sequencing (WGS) data.Here, we evaluate the performance of four popular tools (TBProfiler, MyKrobe, KvarQ, PhyResSE) with 6746 isolates compiled from publicly available databases, and subsequently identify highly probable phenotyping errors in the databases by genetically predicting the drug phenotypes using all four software.Our results show that these bioinformatics tools generally perform well in predicting the resistance status for two key first-line agents (isoniazid, rifampicin), but the accuracy is lower for second-line injectables and fluoroquinolones. The error rates in the databases are also non-trivial, reaching as high as 31.1% for prothionamide, and that phenotypes from ReSeqTB are more susceptible to errors.The good performance of the automated software for drug resistance prediction from TB WGS data shown in this study further substantiates the usefulness and promise of utilising genetic data to accurately profile TB drug resistance, thereby reducing misdiagnoses arising from error-prone culture-based drug susceptibility testing.While the prevalence of tuberculosis (TB) worldwide has been decreasing, the emergence of drug-resistance forms of TB (DR-TB) poses a new public health crisis in numerous parts of the world, with multidrug-resistant TB (MDR-TB) alone being responsible for 480,000 deaths annually [1]. The World Health Organisation (WHO) has recommended that all TB patients should be tested for drug resistance [2], although conventional drug susceptibility testing (DST) can take more than 6\u2009weeks and requires a laboratory equipped for strict biosafety [3]. These infrastructural and time requirements mean the majority of treatment-seeking TB patients in lower- and middle-income countries are started on first-line TB drugs regardless of their drug-resistance status, as there are inadequate resources and capacity to screen every TB patient [1].Drug resistance fundamentally develops through the accrual of specific mutations in the genome of the infecting pathogen Mycobacterium tuberculosis (Mtb) [4]. By performing whole genome sequencing (WGS) of infecting Mtb isolates which have undergone conventional DST, the mutations that confer drug resistance to specific antibiotics can be identified [5]. Sufficiently large databases possessing both WGS and DST information have allowed the drug resistance to several antibiotics to be accurately predicted, to the extent that rapid diagnostics that target these mutations have been developed [6\u20138]. GeneXpert MTB/RIF is one of these rapid diagnostic tests that predicts rifampicin resistance by querying specific positions in the Mtb genome [9].One of the key advantages these diagnostics possess over conventional DST is the speed at which the drug resistance profile can be obtained. For example, GeneXpert MTB/RIF can detect MTB and the genetic determinants of RR-TB in 2\u2009h [10], and this allows the timely identification of optimal treatment regimens. The premise is simple: if a patient is diagnosed early to suffer from a form of TB that is resistant to rifampicin, the physician will prescribe an alternate rifampicin-free regimen in order to ensure efficacious treatment, instead of maintaining a standard but ineffective first-line regimen for 6\u2009months.While the premise is straightforward, the majority of present diagnostics are however confined to screening for resistance to a limited range of antibiotics. What this means is that a positive result from GeneXpert may indicate rifampicin resistance, but does not provide any insights towards the resistance profiles to other anti-TB agents. Physicians escalate to second-line agents except this is again unsupported by whether the patient is resistant to any of the second-line agents.The ability for WGS to simultaneously infer the resistance profiles to numerous anti-TB agents is thus attractive, since the physician can be guided to prescribe a combination of antibiotics that is more likely to be effective. Operationally, this relies on having a well-curated knowledge of the range of genetic mutations that confer resistance to the different anti-TB agents, as well as bioinformatic tools that translate the genomic sequence to drug resistance information that can be understood by physicians.Several databases such as PATRIC [11\u201313] and ReSeqTB [14, 15] possessed both the genomic sequences and DST phenotypes of thousands of infecting Mtb isolates from different parts of the world, and these have facilitated the identification of genomic sites associated with drug-resistance. Drug-resistance prediction software such as TBProfiler [16], MyKrobe [17]. KvarQ [18], and PhyResSE [19], rely on these genomic sites to predict the drug resistance profile of a Mtb genome to the range of anti-TB agents. Oftentimes, the phenotypes from conventional DST are assumed to be free from errors, and this can invariably confound the identification of resistance-conferring genomic sites especially for anti-TB drugs such as streptomycin, ethambutol and pyrazinamide where the DST error rates have been reported to be higher [3].Here, we aim to benchmark the sensitivity and specificity of the four prediction software, and to evaluate the fidelity of the reported DST phenotypes in 6746 Mtb isolates compiled from the PATRIC [11\u201313] and ReSeqTB [14, 15] databases, and 10 other studies. The evaluation is achieved by comparing the reported phenotypes against the genetically inferred phenotypes, and this consequently allowed the identification of the anti-TB drugs where laboratory-determined DST results are more likely to be erroneous.We focused on two databases hosting Mycobacterium tuberculosis (Mtb) isolates with complete genomic sequences and drug-susceptibility phenotypes: (1) PATRIC \u2013 which hosts nearly 150,000 genomes belonging to more than 20 bacterial genera with drug resistance status for nearly 100 antibiotics [12, 13]; and (2) ReSeqTB \u2013 which is a specific database for driving the development of novel rapid diagnostic tests and personalised treatment of TB [14, 15]. The drugs that our study considered include five first-line agents: rifampicin (RIF), isoniazid (INH), pyrazinamide (PZA), ethambutol (EMB), streptomycin (STM); 3\u2009second-line injectables: amikacin (AMK), capreomycin (CAP), kanamycin (KAN); three oral second-line drugs: ethionamide (ETO), prothionamide (PTO), p-aminosalicyclic acid (PAS); and three fluoroquinolones: ciprofloxacin (CIP), moxifloxacin (MFX), ofloxacin (OFX).Data from PATRIC was obtained by querying all Mtb contributions with resistance profiles for anti-TB drugs, and whose genome sequences were also contributed to the Sequencing Reads Archive (SRA) and BioProject accessions (accessed Jan 5, 2018). This yielded a set of 5035 isolates. For ReSeqTB, we considered the samples available from the repository in the folder Databases/ReSeqTB/fullExportDb-1254-External-CSV/msf.xls (accessed Jan 18, 2018) with available SRA accessions, which yielded a set of 3568 isolates.In addition, we included another set of 5471 isolates identified from a literature review by querying PUBMED for \u201cwhole genome sequencing drug resistant tuberculosis\u201d, retaining only samples with both whole genome sequences and conventional DST phenotypes that are available online publicly and conforming to the following inclusion criteria: (1) isolates should be from clinical samples; (2) there should be at least 10 samples from each study; (3) DST must be performed on at least four of the five first-line drugs; and (4) the study must possess at least one drug-resistant TB isolate (see Additional\u00a0file\u00a01). This set is subsequently referred to as the \u201cLitRev\u201d set.For the three datasets that we considered, comprising of PATRIC, ReSeqTB, and LitRev, we highlight there are samples that are found in more than one dataset, including with conflicting DST results for specific drugs. Further details on harmonising the datasets for our analyses can be found in the next section.The mapping quality of the genome sequences for all samples were assessed to identify samples with poor quality sequencing, as well as samples that were incorrectly classified as Mtb (see Additional\u00a0file\u00a02). There are samples with multiple DST outcomes for a specific drug, either due to the sample being located in more than one dataset, or because there are multiple DST results from within one dataset. For these samples, we retained only one final DST outcome according to the following criteria: (i) if the multiple DST outcomes are consistent, then assigning the final outcome is trivial; (ii) if the sample is located in more than one dataset and there is a discordance in the multiple DST outcomes, we discard the outcomes from ReSeqTB and/or PATRIC, and either assign the outcome from LitRev or discard the sample entirely; (iii) if the sample belongs uniquely to PATRIC and contains multiple outcomes, the results obtained from an WHO-endorsed protocol were retained.We considered four bioinformatics software that are designed to infer the resistance profiles of anti-TB drugs using the Mtb genome sequence: TBProfiler [16], MyKrobe [17], KVarQ [18], and PhyResSE [19]. All four software take in raw WGS data such as fastq or bam files, and produce results in varying interfaces that are specifically designed to aid interpretation by clinical microbiologists. These four software predict resistance to all five first-line drugs and most of the key second-line agents, by considering the genetic alleles on different panels of curated mutations that are associated with drug resistance (see Additional\u00a0file\u00a03). The different software utilise different bioinformatics algorithms and may refer to a dissimilar panel of mutations to predict resistance, thereby producing outcomes that may differ between the four tools. Specifically, TBProfiler maps input sequence to a truncated version of the H37Rv reference genome (GenBank accession number: NC_000962.3), which contains only the regions of interest to drug resistance prediction, before identifying the presence of genetic markers of resistance. On the other hand, MyKrobe compares the de Bruijn graph of the input sequence with the graph built from a collection of resistant and susceptible alleles on diverse genetic backgrounds to determine resistance profile. In KVarQ, each read from the input is queried against a series of target sequences constructed from known mutations or regions associated with drug resistance, with each match increasing the confidence of the presence of the genetic markers in the sample. In contrast, PhyResSE follows a more traditional route of mapping the fastq input to the complete reference genome and then calling all single nucleotide polymorphisms (SNPs) and small indels before comparing them to its mutations panel. All four tools however allow the panel of mutations to be updated whenever new markers of drug resistance are discovered, and allow the inference of the lineage of the Mtb samples. Among the four tools, MyKrobe possesses the additional capability to identify species in situations of mixed infections, while PhyResSE additionally allows stringent pre-processing and quality control of sequencing data. In our evaluation, we used TBProfiler version 0.3.2, MyKrobe v0.5.6\u20130-gbd7923a-dirty, KvarQ version 0.12.3a1, and PhyResSE pipeline version metaphyresse.v7 implemented locally with SNP catalogue version 29.All sequencing reads sets were used as inputs to each of the TB resistance prediction algorithms. In case of no calls or if there were discordant results among the reads sets of a sample for a drug, the prediction for the sample for that particular drug was treated as missing. In our sensitivity and specificity calculation and parameter estimation for DST credibility computation, which are subsequently described in detail, we omitted those samples clearly indicated to have been used to train the tools. The sensitivities and specificities of the four algorithms for each drug were first calculated together with their 95% confidence interval, with the corresponding phenotypes in the data collection as the gold standard, except for the case of fluoroquinolones, where the performance was benchmarked to CIP, MFX, and OFX individually; the case of PTO, where the performance was benchmarked against ETO; and the case of KVarQ\u2019s Kanamycin/Amikacin prediction, which was benchmarked against KAN and AMK individually. The DST credibility score for each sample was calculated as the probability that the phenotype of the sample is correctly specified given the prediction results from the four programs. Specifically, for a drug i, let the prevalence of drug resistance be Pi, and the true sensitivity, the true specificity, the proportion of no-call predictions among truly resistant samples and the proportion of no-call predictions among truly susceptible samples of each program be Sensij, Specij, NoRij, and NoSij, for j in {TBProfiler,\u2009KVarQ,\u2009MyKrobe,\u2009PhyResSE}. We denote the predictions of the four algorithms aswith Likelihood(predictions) calculated asThe DST credibility score of a sample thus equals P(sample is resistant\u00a0|\u00a0predictions) if its reported phenotype is resistant, or 1\u2009\u2013\u2009P(sample is resistant\u00a0|\u00a0predictions) if the reported phenotype is susceptible. The quantities Pi, Sensij, Specij, NoRij, and NoSij were maximum likelihood estimates (MLE) obtained from an EM algorithm. We chose to use the MLE estimates, as opposed to incorporate informative prior distributions and obtain maximum a posteriori (MAP) estimates, due to the lack of prior knowledge to select meaningful prior distribution for these parameters. The EM algorithm was initialized with the set of parameters derived by assuming the conventional DST results for the samples were impeccable. The sensitivity and specificity estimates obtained from this model are subsequently called the adjusted sensitivity and specificity. Proxy for the rate of misclassification of phenotypic DST results for each drug in a database, either individual or merged database, is calculated as the proportion of samples with DST credibility score smaller than 0.5 to the total number of valid samples. Proxy for the rate of non-utilizable data for a drug is calculated as the proportion of samples removed from the database due to discordant phenotypic results to the total number of samples with good mapping quality WGS data and drug resistance results for that drug in the database.We considered four bioinformatics software (TBProfiler, KVarQ, MyKrobe, PhyResSE) capable of inferring resistance to a spectrum of antibiotics based on the genomic sequence of the infecting Mtb agent. For each software, we calculated the empirical sensitivity and specificity using the isolates from the databases by comparing the genetically-inferred phenotypes for each drug against the reported DST phenotypes, under the assumption that the latter are accurate.We observed that the sensitivity of TBProfiler was consistently higher than those of the other three predicting software for the majority of the 14 drugs considered (see Table\u00a01). However this came at a marginal compromise on the specificity, which was lower for TBProfiler across most of the drugs when compared to the other three software. For example, in the case of amikacin (AMK) with data from 1649 isolates, the sensitivity from TBProfiler was the highest at 90% whereas those from MyKrobe, KVarQ and PhyResSE were at 75, 75 and 79% respectively; conversely, the specificity of TBProfiler was the lowest at 75% whereas those from the other three software were at 99%. Overall, there were specific drugs whose resistance profiles (whether susceptible or resistant) were better predicted by each software.At the drug-level, the common first-line agents such as isoniazid (INH) and RIF can be predicted with sensitivities and specificities in excess of 90% by all four software; whereas the resistance status for antibiotics such as pyrazinamide (PZA), ethionamide (ETO), prothionamide (PTO), and para-aminosalicylic acid (PAS) reported sensitivities lower than 60%.We have assumed the reported DST phenotypes in the public databases are accurate in assessing the performance of the predicting software. However, laboratory-ascertained DST results can contain errors due to the semi-qualitative nature of the assessment. As such, we used a combination of all four predictors to probabilistically assess the likelihood that each reported phenotype is an error, after accounting for the degree of confidence in ascertaining the accuracy of the genetically inferred phenotype.Despite the adjustment, the ability to predict the presence of drug-resistance remains poor for ETO, PAS, PTO and PZA. However, the recalibration indicated that the majority of the software were able to deliver sensitivity in excess of 90% for INH, RIF, EMB, STM, AMK, CAP, KAN, CIP, MFX, and OFX. The trends in the relative performance between the four software remained unchanged after the recalibration, with TBProfiler being the most sensitive software in the detection of resistance in most of the 14 drugs at the expense of marginally lower specificities compared to MyKrobe, KVarQ and PhyResSE.The ability to rapidly identify which are the viable drugs for treating a TB infection is likely to be increasingly urgent, given the proliferation of drug-resistant strains of the Mtb pathogen. The use of whole-genome sequencing to replace conventional laboratory-based drug-susceptibility testing reduces the time taken to culture and test against individual drugs, but requires sophisticated bioinformatics algorithms to be designed and validated to translate the genetic information into drug susceptibility phenotypes. In this paper, we have evaluated four bioinformatic software for predicting TB drug resistance from Mtb genome sequences, using the largest set of isolates available and for 14 antibiotics. Using latent class modeling, we have identified which isolates in the existing databases are likely to present erroneous DST phenotypes and recalibrated the performance of these bioinformatics software adjusting for errors in the existing databases.All four software present considerably accurate predictions for the majority of the 14 antibiotics, although the ability to correctly detect resistance tends to be lower than the ability to correctly detect the absence of resistance. These results concur with the findings of a previous comparison study [20], as well as that of a recent study where genetic sequencing was established to be sufficiently accurate for drug resistance surveillance in place of laboratory-based DST phenotyping [21]. However, we still observe low sensitivity in detecting resistance to ETO, PAS, PTO, and PZA. This could be attributed to the lack of information around which genetic polymorphisms are responsible in resisting the drugs, especially since the sample sizes for isolates with DST phenotypes at these drugs (except for PZA) are correspondingly lowest. Without a clear understanding to the genetic biomarkers of drug resistance or to their penetrance, or at least more powerful algorithms that can pick up additional intricacies of resistance across multiple drugs, such as what have been done for HIV and cancer [22\u201324], the ability to successfully predict the presence of drug-resistance is reduced. Thus, even though our inference on the accuracy of laboratory-based DST indicated there were higher rates of misclassification at these drugs, it should be highlighted that the estimation of the misclassification rates is itself subject to considerable errors.Our evaluation of the performance of the bioinformatics predictors differed from previous reports, since we also calibrated the performance as we probabilistically determined the likely erroneous phenotype entries in the Mtb databases. These errors are adjusted to the (highly) probable correct phenotypes, inferred using the combined power of the four software with the assumption that the outputs of all four software are independent conditional on the genome sequence. This can, however, lead to the problem of over-fitting the recalibrated performance of the four software, as each software contributes partly to adjust the phenotypes with overlapping sets of genetic markers. For drugs that can be predicted with high levels of sensitivity and specificity, the likelihood of over-fitting is expected to be negligible. For drugs where either the sensitivity or specificity are comparatively lower, the recalibrated accuracy still serves as an upper bound to the performance of these software, while the unadjusted accuracy serves as the lower bound by calibrating the performance against a database with erroneous entries.Our study has quantified the credibility of laboratory-based DST phenotypes for a large number of isolates with publicly available WGS data. The ability to perform in silico TB drug-resistance profiling provides the opportunity for a consistent, standardised, and accredited model to obtain DST phenotypes, one that is independent of variations in laboratory quality control. While there are still infrastructural limitations to the widespread adoption of WGS for identifying DR-TB in resource-poor settings, the availability of accurate bioinformatics predictors will undoubtedly be valuable for translating genetic sequences into clinically actionable information to guide efficacious drug prescription.AmikacinCapreomycinCiprofloxacinDrug-resistant TBDrug susceptibility testingEthambutolEthionamideIsoniazidKanamycinMultidrug-resistant tuberculosisMoxifloxacinMaximum likelihood estimate\nMycobacterium tuberculosis\nOfloxacinP-aminosalicyclic acidProthionamidePyrazinamideRifampicinSingle nucleotide polymorphismStreptomycinTuberculosisWhole genome sequencingWorld Health OrganisationTMN is supported by a scholarship from the NUS Graduate School of Integrative Sciences and Engineering. The funding body had no role in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.The datasets analysed during the current study are freely available in The European Nucleotide Archive (http://www.ebi.ac.uk/ena) [25], the ReSeqTB database (https://platform.reseqtb.org) [15], PATRIC (www.patricbrc.org) [13], and other published studies as described in Additional\u00a0file\u00a05. All run accessions of the sequencing data used in the manuscript are also included in this Additional file.Not applicable.Not applicable.The authors declare that they have no competing interestsSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nwhere Yj\u2009=\u20091 if the sample is predicted resistant, 0 if susceptible, and NA otherwise. The probability of the sample being resistant given the predictions of the four tools is as follows:\n\nwhereand\n\nThe total number of samples across the three databases with both WGS and drug resistance phenotypes for at least one drug is 6756, of which 2302 samples are present in all three databases (see Additional\u00a0file\u00a04: Figure S1, Additional\u00a0file\u00a05). After excluding samples with poor sequencing quality (10 samples, see Additional\u00a0file\u00a02) and discordant DST phenotypes, the number of unique isolates ranges from 4831 to 6694 for first-line agents (STM, RIF respectively), and from 457 to 2424 for second-line and third-line agents (CIP, OFX respectively, see Additional\u00a0file\u00a06). After excluding samples used in the training of the software, the figure ranges from 4357 to 5026 for first-line agents (STM, PZA respectively) and from 191 to 2424 for second- and third-line agents (CIP and OFX respectively, see Table\u00a01).Table 1\nEmpirical sensitivities and specificities of four software for predicting anti-TB drug resistance\n\nNumbers in brackets represent the corresponding 95% confidence intervals. An NA is assigned when the software does not predict the resistance profile for the specific drug\nWe observed the error rates were less than 5% for CIP, RIF, INH and AMK, although two of the remaining three first-line agents (EMB, STM) yielded error rates in excess of 10% (see Fig.\u00a01, Additional\u00a0file\u00a06). The rates of misclassification were considerably higher for the second-line agents such as ETO, PTO and PAS, although the uncertainty of these estimates were considerably larger given the smaller sample sizes and poorer sensitivities of the software in inferring isolates with genuine resistance to these drugs.\n\nFig. 1\nMisclassification rates of laboratory-based DST results by drugs. The solid circles represent the point estimates of the misclassification rates upon comparing the laboratory-based DST phenotypes with genetically-inferred drug-resistant phenotypes. The genetically-inferred phenotypes were probabilistically ascertained using all four software. The vertical lines represent the corresponding 95% confidence intervals\nIn assessing the errors, we also evaluated which database was more prone to erroneous phenotypes. We observed similar trends across the three databases (PATRIC, ReSeqTB, LitRev) for most of the drugs except for PAS, where the misclassification rate was considerably lower at 2.6% in ReSeqTB compared to 20.8% in PATRIC and 10.8% in LitRev (see Table\u00a02). However, we observed that the rate of non-utilizable data was generally highest in ReSeqTB (except for KAN, CIP, ETO and PAS).Table 2\nSummary of isolates in the three TB databases according to the 14 anti-TB drugs\n\nNclean refers to the number of isolates with valid DST results and genetically-inferred credibility scoring; Rerror refers to the misclassification rate in each database, defined as the proportion of the Nclean isolates with DST credibility scores <\u20090.5; Nfull is defined as the summation of Nclean and the number of good mapping quality isolates with no genetically-inferred credibility scoring; and Rmissing refers to the proportion of Nfull isolates that presented unusable laboratory-based DST phenotypes due to either inconsistent results (across multiple DST phenotype entries for the same isolate) or documentation errors across the databases\nWhile inferring the likely erroneous laboratory-based DST phenotypes by their genetically-inferred phenotypes, we also obtained a calibrated performance of the four software as part of the model output. In theory this provides a better indication of the performance of these software, except there is the possibility of overfitting (see Discussion later). In general, the estimated sensitivity and specificity figures were higher after adjusting the phenotypes (see Table\u00a03), although surprisingly there were some decreases in sensitivity (PAS) and specificity (AMK, INH) for TBProfiler.Table 3\nCalibrated sensitivities and specificities of four software for predicting anti-TB drug resistance\n\nAn NA is assigned when the software does not predict the resistance profile for the specific drug\n\n\nAdditional file 1:\nStudies reviewed for the LitRev data set. This file presents the list of studies surveyed for LitRev. (XLSX 27 kb)\n\n\n\nAdditional file 2:\nQuality screening of sequencing data. This file details the quality control step of the study. (DOCX 26 kb)\n\n\n\nAdditional file 3:\nCharacteristics of the four bioinformatics software for predicting drug resistance from Mtb genome sequences. This file delineates the features of the four tools TBProfiler, MyKrobe, KVarQ, and PhyResSE, as well as the panel of drugs available in each tool. (DOCX 22 kb)\n\n\n\nAdditional file 4:\nFigure S1. Distribution of sample numbers in three TB databases. Number of isolates with available DST phenotypes for at least one drug as well as WGS data in the three TB databases PATRIC, ReSeqTB, and LitRev. (DOCX 2339 kb)\n\n\n\nAdditional file 5:\nComposition of the dataset analysed in this study. This file presents the details of the databases and studies from which data for the 6756\u2009TB isolates is obtained. (XLSX 115 kb)\n\n\n\nAdditional file 6:\nThe total number of samples available and the estimated rate of misclassification of phenotypic DST results for each drug. This file presents number of samples with resistance status among the 6756\u2009TB isolates for each drug and the estimated rate of misclassification estimated from the latent class model. (XLSX 9 kb)", "s12859-018-2571-x": "The main objectives of this study were sequencing, assembling, and annotation of chloroplast genome of one of the main Siberian boreal forest tree conifer species Siberian larch (Larix sibirica Ledeb.) and detection of polymorphic genetic markers \u2013 microsatellite loci or simple sequence repeats (SSRs) and single nucleotide polymorphisms (SNPs).We used the\u00a0data of the whole genome sequencing of three Siberian larch trees from different regions - the\u00a0Urals, Krasnoyarsk, and Khakassia, respectively. Sequence reads were obtained using the Illumina HiSeq2000 in the Laboratory of Forest Genomics at the Genome Research and Education Center of\u00a0the Siberian Federal University. The assembling was done using the Bowtie2 mapping program and the SPAdes genomic assembler. The genome annotation was performed using the RAST service. We used the GMATo program for the SSRs search, and the Bowtie2 and UGENE programs for the SNPs detection. Length of the assembled chloroplast genome was 122,561\u2009bp, which is similar to 122,474\u2009bp in the closely related European larch (Larix decidua Mill.). As a result of annotation and comparison of the data with the\u00a0existing data available only for three larch species - L. decidua, L. potaninii var. chinensis (complete genome 122,492\u2009bp), and L. occidentalis (partial genome of 119,680\u2009bp), we identified 110 genes, 34 of which represented tRNA, 4 rRNA, and 72 protein-coding genes. In total, 13 SNPs were detected; two of them were in the tRNA-Arg and Cell division protein FtsH genes, respectively. In addition, 23 SSR loci were identified.The complete chloroplast genome sequence was obtained for Siberian larch for the first time. The reference complete chloroplast genomes, such as one described here, would greatly help in the chloroplast resequencing and search for additional genetic markers using population samples. The results of this research will be useful for further phylogenetic and gene flow studies in conifers.The chloroplast genome in conifers, including larch species [1], has a unique, strictly paternal inheritance via pollen, unlike angiosperms, where it has a maternal inheritance via seeds [2]. It allows tracing paternal gene flow and lineages separately from maternal (mitochondrial genes) and bi-parental (nuclear genes) ones. Therefore, chloroplast DNA sequences are the most important source of genetic markers to study distribution of paternal genes and paternally based molecular phylogenetic relationships in conifers.Larch species, as well as many other conifer species are the main boreal forest tree species, which comprise ~\u200930% of the world\u2019s forested lands [3]. Boreal forests play a\u00a0very important ecological role, but are also affected by the global climate change. On one hand, they suffer now from more frequent and drastic droughts, but on the other hand, their area is expanding in the northern regions, and their tree line is moving towards the\u00a0north creating an ecotone, a highly dynamic transition area [4]. It is important to know how much of paternal associated gene flow by pollen contributes into establishing this zone, compared to the maternal and bi-parental contributions by seeds. Such studies require chloroplast markers. The next generation sequencing (NGS) technique allows whole chloroplast genome sequencing in multiple individuals and makes a search for molecular genetic markers more efficient. For instance, Parks et al. [5] sequenced chloroplast genomes in 37 pine species using NGS\u00a0nearly completely. They found a\u00a0significant amount of variation (especially in two loci ycf1 and ycf2) that provided them with additional data for inferring intrageneric phylogeny of genus Pinus.Whole chloroplast genome comparison across different species and genera allows also studying organelle evolution and how it is associated with speciation and dispersal. Complete chloroplast genome sequences are available in NCBI Genbank for multiple plant species, including conifers. However, most of them represent the Pinus genus, and only three chloroplast genomes are available for the Larix genus: complete -\u00a0for European (Larix decidua Mill.; AB501189.1) and Chinese (L. potaninii var. chinensis Beissn.; KX808508) larch and partial -\u00a0for Western larch (L. occidentalis Nutt.; FJ899578.1).Variation in the chloroplast genome is effectively used in phylogenetics at different levels. It allowed discriminating different subgenera and genera. For instance, Cronn et al. [6] compared chloroplast genome sequences of seven pine and one spruce species and found three regions that have deletions corresponding to the subgenera specific deletions in three genes: ycf12 (78\u2009bp at the nucleotide starting position 51,051), psaM (93\u2009bp at position 51,442), and ndhI (371\u2009bp at position 101,988), respectively. These are common deletions in the chloroplast genome in pine species of the subgenus Strobus (i.e., P. gerardiana, P. krempfii, P. lambertiana, P. longaeva, P. monophylla, P. nelsonii, P. koraiensis); the corresponding genes were present in the subgenus Pinus (P. contorta, P. ponderosa, P. thunbergii) and in spruce Picea sitchensis [6].Variation in the chloroplast genome can be also effectively used in discriminating different populations of the same species. For instance, Whittall et al. [7] demonstrated a strong differentiation between the\u00a0mainland and island populations of Torrey pine (Pinus torreyana) based on 5 SNPs found in the entire chloroplast genome of 120 Kbp.We used the\u00a0data of the whole genome sequencing of three Siberian larch trees generated by Illumina HiSeq2000 [8]. DNA samples were isolated from needles and haploid callus of three Siberian larch trees, representing different regions in Russia \u2013 the\u00a0Ural Mountains, Krasnoyarsk Region and Khakassia Republic, respectively. The\u00a0Larix decidua Mill. [9] and L. occidentalis Nutt. [5] chloroplast genomes were used as a\u00a0reference (NCBI Genbank accession numbers AB501189.1 and FJ899578.1, respectively). We did not use the chloroplast genome of L. potaninii [10] as a reference, because it was assembled by using the chloroplast genome of L. decidua (NC_016058; [9]) as a reference, but we used it in the comparative analysis. The paired-end (PE) and mate-pair (MP) libraries with fragment sizes of 400\u2013500\u2009bp (Ural and Krasnoyarsk trees) and 300\u2013400\u2009bp (Khakassia tree), respectively, were used for sequencing via 2\u2009\u00d7\u2009100\u2009cycles by Illumina HiSeq2000.The sequence reads were mapped to the reference chloroplast genomes using the Bowtie2 software [11], which is good for mapping short sequence reads to medium-sized and large genomes. This software implements an algorithm to derive the\u00a0FM-index based on the\u00a0Burrows-Wheeler Transform. The SPAdes genome assembler has been used to assemble the larch genome, which implements the De Bruijn graph approach [12]. The Rapid Annotation service with Subsystem Technology (RAST) has been used for annotation [13].The first step in our assembly procedure consisted of mapping short reads to the available chloroplast genome references of L. decidua and L. occidentalis using the Bowtie2 software. Then, the aligned reads were assembled by SPAdes. The obtained contigs were aligned again on the reference of L. decidua using BLAST. In\u00a0the third step, the selected contigs were verified to get the \u201ctrusted\u201d status. Then, the assembly was carried out using SPAdes. The final step of the assembly was scaffolding, which was done using the generated contigs and MP reads using the SSPACE program [14].Considering the\u00a0well-known fact that chloroplast organelle originated from cyanobacteria, and that, therefore, chloroplast genes are still very similar to the bacterial ones, the RAST service, which was designed for annotation of bacterial and archaeal genomes, was used for the larch genome annotation. The annotation obtained by the RAST contained both the\u00a0confirmed known genes and the\u00a0predicted genes, potentially coding hypothetical proteins. In order to clarify the roles of these hypothetical coding regions, our annotation was compared with the\u00a0annotations of two closely related species, L. decidua and L. occidentalis, respectively. In addition, some fragments of the genome have been also selectively aligned with BLAST. The sites of hypothetical proteins confirmed by BLAST were identified and recorded.The assembled chloroplast genome of L. sibirica has been deposited in the NCBI GenBank with the accession number NC_036811.1 and used as a reference to search for polymorphisms (SNPs and SSRs) among the\u00a0three above mentioned trees. SNPs were searched using the Bowtie2 and UGENE [15] software (option Call Variants with SAMtools). First, the\u00a0reads of the\u00a0Urals and Khakassian trees were mapped to the finally assembled genome of the Krasnoyarsk tree. Then, the resulting sam-file together with the assembled genome was used by the UGENE program to search for SNPs. The SSR loci were searched using GMATo [16] with a threshold of minimum 6 repeats for di-, tri-, tetra-, penta-, and hexanucleotide motifs, and 10 minimum repeats for mononucleotide motifs.The total length of the final Siberian larch chloroplast genome assembly was 122,560\u2009bp, which is very close to 122,474\u2009bp in the\u00a0closely related European larch (Larix decidua). The annotation through comparison with the\u00a0available data for L. decidua and L. occidentalis identified 110 genes, from which 34 represented tRNA genes, 4 rRNA, and 72 protein-coding genes. In three trees 13 SNPs were detected. Two of them were found in the coding regions of the tRNA-Arg and Cell division protein ycf2 genes.The chloroplast genome variation in most plants is often limited due to a relatively low frequency of mutations in this organelle. For example, the mutation rate of the chloroplast genome in pines is approximately 0.2\u20130.4\u2009\u00d7\u200910\u2212\u20099 synonymous substitutions per nucleotide per year [18, 19]. However, with an average length of 120\u2013160 Kbp and 130 genes, the\u00a0chloroplast genomes are sufficiently large and complex, and include structural and point mutations that reflect population differentiation and evolutionary divergence [6].Unlike angiosperms, the\u00a0conifer chloroplast DNA (cpDNA) lacks large inverted repeats (IR), but contains dispersed repetitive DNA that is associated with structural rearrangements. In addition to large dispersed repeated sequences, the\u00a0conifer cpDNA also possesses a number of small repeats. It contains variable numbers of tandem repeats of 124 to 150\u2009bp in size, which are associated with the\u00a0polymorphic rearranged region near trnK-psbA, where the psbA gene has been duplicated [20].Most variation in the chloroplast genome is associated with microsatellite loci [21, 22]. However, these markers have a too high mutation rate that can lead to incorrect phylogenetic inferences [23\u201325]. SNPs could be better markers for phylogenetic inferences, and comparative complete chloroplast genome studies are needed to discover these markers. The reference complete chloroplast genomes, such as the\u00a0one described here, would greatly help in chloroplast resequencing and search for SNPs using population samples.The complete chloroplast genome sequence was obtained for Siberian larch for the first time. Annotation and comparison of the obtained data with data available only for two other larch species helped us identify and verify 110 coding regions representing 38 RNA and 72 protein genes. The total of\u00a013 SNPs were detected; two of them were in the coding regions of the genome. The results of this research will be useful for further phylogenetic and gene flow studies in conifers.The presented study was a part of the project \u201cGenomic studies of major boreal coniferous forest tree species and their most dangerous pathogens in the Russian Federation\u201d funded by the Government of the Russian Federation (grant \u2116 14.Y26.31.0004). Publication costs are funded by the BioMed Central Membership of the University of G\u00f6ttingen.The annotated chloroplast genome of L. sibirica has been deposited in the NCBI GenBank with the accession number NC_036811.1.This article has been published as part of BMC Bioinformatics Volume 20 Supplement 1, 2019: Selected articles from BGRS\\SB-2018: bioinformatics. The full contents of the supplement are available online at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-1.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.We used the\u00a0available software, such as Bowtie2, BLAST, and SPAdes to assemble the\u00a0chloroplast genome using the\u00a0reads generated in the whole genome sequencing of the\u00a0Siberian larch project. We used SSPACE for scaffolding and the RAST service for annotation of the\u00a0obtained chloroplast genome. We developed a procedure that allowed us to successfully extract chloroplast genome specific reads and then assemble and annotate the resulting sequences. We identified and verified 110 coding regions representing 38 RNA and 72 protein genes, which is equal to the number of genes in chloroplast sequences of L. decidua and L. potaninii and close to 105 genes in the\u00a0partial chloroplast genome sequence of L. occidentalis. A gene map of the genome was generated using OGDRAW [17] and is\u00a0presented in Fig.\u00a01. The search for SNPs using UGENE revealed a relatively small number of SNPs (Fig.\u00a02; Additional\u00a0file\u00a01), but it is only preliminary data based on a limited sample size. In addition, 23 SSR loci (16 with mono- and 7 with dinucleotide repeat motifs) were identified in the chloroplast genome (Additional\u00a0file\u00a02). No SSR loci with tri-, tetra-, penta-, and hexanucleotide repeats were found with the search parameters used.\n\nFig. 1\nGene map of the Larix sibirica chloroplast genome. Genes belonging to different functional groups are color-coded. The dark and light grey in the inner circle represents the GC and AT content, respectively\n\n\nFig. 2\nVariation detected in the Larix sibirica chloroplast genome (see also Additional file 1)\n\n\nAdditional file 1:\nTable S1. Genetic variants identified in the Siberian larch chloroplast genome. (XLSX 24 kb)\n\n\n\nAdditional file 2:\nTable S2. Chloroplast microsatellite (SSR) identified in the Siberian larch chloroplast genome. (DOCX 14 kb)", "s12859-018-2549-8": "Functional modules in protein-protein interaction networks (PPIN) are defined by maximal sets of functionally associated proteins and are vital to understanding cellular mechanisms and identifying disease associated proteins. Topological modules of the human proteome have been shown to be related to functional modules of PPIN. However, the effects of the weights of interactions between protein pairs and the integration of physical (direct) interactions with functional (indirect expression-based) interactions have not been investigated in the detection of functional modules of the human proteome.We investigated functional homogeneity and specificity of topological modules of the human proteome and validated them with known biological and disease pathways. Specifically, we determined the effects on functional homogeneity and heterogeneity of topological modules (i) with both physical and functional protein-protein interactions; and (ii) with incorporation of functional similarities between proteins as weights of interactions. With functional enrichment analyses and a novel measure for functional specificity, we evaluated functional relevance and specificity of topological modules of the human proteome.The topological modules ranked using specificity scores show high enrichment with gene sets of known functions. Physical interactions in PPIN contribute to high specificity of the topological modules of the human proteome whereas functional interactions contribute to high homogeneity of the modules. Weighted networks result in more number of topological modules but did not affect their functional propensity. Modules of human proteome are more homogeneous for molecular functions than biological processes.Even after decades of research in the field of human genes, gene products and functions, understanding of genotype-phenotype relationship is far from complete. Biomolecules (genes, RNA, proteins, metabolites) interact with each other and environmental factors in order to accomplish various biological processes. Representing these interactions as biological networks (metabolic, protein-protein interactions, gene regulatory, co-expression) and their analyses provide insights in finding genes associated with cellular processes such as immune response, signalling pathways or with a complex disease like cancer [1].Protein-protein interaction networks (PPIN) like most biological networks are believed to be modular in nature [4, 9, 10] and detecting functional modules of PPIN are vital for understanding gene-function associations and designing therapeutics. Topological modules are sub-networks where nodes within a module have dense connections as compared to the nodes of the other modules [11]. Functional module, on the other hand, is a sub-network that contribute to similar biological functions [4, 9]. Computational methods accurately inferring functional and disease modules of the human proteome would be of paramount importance for studying cellular and disease mechanisms.Numerous computational algorithms have been attempted on biological networks in order to identify modules by using networks\u2019 topological properties based on node neighbours [12], edge weights [13] and modularity [14, 15]. Other sub-network identifying algorithms including those finding core and loop structures [16, 17], cliques [18] and frequent graph patterns [19] have also been attempted to find topological modules in biological networks. However, only a few studies have compared their functional properties and their relevance to functional modules [20\u201322]. Usual approach to evaluate the functional significance of topological modules is to perform functional enrichment analysis and decide on the significantly enriched biological functions [21, 23, 24]. This approach is however inconclusive of determining functional coherence and specificity of topological modules [25]. In present work, we introduce a novel functional specificity measure that encompasses both functional homogeneity and heterogeneity of the topological modules. Top ranked topological modules are thereby identified and validated for their functional specificity.We combine functional interactions inferred from expression data [26, 27] and physical interactions of PPIN [6, 16] to provide holistic functional attributes to protein nodes and interactions of the network for the determination of functional modules [28\u201330]. Though several studies have reported characteristics of resulting modules of different biological networks [13, 17, 21], there is a need of a systematic study elucidating the effects of using both functional and physical interactions of PPIN on detecting topological and functional modules. Previously, Theofilatos et al. and Lubovac et al. have applied weighted PPIN to predict protein complexes using a Markov clustering based approach and ranking measure on the basis of weighted neighborhood property, respectively [31, 32]. But here we investigate the role of edge weights incorporated from gene functional similarities in the modular detection of PPIN.Our contributions in this study are (i) evaluation of functional coherence and specificity of the topological modules of the human proteome by using novel measures, (ii) determination of the effect of using both direct physical and indirect functional links of PPIN on detection of functional modules, and (iii) systematic analysis of incorporating functional context of interactions as edge weights using functional similarities of genes. We have used three different PPIN datasets of the human proteome and Louvain community detection algorithm [14] for modular detection. The weighted PPIN were generated by calculating functional similarity between interacting proteins by using molecular functions, biological processes and cellular components of Gene Ontology (GO) [33]. We also elaborate on how physical and functional interactions between proteins affect functional diversity of topological modules.PPIN like other biological networks such as metabolic and gene-regulatory networks are characterised by specific interactions between proteins (nodes) and functions of proteins and therefore demonstrate small world properties (i.e., short path length) and scale free characteristics (i.e., few nodes with large number of neighbours) (Tables\u00a01 and 2).Topological modules of binary and weighted PPIN were detected using Louvain algorithm and analysed to investigate how (i) different interactions (physical and functional) and (ii) different biological contexts (i.e., MF, BP and CC ontologies) affect the functional properties of the modules.Functional homogeneity of a module quantifies functional consistency of a topological module as defined by the maximal fraction of proteins associated with a biological function. The homogeneity ranges from 0 to 1 where a value of 1 indicates that all genes in the module exhibit that function. A module\u2019s heterogeneity value estimates how specific a function is for a particular module.Modularity-based algorithms for module detection often suffer from resolution limit [39] as the scale of modularization depends upon the inter-connectedness of the modules. This leads to the inability to detect smaller modules in a given network. To study the effect of resolution limit in detecting topological modules, we also implemented the Incremental Louvain algorithm [35], which first finds modules by maximizing modularity while incrementally modularizing larger modules into smaller sub-networks, thus converging the algorithm for modules with size greater than a threshold size.The three different PPIN (physical, functional and combined) were modularized and their functional relevance was analysed using functional enrichment analysis. As observed from Table 1, physical PPIN are sparser (have high average path length and low edge density) than functional PPIN, resulting due to high number of functional interactions and noise in the gene expression experiments. For weighted networks (Table 2), the edges with low functional similarity between proteins reduce the average path length to lower values than binary PPIN (ranges from 0.2 to 1.5 as compared to 3.1 to 6.9). There is a high overlap between functional and physical PPIN with 9069 common nodes between the two, underlining that most physical interactions also exert functional interactions. However, small amount of non-overlapping edges between physical and functional PPIN suffices to cause changes in edge density and clustering coefficient for the combined network.When modularized using Louvain algorithm, size distribution of topological modules in three PPIN (Figs. 2, 3, 4 and Table\u00a03) shows that weighting interactions with functional similarities of proteins removes weak protein-protein interactions in PPIN and leads to higher number of compact modules.Figure 5 (and Additional file 1: Figure S2) shows the functionally enriched GO terms in the PPIN modules. The number of enriched cellular functions and processes are observed to be higher for the weighted PPIN despite the smaller size of the modules. The number of cellular locations decreases however with the inclusion of weights of protein interactions. Overall, combined PPIN are enriched by more GO terms, with biological processes approximately 1.5 to 3 times more, molecular functions up to 3 times more and cellular locations approximately 1.4 times more, than those in physical and functional binary PPIN.Functional homogeneity analysis (Tables\u00a04 and 5) shows Physical PPIN modules to be more specific than functional networks, in case of molecular functions as compared to bioprocesses and cellular localizations. Overall, homogeneity and heterogeneity values are not much different when weighted interactions are considered, indicating that topological modules are more resilient to edge weights when functional annotations are considered. We also conclude that topological modules in PPIN are more homogeneous and specific in molecular functions, and less homogeneous (diverse) in terms of biological processes. This is in agreement with the fact that a biological process may involve multiple sets of molecular functions and thus functional modules map to a number of molecular functions but less number of biological processes. Most importantly, the results indicate that the functional modules are observed to be more homogenous and specific when direct interactions in PPIN are also considered (as seen in the combined network), a fact to kept in mind when identifying biologically relevant modules by using computational methods. To study the effect of resolution limit on functional properties of modules, three PPIN were modularized using Incremental Louvain Algorithm that resulted in modules, eight times more in number but smaller in size than Louvain (Additional file 1: Figure S4). Despite the differences, enrichment analyses of modules from both type of algorithms show that physical networks are more specific than functional ones (see Tables 6 and 7). Thus topological modules are more specific and homogeneous when direct interactions are considered with indirect functional associations (such as derived from co-expression or microarray based experiments).A specificity score is introduced in this study that considers both functional homogeneity and heterogeneity of a module. Topological modules with specificity score greater than 0.90 were labelled functionally specific modules and the others as general modules. Table 8 shows that physical PPIN modules are more enriched in specific modules than functional PPIN. As seen in Fig. 6 and Additional file 1: Figure S3, the modules appear to become smaller and the biological functions re-distributed into more number of highly specific modules when edge weights are introduced to physical and functional protein interaction networks. However, combining functional interactions with physical interactions led to formation of few and larger specific modules. This limitation due to increasing module size can be handled by optimizing modularizing algorithm for detecting smaller modules of high functional specificity in future and is beyond the scope of present study.Biological relevance of top ranked specific modules in physical, functional and combined PPIN was evaluated on the basis of their enrichment with genes from experimentally known biological pathways such as glycolysis, transcriptional regulation, lung cancer and breast cancer. As shown in Fig. 7, specific modules are overall found to be more enriched than general modules for all four biological pathways, but the specific modules from binary PPIN were observed to be highly enriched than those of weighted PPIN. This indicates that the specific modules obtained by using specificity scores of enriched functions are highly enriched with known functional and disease pathways. However, inclusion of weights did not improve the enrichment of biological and disease pathways in physical and functional networks.We systematically analysed functional properties of topological modules in human proteome and investigated the effect of physical and functional interactions in PPIN on functional specificity of modules. We also studied the contribution of weighting edges with functional similarities on topological modules. A specificity score was introduced to identify more accurate biologically relevant and specific modules. Functionally homogeneity was earlier used to evaluate functional value of topological modules detected in biological networks [24, 25] but failed to consider the heterogeneity of functional modules of biological networks due to a protein or gene mapping to a number of cellular processes. Thus, a set of proteins (in a module) are involved in more than one function and also a biological function is mapped to more than one module. In order to handle this, functional specificity was introduced which considers both functional homogeneity within the module and functional heterogeneity across the modules. The function specificity helps in identifying functional modules or specific functions of topological modules and one may use our methods to confidently map specific functions to topological modules of PPIN.The topological modules detected using physical, functional and combined PPIN are found to be homogeneous, highly specific, and enriched in a number of significant biological functions, processes and cellular localizations (Fig.\u00a05 and Additional file 1: Figure S2). Though weighted edges do not affect the homogeneity and heterogeneity of the modules, incorporating functional similarities of edge do help in identifying compact and highly specific functional modules based on topological properties.Functional or indirect interactions are generally noisy as they are determined using statistical inferences from gene expressions based experiments and vary on tissue and patient sample basis [40]. But functional interactions encompass the whole interaction profile of genes involved in a cellular function or a disease and thus important for systematic analysis and prediction of functional modules. Present study provides a first hand insight into the effect of these different type of protein-protein interactions on topological modules of human proteome. We conclude that instead of using only co-expression based networks in identifying functionally relevant topological modules, one should combine the accuracy of physical interactions with the larger coverage of interactome landscape by functional networks. Though our methodology provides an edge over usual methods (like homogeneity, GO enrichment) for functional validation of topological modules and helps in identifying specific functions of these modules, it does not identify core components of a biological pathway. One limitation of our study is that our methods do not handle the overlapping modules and consider overlapping properties of functional modules. It would be interesting to study overlapping sub-modules, core modules, and the hierarchical organization of functionally specific topological modules as future work of this study.Physical PPIN enlists curated binary interactions of proteins, representing physical or direct interactions that are determined using in vivo (e.g. co-immunoprecipitation), in vitro (e.g. GST pull-down assays) or yeast two-hybrid experiments.Functional PPIN represents functional interactions of proteins, i.e., these proteins may or may not physically interact but they do participate in a biological function by influencing each other genetically through co-regulation or co-expression, which are determined using experimental techniques like microarray expression data analysis or double mutant analysis.Combined PPIN is the inclusive set of both the physical and functional networks mentioned above.Weighted PPIN are obtained by assigning functional similarities between proteins as edge weights, considering different GO domains: molecular function (MF), biological process (BP) and cellular component (CC). We used popular Wang\u2019s semantic similarity measure [34, 45] to evaluate the functional similarity between genes (i.e., weights of protein-protein interactions).The functional enrichment analysis was performed in order to find the GO terms in MF, BP, and CC contexts, which are significantly represented (enriched) by the proteins in the predicted topological modules. The functional enrichment analysis was implemented using R package BioStats [47]. The statistical significance of a GO term in a module was estimated by evaluating its overrepresentation using a hypergeometric test. A functionally enriched module signifies that the number of genes observed to be annotated with a function (i.e., the GO term) is more than the expected number of genes annotated to that function. The \u2018expected value\u2019 for a function is the number of genes having that specific function in the given module, with respect to the reference list (whole list of human genes).In this section, we introduce measures to quantify functional homogeneity and heterogeneity of topological modules of PPIN. First, functional enrichment analysis is performed on the modules to identify biological functions (GO terms) that are significantly enriched (p-value <\u20090.0001) in the modules and the functions are ranked according to their significance values. Systematic estimation of p-value is done using a set of detailed experiments explained in Additional file 1. We selected the enriched functions for each module and identified the set F of enriched functions in all the modules.The values of specificity scores across all enriched functions are normalized to a range between 0 and 1. The functional specificity value measures how exclusively the module is enriched by the specific biological function. Modules are ranked using the functional specificity score and the top ranked modules are considered as highly specific modules.Biological ProcessCombinedCellular ComponentFunctionalGene ontologyMolecular functionPhysicalProtein-protein interactionsProtein-protein interaction networksThis research and publications costs are supported by Tier-2 MOE2016-T2\u20131-029 grant by the Ministry of Education, Singapore.The dataset used in this paper can be downloaded from https://github.com/ramakaalia/moduledetectionPPIN.This article has been published as part of BMC Bioinformatics, Volume 19 Supplement 13, 2018: 17th International Conference on Bioinformatics (InCoB 2018): bioinformatics. The full contents of the supplement are available at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-19-supplement-13.Not applicable.Not applicable.The authors declare that they have no competing interests.Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Currently, 20,231 proteins of the human proteome have been identified [2] but the landscape of their interactions is only partially known. Protein interactions may be physical when their amino acid residues physically interact through electrostatic forces like hydrophobic or functional interactions when a protein influences the activity of another protein through regulation, co-expression, or some other genetic interaction [3, 4] (Fig.\u00a01). Large scale experiments like yeast two-hybrid and affinity purification coupled to mass spectrometry identify physical protein interactions [5, 6] while high throughput expression techniques like microarray and RNA-seq elucidate functional links between proteins [7, 8].\n\nFig. 1\nIllustrations of physical, functional and combined protein-protein interaction networks (PPIN)\nThe present study considers three types of human PPIN based on physical, functional, and combined interactions as given in Table\u00a01. The strengths or weights of protein-protein interactions with respect to their functional context (MF, BP and CC) are calculated from functional similarities of respective GO context, using Wang measure [34]. This led to nine sets of weighted PPIN and their network properties are listed in Table\u00a02.Table 1\nProperties of different binary PPIN: physical (P), functional (F), and combined (C)\nTable 2\nProperties of weighted PPIN: physical (P), functional (F) and combined (C) PPIN weighted by functional contexts: molecular function (MF), biological process (BF) or cellular components (CC)\nAs shown in Table 3, the number of modules predicted for different networks vary considerably although the modularity values remain almost the same. We note that the number of modules predicted for weighted networks (1586 to 2912) is much more than that of binary networks (34 to 64), but only 0.3 to 1.2% of these modules are mesoscale (size>\u200910) as compared to 20\u201327% of binary networks. A closer inspection of Figs.\u00a02, 3 and 4 finds that most of the modules are of size two, corresponding to isolated protein pairs whose interactions with others is yet be known or weak.Table 3\nProperties of topological modules of different PPIN\n\naMesoscale modules refer to the modules with size more than 10\n\n\nFig. 2\nSize distributions for modules detected using Louvain algorithm in physical networks of human proteome: x-axis represents the size of modules while y-axis represents the count of meso-modules of size more than 10 nodes. P denotes the binary physical network while P-MF, P-BP and P-CC denote the weighted networks with edges scored according to functional similarity based on molecular functions (MF), biological process (BP) and cellular component (CC), respectively\n\n\nFig. 3\nSize distributions of modules detected using Louvain algorithm in functional networks of human proteome. x-axis represents the size of modules while y-axis represents the count of meso-modules of size more than 10 nodes. F denotes the binary functional network while F-MF, F-BP and F-CC denote the weighted networks with edges scored according to similarity based on molecular functions (MF), biological process (BP), and cellular component (CC), respectively\n\n\nFig. 4\nSize distributions for modules detected using Louvain algorithm in combined networks (physical and functional) of human proteome. x-axis represents the size of modules while y-axis represents the count of meso-modules of size more than 10 nodes. C denotes the combined physical network while C-MF, C-BP and C-CC denote the weighted networks where edges scored according to similarity based on molecular functions, biological process (BP), and cellular components, respectively\nMore importantly, proteins in topological modules ought to share the same functional profile. To study functional relevance of topological modules in the human proteome, mesoscale modules from all networks were tested for their biological relevance by using functional enrichment analysis. The enriched function set F is given by the union of all significantly enriched functions across topological modules and functional specificities of the set of enriched functions were computed for each PPIN. Figure\u00a05 (and Additional file 1: Figure S2) shows the distribution of significantly enriched biological functions and size of topological modules of binary and weighted physical PPIN.\n\nFig. 5\nFunctional enrichment analyses of topological modules: (a) and (b) show distributions of enriched molecular functions in topological modules of PPIN networks. X-axis, Y-axis (left) and Y-axis (right) represent the modules, number of statistically significant GO terms, and size of modules, respectively. See Additional file 1: Figure S2 for the set of enriched biological processes and cellular locations in the modules\nA recent study of human proteome [35] discussed how most of the topological modules are functionally diverse despite high homogeneity values. In our study, we further this observation by including functional interactions and incorporating the weights to PPIN. As shown in Table\u00a04, the MF and BP homogeneity values are observed to be higher (0.79 and 0.59) for physical networks than functional networks (0.64 and 0.57) whereas cellular localizations (~\u20090.7) do not vary much across different networks. We conclude that functional interactions lead to low homogeneity values in networks because they mostly represent cross talks between modules with not much variations in cellular localizations. For example, cross talks in TGF-beta signalling is known to be involved in many developmental defects and cancer [36]. This observation concurs with homogeneity values derived in gene-disease associations (a type of functional interactions) in disease networks [24, 37].Table 4\nFunctional homogeneity of mesoscale modules detected by Louvain algorithm, evaluated using three ontologies: MF, BP, and CC\nTable\u00a05 shows heterogeneity values for enriched functions of the modules. On average, molecular function homogeneity was observed to be higher than bioprocess homogeneity for physical (0.80\u2009>\u20090.42) and combined (0.72\u2009>\u20090.45) networks except for functional networks (0.42\u2009<\u20090.60). But homogeneity and heterogeneity values are more varied (high standard deviation) for functional PPIN than physical and combined. Thus, it is advantageous to integrate physical protein interactions with expression based networks for functional analyses as attempted in some reported studies [29, 38].Table 5\nFunctional heterogeneity of modules detected by Louvain algorithm, calculated for all the enriched functions\nHere, we observed on average eight times more mesoscale modules as compared to the Louvain algorithm, the majority of modules being smaller in the size range of 10 to 200 (Additional file 1: Figure S4). In case of smaller modules detected using Incremental Louvain algorithm, an increase in the homogeneity values is observed when indirect functional interactions are combined with physical PPI (Tables\u00a06 and 7). While functional homogeneities of modules detected with the Louvain algorithm decreased when functional interactions are introduced into PPI network. This phenomenon can be simply attributed to difference in module sizes. When compared with respect to three ontologies, the homogeneity of modules shows on average 3.4% decrease for MF, 47.08% increase for BP and 4.6% decrease in CC. And heterogeneity values showed large percentage of decrease for these smaller modules (85.1, 78.9 and 87% decrease in MF, BP and CC, respectively). Weighting interactions in PPI network improves homogeneity of these modules but no change in heterogeneity values is observed.Table 6\nFunctional homogeneity of mesoscale modules detected by Incremental Louvain algorithm, evaluated using three ontologies: MF, BP, and CC\nTable 7\nFunctional heterogeneity of the modules detected by Incremental Louvain algorithm, calculated for all the enriched functions\nThe specificity of a particular function takes both its homogeneity within the module and its diversity across the modules into account. The normalized specificity scores for all significantly enriched functions across modules are summarized in Fig.\u00a06. As seen from the patterns of homogeneity and heterogeneity values, physical PPIN produce more functionally specific modules (highly homogenous and less diverse) than functional and combined PPIN, underscoring the benefit of including proteomics while analysing expression based networks in the identification of functional modules.\n\nFig. 6\nFunctional specificity of significantly enriched molecular functions of topological modules. See Additional file 1: Figure S3 for specificity scores of topological modules for BP and CC enrichment\nTopological modules were ranked using the specificity score and we labelled the modules with normalized specificity greater than 0.90 as functionally specific modules and the others as general modules. Table\u00a08 summarizes the biological functions and Table\u00a09 enlists enriched biological pathways of specific modules. Main functions specific to the modules were enzymatic activities like kinase, hydrolase, and transferase; and protein and nucleotide binding activities. About 36 to 55% of topological modules in binary and 14 to 32% of in weighted networks were classified as specific modules according to above mentioned criteria. More number of modules are found to be functionally specific (55%) in physical PPIN as compared to functional and combined PPIN (Table 8). This is in agreement with the effect of heterogeneity and homogeneity values of physical networks. This maybe imparted to the fact that direct interaction between proteins which are elucidated through high throughput screening experiments [6, 40] are more often studied and more popularly annotated with functions and that gene-function associations as annotation based functional enrichment analysis are affected by missing annotations. See Additional file 1: Tables S1 and S2 for specific modules enriched in biological processes and cellular locations.Table 8\nThe percentage (%), mean size, and summary of molecular functions of the specific modules of physical (P), functional (F) and combined (C) PPIN\nTable 9\nThe top enriched protein pathways in the specific modules of physical (P), functional (F) and combined (C) PPIN. Pathways are mapped using PANTHER Pathway database (http://www.pantherdb.org/pathway/)\nTo validate biological relevance of top ranked specific modules, their enrichment with genes from experimentally known biological pathways was computed. Four gene sets of known pathways were considered: glycolysis, transcriptional regulation, lung cancer and breast cancer, and their details [40\u201342] are given in Table\u00a010. Breast and lung cancer pathway set has a total 363 and 300 genes, out of which 347 and 286 are present in the physical, 260 and 219 in the functional and 349 and 288 in the combined PPIN. Out of 244 genes from glycolysis pathway, 158 are present in the physical, 187 in the functional and 226 in the combined PPIN.Table 10\nDetails of biological pathways used for validation of functional modules\nThe overlapped fractions of genes of known pathways to those in specific and general modules were calculated in order to estimate validity of the topological modules. As shown in Fig.\u00a07, specific modules from binary combined PPIN retrieved ~\u200979% of breast and lung cancer genes as compared to 43\u201345% by modules of weighted PPIN. In a similar fashion, for physical and functional PPIN, specific modules of binary PPIN were enriched with more cancer pathway genes (69 and 89% for breast cancer, 69 and 85% for lung cancer) than respective modules from weighted PPIN (56 and 45% for breast cancer, ~\u200949% for lung cancer). Specific modules of binary networks were also highly enriched with 70, 90, and 76% of glycolysis genes and 71, 87 and 77% of transcriptional regulation genes in physical, functional and combined networks, respectively.\n\nFig. 7\nOverlap of specific topological modules (specificity score\u2009>\u20090.9) and general topological modules (specificity score\u2009<\u20090.9) with experimentally known biological pathways: glycolysis, transcriptional regulation, lung cancer and breast cancer. Bars represent overlap of genes involved in biologically validated pathways with specific modules (brown colour) and general modules (green colour). Topological modules are detected via molecular function enrichment for binary and weighted physical(P), functional(F) and combined (C) PPINs\nIn a cellular machinery, proteins function as enzymes, transcription factors, receptors or structural proteins, and interact with other biomolecules. Protein interactions are either physical (direct) or functional (indirect). For studying the role of these two types of interactions on detection of modules of PPIN, three datasets were used: Physical, Functional and Combined (see Fig. 1, Table 1). These three datasets were prepared from HPRD (Human Protein Reference Database) (version Release9) [43] and STRING database (version 10) [44]; and include experimental information from other well-known databases like BIND, DIP, GRID, HPRD, IntAct, MINT and PID (updated till 14 May 2017). All the proteins were mapped to their Entrez gene ids. Details of data pre-processing are provided in Additional file 1.1.\nPhysical PPIN enlists curated binary interactions of proteins, representing physical or direct interactions that are determined using in vivo (e.g. co-immunoprecipitation), in vitro (e.g. GST pull-down assays) or yeast two-hybrid experiments.\n\u00a02.\nFunctional PPIN represents functional interactions of proteins, i.e., these proteins may or may not physically interact but they do participate in a biological function by influencing each other genetically through co-regulation or co-expression, which are determined using experimental techniques like microarray expression data analysis or double mutant analysis.\n\u00a03.\nCombined PPIN is the inclusive set of both the physical and functional networks mentioned above.\n\u00a0Functional modules of PPIN correspond to communities or sub-networks of proteins having specific and similar biological functions [4, 46]. We chose the Louvain algorithm modular detection algorithm to find topological modules of PPIN because it has demonstrated excellent performance and low computational complexity on benchmark networks [20] (Lancichinetti & Fortunato, 2009). The Louvain algorithm finds the community or modular structure by optimizing the modularity Q (the quality function) of the network: (1)where eij\u00a0is fraction of edges between modules i and j, and\u00a0ai is the fraction of edges connected to the nodes in module i. The modular structure is found by maximizing the modularity in an iterative manner. All the nodes in the network are assigned to independent modules in the beginning and the algorithm progressively merges two communities that best increase the modularity of the resulting network structure. Merging of nodes and modules continues until there is no further increase in the modularity of the network.Homogeneity of a module with respect to a particular function is computed by the proportion of genes annotated by the function. That is, the homogeneity of a function f\u2009\u2208\u2009F within a module is given bywhere nf is the number of genes annotated by the function and N is the total number of genes in the module. The functional homogeneity (H) of a module is defined as the homogeneity of maximally enriched function in the module. The heterogeneity of a function is defined as the proportion of the modules where the function f\u2009\u2208\u2009F is enriched. That is,where kf is the number of modules enriched with function f and K is the total number of modules detected in PPIN.Functional homogeneity measures functional coherence of the modules while functional heterogeneity indicates how exclusive the modules are for the function across all predicted modules. To combine functional homogeneity and heterogeneity of a module, functional specificity for an enriched function is defined as follows: (2)\n\nAdditional file 1:\nSupplementary information. (PDF 1521 kb)"}