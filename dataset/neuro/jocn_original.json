{"s10827-008-0095-5": "The impact of structure in modulating synaptic signals originating in dendrites is widely recognized. In this study, we focused on the impact of dendrite morphology on a local spike generating mechanism which has been implicated in hormone secretion, the after depolarization potential (ADP). Using multi-compartmental models of hypothalamic GnRH neurons, we systematically truncated dendrite length and determined the consequence on ADP amplitude and repetitive firing. Decreasing the length of the dendrite significantly increased the amplitude of the ADP and increased repetitive firing. These effects were observed in dendrites both with and without active conductances suggesting they largely reflect passive characteristics of the dendrite. In order to test the findings of the model, we performed whole-cell recordings in GnRH neurons and elicited ADPs using current injection. During recordings, neurons were filled with biocytin so that we could determine dendritic and total projection (dendrite plus axon) length. Neurons exhibited ADPs and increasing ADP amplitude was associated with decreasing dendrite length, in keeping with the predictions of the models. Thus, despite the relatively simple morphology of the GnRH neuron\u2019s dendrite, it can still exert a substantial impact on the final neuronal output.The activity of hypothalamic GnRH neurons underlies the intermittent release of the decapeptide gonadotropin-releasing hormone (GnRH) that is required for sexual reproduction. Approximately every hour, a bolus of GnRH is released from the GnRH neurons. The neuropeptide is conveyed to the anterior pituitary through the hypophysial portal system, where it stimulates gonadotropin secretion. Thus, the intermittent profile of luteinizing hormone (LH) detected in peripheral blood is a direct consequence of the output of the hypothalamic GnRH oscillator (Cardenas et al. ).The release of GnRH has been proposed to reflect integration of intrinsic properties of and synaptic input to GnRH neurons (Kusano et al. ). In the present study, we focused on a key mode of activity that has been implicated in repetitive firing in magnocellular neurosecretory cells (MNC), the neurons that release the hormones oxytocin and vasopressin from their nerve terminals which form the posterior pituitary. In MNC, a brief pulse of injected depolarizing current initiates protracted periods of repetitive firing. Sustained firing is driven through the generation of after-depolarization potentials (ADPs). An after-depolarization potential is a state of a cell where the membrane potential remains depolarized above the resting membrane potential for some period after the firing of an action potential. These ADPs can sum to form plateau potentials, which leave the membrane depolarized to a level much higher than that from a single ADP. The sustained depolarization of the plateau potential can provide a continuing depolarizing stimulus (Andrew and Dudek ). Thus, in MNC summation of ADPs is a robust mechanism which contributes to the repetitive firing that underlies secretion of vasopressin (Legendre and Poulain ) from the posterior pituitary.With respect to control of anterior pituitary hormone secretion, growth hormone releasing hormone neurons also exhibit repetitive firing in response to short current injections (Balthasar et al. ). Likewise, acutely cultured GnRH somata that have been isolated from immature GnRH neurons in slices can express relatively large ADPs and their summation can lead to sustained, repetitive firing (Kuehl-Kovarik et al. ). ADPs have also been observed in GnRH neurons in brain slices (Sim et al. ; Chu and Moenter ). However, the ADPs observed after single action potentials in slices are significantly smaller in amplitude (2.9\u2009\u00b1\u20090.1\u00a0mV (Chu and Moenter ))than those observed in acutely dispersed GnRH somata (9.1\u00a0mV (Kuehl-Kovarik et al. )).One of the principle differences between cultured GnRH neurons and GnRH neurons in slices is the relative preservation of the dendritic structure in slices. Acutely cultured somata have rudimentary dendrites, with typical lengths of approximately 30 micrometers (\u03bcm) (Kuehl-Kovarik et al. , ). In contrast, the dendritic structure of GnRH neurons in slices is at least partially intact. Reconstructions of biocytin-filled GnRH neurons from coronal and hemisagittal slices have shown dendrites with average lengths of 300\u2013500\u00a0\u03bcm, with some dendrites in excess of 1 mm (Campbell et al. ; Roberts et al. ).The difference in magnitude of ADPs between cultured GnRH somata and GnRH neurons in brain slices is interesting given the observation that in multiple populations of neurons across many species, the structural and active electrical properties of dendrites have been shown to influence the activity of neurons. Modes of activity such as local spike initiation have been shown to occur in dendrites of hippocampal pyramidal neurons (Williams and Stuart ). Localized action potential generation via AMPA receptor activation has been described in CA1 Pyramidal cell basal dendrites (Schiller and Schiller ). In MNC, a correlation between physiologically modulated decreases in total dendrite length, and the increases in firing rate associated with lactation have been observed (Stern and Armstrong ). This finding may be due in part to ADPs, since the incidence of ADPs in oxytocin neurons is increased during lactation when dendrites are shorter (Stern and Armstrong ; Teruyama and Armstrong ).The persistence of ADPs in acutely dispersed GnRH somata indicate that they are most likely generated in the proximal regions of the GnRH neuron (e.g., somata or within the first 50\u00a0\u03bcm of the dendrite). Nonetheless, these locally generated ADPs could be influenced by the dendrite in the morphologically intact GnRH neurons. For example, the extended dendrite of the intact GnRH neuron could contribute a capacitive electrical load which would minimize ADPs. Alternatively, the absence of the dendrite might eliminate hyperpolarizing conductances, such as the passive leak, and active voltage-gated K+ currents. Either or both of these influences could directly impact on the magnitude of locally generated ADPs.Based on the findings in MNC, we postulated that dendrite length could underlie the difference between ADPs in GnRH neurons in slices and in cultured GnRH somata. Thus, we systematically explored the effects of truncating the dendrites of model GnRH neurons with both active and passive dendrites. We then examined the impact of dendrite length on ADP amplitude in living GnRH neurons.The details of constructing our compartmental models has been previously described (Roberts et al.  and see Results section). To test the model predictions, hypothalamic slices (300\u00a0\u03bcm) were prepared in the coronal orientation using a vibrating microtome (HM 650\u00a0V; Richard Harlan) from female mice in which GnRH neurons express green fluorescent protein (GFP; Spergel et al. ). Mice had an average age of 83.7\u2009\u00b1\u20092.4 days (n\u2009=\u200910), and an average weight of 19.1\u2009\u00b1\u20090.40\u00a0grams.Prior to experiments, female mice had been ovariectomized and treated for at least 48 hours (49.6\u2009\u00b1\u20092.8 hours) with a silastic implant containing 0.625\u00a0\u03bcg of estradiol, an approach which has been reported to increase ADP amplitude (Chu and Moenter ). Moreover, recordings were performed between 1600 and 0100 hours since an earlier study indicated that ADPs were more robust during the evening hours (Chu and Moenter ). The Institutional Animal Care and Use Committee of the University of Texas at San Antonio approved all procedures.Mice were anesthetized with isoflurane and decapitated. Brains were quickly removed and placed in cold (1\u20132\u00b0C), artificial cerebrospinal fluid (ACSF) solution containing (in mM): NaCl (125), NaHCO3 (24), KCl (2.5), CaCl2 (1), MgCl2 (1), and glucose (10), equilibrated with 95% O2/5% CO2, pH 7.3\u20137.4. Slices were incubated in ACSF for 1\u20132 hours at 32\u00b0C, transferred to a recording chamber mounted on the stage of an upright microscope (Axioskop, Carl Zeiss Microimaging Inc., Thornwood, NY), and then continuously perfused with ACSF (32\u00b0C). GnRH neurons were identified through their GFP expression using epifluorescent excitation at 470\u00a0nm with a 60X water immersion.Living neurons were selected (using GFP expression) for a variety of dendrite lengths. Recordings were made with an Axoclamp 2B amplifier in bridge mode (Axon Instruments; Union City, CA). Pipettes (3\u20136 M\u03a9) were fabricated from borosilicate glass (AM Systems, Carlsborg, WA) using a pipette puller (PC 10; Narishige, Tokyo, Japan) and coated with Sylgard 184 (Dow Corning, Midland, MI). Pipettes were filled with (in mM): K-gluconate (130), HEPES (10), EGTA (0.2), NaCl (6), MgCl2 (2), NaATP (4), NaGTP (0.4), spermine (0.05), glutathione (5) and biocytin (0.5%). Electrodes were positioned using piezoelectric micromanipulators (Luigs and Neumann, Ratingen, Germany). Pipette capacitance and series resistance were compensated for electronically.During whole-cell recordings, current injection pulses were applied to GnRH somata in order to induce ADPs. The voltage response of cells to the ADP-inducing current injection protocol, (0.2\u00a0nA for 3\u00a0ms), was recorded during repeated trials. Custom Matlab analysis software identified induced action potentials, and collected traces with ADPs. For each cell, an average ADP amplitude, with standard error, was calculated from all ADP events detected in the recordings from the cell.After recording and filling with biocytin, slices were fixed overnight in 4% paraformaldehyde. Slices were then washed in potassium phosphate buffered saline (KPBS) and incubated in 1% H202 with 2% albumin and 10% methanol in KPBS. Biocytin-containing neurons were visualized using the Vector ABC protocol (Vector Laboratories; Burlingame, CA). Slices were incubated overnight in 0.1% Triton-X, 2% bovine serum albumin (both from Sigma Chemical, St. Louis, MO) and avidin\u2013biotin\u2013horseradish peroxidase complex (Vector Laboratories; Burlingame, CA). Biocytin-labeled cells were visualized with 3,3\u2032 diaminobenzidine (0.06%, 1% NiCl, 0.003% H2O2, KPBS; pH 7.4) with nickel enhancement. Slices were then dehydrated in increasing concentrations of ethanol (30\u2013100%), and coverslipped out of xylene in Permount (Sigma Chemical; St. Louis). Morphologies were determined using a three-dimensional reconstruction system (Neurolucida, MicroBright Field, Inc.). For each recovered GnRH neuron, a Neurolucida morphology file was generated by tracing the cell, to determine its dendrite and axon length.Short Current Injection Pulses (CIPs) of 0.5\u00a0nA for 0.5\u00a0ms were repeatedly applied to the GnRH neuron somata via whole-cell patch in current-clamp mode. Active responses to these short-CIPs were single action potentials.In addition, prior to applying the solution to render neurons electrically passive, responses to longer, weaker CIPs of 0.03\u00a0nA for 50\u00a0ms were measured to give a datum on frequency response versus injected current.After application of the solution to render neurons electrically passive, short-CIPs of + 1\u00a0nA and \u22121\u00a0nA, for 0.5\u00a0ms each were alternately and repeatedly applied to the cells, and an average measured voltage response calculated from >100 rectified positive and negative pulses. It was this average passive short-CIP response that was used to tune the passive properties of the model cells (Major et al. ; Roberts et al. ).Models were constructed in GENESIS (GEneral NEural SImulation System http://www.genesis-sim.org/GENESIS/). Model scripts and morphology files are available from the authors upon request. To examine the impact of dendrites on ADP generation and repetitive firing, we started with models with complete morphology and sequentially truncated their dendrites. Details of the active voltage-gated channel kinetics are in the appendix. Two different types of models were generated and analyzed, one set with passive dendrites, and another set with dendrites including fast and persistent Na+ conductances. The somata of all models included, in addition to a passive leakage conductance, fast and persistent Na+ (NaF and NaP), delayed-rectifier, inward-rectifier and muscarinic K+ (Kdr, Kir and Km), and T and L type Ca2+ (CaT and CaL) conductances. Model axons had NaF, NaP and Kdr conductances. Previous studies in GnRH neurons have indicated that ADPs, when present, are sensitive to the Na+ channel blocker Tetrodotoxin (TTX) (Chu and Moenter ). A previous model of a hypoglossal motoneuron showed ADPs which were dependent on a persistent Na+ conductance (Purvis and Butera ). The model of the NaP conductance from that study was included in the present models. All other voltage-gated conductances were based on models of GT1\u20137 cells by LeBeau et al. (). The GT1\u20137 cell line is an immortalized hypothalamic GnRH cell line, originally produced by genetically targeted tumorigenesis (Mellon et al. ). This cell line has been shown to release GnRH, and is a well-studied model of the GnRH neuron system.As described in Roberts et al. (), the passive properties of the models were tuned with a Genetic Algorithm. Specific membrane capacitance and resistance, as well as axial resistivity, were adjusted to minimize the differences between the voltage responses of the models and an averaged passive electrophysiological response to the short CIP stimulus. Details of the passive response fitting procedure are in the .A second Genetic Algorithm was set up to tune the active conductance densities of the models to duplicate as accurately as possible the active responses of GnRH neurons in slices to current injection. Models were subjected to a series of simulated current injections to compare their responses to the measured electrophysiology. First, a 0.8\u00a0nA, 0.5\u00a0ms CIP was applied to the soma to cause models in states of unstable equilibrium to re-equilibrate. After a period of several hundred ms for the model to settle down after this initial stimulus, a CIP of 0.03\u00a0nA for 50\u00a0ms was applied to match the frequency versus injected current datum (normally two spikes) from the experiments. After another delay to allow equilibration, a short somatic current pulse (0.5\u00a0nA for 0.5\u00a0ms) was applied, intended to induce a single spike. Finally, for the active dendrite models, a simulated CIP of 0.5\u00a0nA for 0.5\u00a0ms was injected into the distal tip of the dendrite to attempt to induce a dendritic-initiated spike.In addition to requiring models to propagate spikes to the ends of the axons, and in the case of the active dendrite models, to propagate action potentials down the dendrites, the Genetic Algorithm was set up to match the shape of a single action potential as accurately as possible. The previous models had action potentials whose width was much narrower than in the living cells. This narrower action potential could have led to lower levels of activation of voltage-dependent conductances that could generate ADPs.Figure (e) compares the voltage response to current injection of our bipolar model cell and the living cell upon which the model was based. The living cell and the model cell were both subjected to two different current injection pulses. As shown in the figure, the voltage response of the model cell closely follows that of the living cell, in amplitude, frequency and spike shape.In order to test the hypothesis that the difference between the ADP activity in hemisagittal slices and acutely dispersed somata was due to differences in cell morphology, we systematically truncated dendrites by removing dendrite compartments one at a time from the model, and measured the magnitude of any ADP and rate of repetitive firing in the response to simulated current injection.For this task, a set of Matlab scripts were prepared that read in, parse, modify and re-write Genesis morphology files. The morphology file parsing program also calculates the total distance that each compartment is from the soma, calculated along the path of the dendrite. Another program takes as an input argument the number of compartments to be removed from the end of each dendrite branch, and writes a new morphology file with this truncated morphology. The truncation program was called iteratively in a loop in another Matlab program that then ran a Genesis simulation on a model with the truncated morphology.The simulation consisted of applying a short CIP of 0.8\u00a0nA for 0.5\u00a0ms to cause the model to spike, waiting 500\u00a0ms of simulated time for the model to re-equilibrate, applying a second somatic CIP of 0.4 to 0.5\u00a0nA (depending on model soma diameter) for 0.5\u00a0ms, and then recording somatic voltage for an additional 500\u00a0ms. The stored voltage output from this simulation was analyzed, and the maximum voltage in a time window of 10\u2013300\u00a0ms after the spike was stored as the ADP amplitude.The ADP amplitudes for each truncated morphology were stored in a data file, along with the total dendrite lengths for that model. Simulations were run for every possible value of the dendrite length, along both branches for the branching cell model, and also systematically truncating the axon in the bipolar cell model. The effects of axon truncation in the bipolar cell model were found to be much less significant than the effects of truncating the dendrite, so data presented later are for models with full axons, but systematically truncated dendrites. Matlab scripts to read, analyze, modify and write Genesis morphology files are available upon request from the authors.Longer dendrite length significantly reduced amplitude of the ADP. Earlier studies indicate that dendrites of GnRH neurons are, on average, 300\u2013500\u00a0\u03bcm (Campbell et al. ; Roberts et al. ). At 300\u00a0\u03bcm, amplitude of ADPs was decreased by 67% and was less than 0.5\u00a0mV. In the branching neuron model (3b), the maximum ADP amplitude was observed with a short branch length of 157.8\u00a0\u03bcm and a long branch length of 177.0 \u03bcm. The amplitude of the ADP decreased with increasing dendrite length in either the long or short branch. For dendrite branch lengths greater than 300\u00a0\u03bcm, the amplitude of the ADP was reduced by at least 58% and was less than 0.61\u00a0mV. For all models studied, there was a dendrite length below which the models fired repetitively after a single stimulated action potential. For these models, it was not possible to assign a value to the ADP. This is the reason that the curves of ADP versus dendrite length do not extend to zero dendrite length.Figure (c) and (d) shows repetitive firing rates in model GnRH neurons with different dendrite lengths. Models were subjected to a current injection protocol of 16 evenly spaced pulses (0.41\u00a0nA, 0.5\u00a0ms) in a period of 320\u00a0ms (50\u00a0Hz) for the bipolar models, or 533\u00a0ms (30\u00a0Hz) for the branching models. The longer bipolar cell fired at a maximum frequency of less than 40\u00a0Hz, (see 5d), due to its larger total capacitance and thus a lower stimulus frequency was used to induce it to fire repetitively. For short dendrite models, firing continued after the cessation of current input. Increasing dendrite length leads to a steady decrease in sustained firing rate. In the passive dendrite bipolar model, by a dendrite length of 109 \u00b5m, the ADP becomes insufficient to sustain firing after termination of current injection. If both branches of the passive dendrite branching cell model are longer than 230\u00a0\u03bcm, firing terminates with the cessation of input. As noted above, in the branching cell model, only one dendrite branch at a time was truncated, leaving the rest of the cell unchanged.The preceding results were derived from models with passive dendrites and thus isolate the impact of passive membrane properties. As previously noted however, many dendrites are not passive but rather contain active conductances. Therefore, truncations and simulations were performed on the bipolar cell following incorporation of fast and persistent Na+ conductances in the dendrite compartments. As described previously, this model was tuned to not only accurately duplicate the action potential shape and frequency response to somatic current injection from the measured electrophysiology, but to also be capable of initiating action potentials at the distal tip of the dendrite, which propagated all the way to the tip of the axon.In order to examine the mechanism of generation of the ADPs in the models, simulations were run where the magnitudes of all active voltage-gated currents in the soma were saved. Since the magnitude of the ADP was significantly larger in the active dendrite bipolar cell model, it was this model whose somatic currents were analyzed to determine which currents were active during the ADP phase of the voltage waveform, and how their amplitudes changed with changing dendrite length. Simulations were run on models with 381, 299, 199 and 109\u00a0\u03bcm dendrites. Analysis of these data indicated that the main depolarizing currents active during the ADP are the fast and persistent Na+ currents, as well as the L type Ca2+ currents. These results are consistent with the experimental results from Chu and Moenter (), who showed that ADPs measured in slices could be reduced or eliminated by using Na+ and Ca2+ blockers. There is also, in the model, an increase in somatic delayed rectifier K+ current, due to the sustained depolarization of the ADP.The average voltage trace, and reconstructions of the cell morphologies are shown for the cells with the maximum and minimum ADP response in Fig.\u00a0(c) and (d), respectively. The corresponding data points are indicated by one and two asterisks in the panels.The role of dendritic architecture in tuning incoming synaptic signals originating in dendrites has been well-reviewed (Gulledge et al. ; Kath ). Dendrites can significantly reduce the effects of distal synaptic currents on somatic action potential initiation through passive attenuation.The focus of this study was on the impact of dendritic morphology on an intrinsic electrophysiological phenomenon which appears to be a largely local spike generating mechanism. In hippocampal pyramidal neurons, is has been shown that physical truncation of the apical dendrite does not eliminate observed ADPs (Yue et al. ). Likewise, ADP generation in GnRH somata suggests mostly local mechanisms (Kuehl-Kovarik et al. ). These results, along with the observation that a decrease in the length of dendrites is associated with an increase in the incidence of ADPs observed in oxytocin neurons (Stern and Armstrong ; Stern and Armstrong ; Teruyama and Armstrong ) led us to examine the role that dendritic morphology might play in modulation of this proximally generated ADP, a phenomenon which has been widely implicated in release of hormones from neuroendocrine neurons.Using conductance-based compartmental models with full morphology, we investigated the effects of systematic truncation of the dendrite on the magnitude of evoked ADP response. Our results, both from our compartmental models and from measurements of living GnRH neurons in slices, indicate that the ADP response of GnRH neurons is strongly influenced by neuronal morphology. In the present study, we show that ADPs can generate repetitive firing in truncated dendrite models, a situation comparable to studies using cultured GnRH somata (Kuehl-Kovarik et al. , ). However, in models with full morphology, the extended dendrite counteracts the currents which generate ADPs and reduces or eliminates repetitive firing in response to current injection pulses. Using compartmental computer models (where only the length of the dendrite is altered), it appears these differences in ADP response can be accounted for by passive properties of GnRH neuron dendrites since this phenomenon is seen both in models with completely passive dendrites and in models whose active dendrites are capable of generating and propagating action potentials. Moreover, even when active Na+ channels are present to partially compensate for membrane leakage, the extended dendrite adds a capacitive load to the cell which defines the neuronal response.Differences in key passive properties of intact GnRH neurons and cultured somata have been noted in experimental preparations. For example, input resistances in isolated somata are almost twice those reported in intact GnRH neurons (see Spergel et al. ; Suter et al. , and Sim et al.  vs. Kuehl-Kovarik et al. , ). This is because dendrites and axons contain leak channels (e.g., the resting potassium conductance), which increase the input conductance, and thus reduce the input resistance of the cell. These leak channels provide a steady hyperpolarizing current which tends to neutralize or \u201cshunt\u201d depolarizing currents from voltage gated channels, synaptic inputs or current injections. Cultured somata contain only rudimentary processes (Kuehl-Kovarik et al. , ). Therefore, shunting is minimized in cultured somata relative to what would occur in morphologically intact neurons with their extended electrically \u201cleaky cables\u201d. This passive leak is also present in models with active dendrites, and is only partially compensated for by depolarizing current from Na+ channels at resting membrane potential. Thus, a given amount of depolarizing current injected into an isolated soma will be much more effective at charging up the membrane capacitance, and thus raising the membrane voltage, than it would be in an intact GnRH neuron in a slice or in the hypothalamus. Secondly, the dendrite alters the responsiveness of GnRH neurons to currents by increasing the cell capacitance. Cultured GnRH somata have a capacitance that is 4\u20136 times lower than that of GnRH neurons in slices (Kuehl-Kovarik et al. , ). The first study using GFP-identified GnRH neurons in slices made note of the higher capacitance relative to culture models (Spergel et al. ). In the present study, the soma compartments of the models had comparable capacitance values to those reported for cultured GnRH somata (about 8.0\u00a0pF; Kuehl-Kovarik et al. ). By contrast, one full morphology model neuron had a total capacitance of 30\u00a0pF whereas in the second cell the capacitance was 59\u00a0pF. These capacitance values are similar to those reported for living GnRH neurons in brain slices (see Table in Spergel et al. ). The lower capacitance due to the lack of processes in cultured somata means that less injected charge is required to bring the cell to spike threshold. Accordingly, while cultured somata probably provide a reasonable representation of the types of ionic conductances and biochemical processes expressed in GnRH neurons, they dramatically underestimate the passive electronic characteristics of morphologically intact GnRH neurons.Our findings underscore the important contribution that morphology and passive membrane properties of the GnRH neuron alone make in control of firing. We had anticipated dendrite architecture might exert some influence on GnRH neuronal output (based on theoretical considerations and Roberts et al. ). However, the magnitude of the impact was quite unexpected. The length of axons could be reasonably anticipated to have a similar influence, and indeed this is seen in models with truncated axons, though the effect is much less dramatic than for truncated dendrites. This was confirmed in the analysis of the experimental data, where the inverse correlation of ADP amplitude with dendrite length was much stronger than when the same data were plotted against total process length, including axon length. The issue of axon length is probably only important from an experimental perspective since it is unlikely that axon length is physiologically modulated, or at least such modulation of axon length has not been observed.In contrast, the finding that the structure of the GnRH dendrite itself can alter neuronal output is exceedingly important. First, the emerging picture with respect to dendritic structure reveals a level of complexity which was heretofore underappreciated (Campbell et al. ; Roberts et al. ). The present study suggests variations in branching and dendrite length, through passive membrane properties, likely account for at least some of the functional heterogeneity that is exhibited by hypothalamic GnRH neurons (Sim et al. ). Thus, morphological differences between neurons may define functional subpopulations within the larger GnRH network. Moreover, it has been recently demonstrated that the structure of GnRH dendrites are capable of remarkable plasticity between both physiological (Cottrell et al. ) and behavioral states (Russell Fernald; personal communication). As previously noted, an earlier study in a different population of neuroendocrine neurons indicated that total dendrite length of oxytocin neurons decreased by about 41% during lactation (Stern and Armstrong ). Our findings indicate that dynamic changes in dendrite length, even excluding any associated alterations in synaptic input or intrinsic conductances, can profoundly tune neuronal responsiveness and as a consequence, directly alter firing patterns. Thus, this structural change alone may in part underlie the increase in burst firing observed in oxytocin neurons in lactating animals (Wakerley and Lincoln ). By extension, our findings indicate that changes in dendrite structure likely influence neuronal firing and secretion of GnRH during physiological transitions in reproductive state.Holding the passive parameters constant, a separate genetic algorithm was used to search for optimum conductance densities and kinetic parameters to match model action potentials to those measured in the active living cells.The conductances used in this study were Fast Sodium (NaF), Delayed-Rectifier Potassium (Kdr), Inward Rectifier Potassium (Kir), M-type Potassium (Km), L-type Calcium (CaL), T-type calcium (CaT) and Persistent Sodium (NaP). The channel definition equations for all the channels except NaP were taken from the GT1\u20137 models of LeBeau et al. (), while the NaP is adapted from Purvis and Butera ().The exponents on the gating variables, m, h and n indicate the number of gates in each conductance type. For example, the NaF conductance has three activation (m) gates and one inactivation (h) gate. The maximum conductance, , represents the single channel conductance times the channel density. It is these maximum conductance values that are \u201ctuned\u201d to make the model duplicate measured electrophysiological activity.The voltage and time dependence of any conductance then can be described by specifying the voltage dependence of the steady-state gating variables (m, n, h) and their associated time constants. The specific equations used in these models were (all voltage values in mV, all times in ms):The model for the Kir conductance has the activation immediately assuming its steady-state value, so it is programmed with a time constant of 1\u00a0\u03bcs, or 1% of the step time in the model.As noted above, the kinetics for all conductances except the NaP came from LeBeau et al. (). The time constants were all divided by 5 to reflect a Q10 factor of 3, since the models in LeBeau et al. () were based on experiments done a 25\u00b0C, while the present study was based on measurements of brain slices measured at 32\u00b0C.In addition, as part of the genetic algorithm tuning of the model to match peak shape, the half activation of the NaF channel, and the time constants of the NaF and Kdr channels were allowed to vary. The values in the equations above reflect the temperature scaling and adjustments from the genetic algorithm.For the NaP conductance, the half activations of the steady-state activation and inactivation were adjusted downwards by 22.9\u00a0mV and 15\u00a0mV, respectively, and the time constants for activation and inactivation were multiplied by factors of 50 and 4 respectively, relative to the equations in Purvis and Butera (). Again these adjustments in kinetic parameters were performed to more closely match the model action potential shape to the measured electrophysiology.Actual simulation codes and cell morphology files are available from the authors upon request.The passive properties of the model cells; specific membrane capacitance and resistance, as well as axial resistivity, were adjusted to minimize the differences between the voltage responses of the models and an averaged passive electrophysiological response to the short CIP stimulus. The procedure for comparing model and measured voltage response is outlined below. The evaluation procedure described here was used to generate a single \u201cfitness\u201d value that was minimized in a Genetic Algorithm parameter search, varying the passive parameters. The set of passive parameters that gave the best fit for both model morphology types was used as a starting point to generate the active models.", "s10827-008-0097-3": "Parametric and non-parametric modeling methods are combined to study the short-term plasticity (STP) of synapses in the central nervous system (CNS). The nonlinear dynamics of STP are modeled by means: (1) previously proposed parametric models based on mechanistic hypotheses and/or specific dynamical processes, and (2) non-parametric models (in the form of Volterra kernels) that transforms the presynaptic signals into postsynaptic signals. In order to synergistically use the two approaches, we estimate the Volterra kernels of the parametric models of STP for four types of synapses using synthetic broadband input\u2013output data. Results show that the non-parametric models accurately and efficiently replicate the input\u2013output transformations of the parametric models. Volterra kernels provide a general and quantitative representation of the STP.The study on STP dates back to the 1940s (Eccles et al. ; Feng ). Many underlying mechanisms of STP have been inferred from numerous studies involving various experimental techniques and preparations (Katz and Miledi ; Creager et al. ; Debanne et al. ; Dobrunz and Stevens ; Dittman and Regehr ; Hanse and Gustafsson ). These mechanisms are generally classified into two major categories: facilitation (when the presence of previous presynaptic events causes increased postsynaptic response at present) and depression (when the opposite effect occurs). The most widely accepted biological mechanisms for such STP effects are the residual calcium hypothesis of facilitation and the depletion model of depression (Liley and North ; Katz and Miledi ; Betz ). According to these hypotheses, facilitation is caused by the accumulation of residual calcium in the presynaptic bouton, while depression is due to the depletion of the neurotransmitter vesicle pool. Additionally, the recovery of the depleted vesicle pool has been shown to be dependent on the residual calcium concentration (Dittman and Regehr ; Wang and Kaczmarek ; Hosoi et al. ). Depletion-independent mechanisms of depression were also reported (Hsu et al. ; Dobrunz et al. ; Kraushaar and Jonas ; Waldeck et al. ; Gover et al. ; Kirischuk et al. ; Pedroarena and Schwarz ; Fuhrmann et al. ). Furthermore, postsynaptic mechanisms (e.g., the desensitization of AMPA receptors) are potential contributors to synaptic depression (Trussell and Fischbach ; Trussell et al. ; Jones and Westbrook ).Many STP models have been developed with distinct scientific aims (Mallart and Martin ; Friesen ; Magleby and Zengel ; Krausz and Friesen ; Zengel and Magleby ; Yamada and Zucker ; Sen et al. ; Tsodyks and Markram ; Varela et al. ; Tsodyks et al. ; Hunter and Milton ). In one approach, the model is built to explain the physiological mechanisms underlying synaptic transmission. One excellent example of such model was proposed by Dittman et al. (). In their model, the STP dynamics is described by the interplay between facilitation, depression and residual calcium at the presynaptic terminal. This type of model can be termed \u201cparametric model\u201d due to the fact that it is expressed with very specific model structures/functions inspired by physiological mechanisms, and a number of adjustable parameters that can be related to certain biological processes. It has a predictive power in uncovering the nature of the underlying physiological processes/mechanisms. For example, by altering the values of its key model parameters, e.g., initial release probability of synapse, one can replicate several distinct forms of STP dynamical characteristics in a physiologically interpretable manner. In terms of reproducing the STP dynamics, such models are often qualitative due to the aim it is developed for.The other class of STP models were developed to quantitatively identify/replicate the nonlinear dynamical input\u2013output characteristics of STP (Krausz and Friesen ; Zengel and Magleby ; Sen et al. ; Varela et al. ). This can be done in a parametrical fashion: the model structure can be determined based on the principle physiological mechanisms, e.g., facilitation and depletion for STP. The values of the unknown parameters can then be estimated from experimental data so that the resulted model replicated the nonlinear dynamical input\u2013output transformation with minimum error. However, for this specific modeling aim, the parametric modeling approach has the inherent limitations associated with their fixed model structure that is determined a priori based on the partial knowledge and assumptions to the modeled system, and thus is subject to potential biases. For example, there almost certainly exist unknown mechanisms/processes that are not described by the parametric model. A secondary source of errors may be the accuracy of the required parameter estimation under realistic input characteristics if the parameter estimation is performed on the basis of input\u2013output data obtained with highly specific experimental protocols that contains built-in biases. The latter provide only limited information about the functional characteristics of the system and may bias the estimated parameter values.An alternative approach is the use of \u201cnon-parametric\u201d models that are obtained directly from input\u2013output data collected under broader experimental conditions (e.g. random stimuli that probe the system function with a broader repertoire of inputs) within the framework of a general model form (the functional Volterra series). In the non-parametric modeling approach, no specific assumptions are made a priori about the model structure, since the model takes a general form that is applicable to almost all causal systems (Krausz and Friesen ; Marmarelis and Marmarelis ; Bishop ; Marmarelis ). Instead of searching for the proper parameter values within a postulated model structure, the non-parametric approach searches for the optimal functions (Volterra kernels) contained within the general model that represents the input\u2013output relationship of the system. The nonlinear dynamics underlying synaptic STP can be represented quantitatively by the kernels of the Volterra model that has predictive capability for broadband data. The key point is that the non-parametric model utilizes a general model form (Volterra kernels), thus avoids potential errors in the postulation of the model structure\u2014as required in the parametric modeling approach. The unknown quantities in the non-parametric Volterra model are the kernels that are estimated from input\u2013output data collected under broadband conditions (e.g. random inputs). Thus, the non-parametric model, being inductive on the basis of broadband data, has the potential to captures the complete input\u2013output nonlinear dynamical characteristics of STP, as determined by all relevant biological processes and underlying mechanisms active under the broadband conditions (Marmarelis and Marmarelis ; Berger et al. ; Marmarelis ).As stated above, since parametric and non-parametric models are developed for distinct aims and lay emphasis on different aspects of the modeled system, they are complementary in nature. The main aim of these two papers is to combine both parametric and non-parametric modeling methods in a synergistic manner to study the STP. In this paper (part I), we introduce a recently developed variant of Volterra modeling that employs Laguerre expansions and allows efficient high-order model representation and accurate kernel estimation with short input\u2013output datasets (Marmarelis ). This method was applied to overcome the major difficulty of the non-parametric model\u2014its representational complexity caused by the possible high-order kernels required for completeness. We estimated non-parametric models of four different types of central synapses using synthetic broadband input\u2013output data simulated with published parametric models of STP (Varela et al. ; Dittman et al. ). Results show that the non-parametric models accurately capture the input\u2013output relations defined by the respective parametric models of STP for a broad repertoire of input patterns similar to those encountered under physiological conditions. The effects of specific parameters of the parametric models on the system input\u2013output transformation are reflected directly on specific features of the estimated kernels.More importantly, since the non-parametric (Volterra) model constitutes a canonical and complete representation of the system (nonlinear) dynamics and it is not restricted by any prior assumptions, it forms a model-free, nearly direct representation of the input\u2013output data themselves. The non-parametric model estimated from experimental data can be used as the \u201cground truth\u201d to evaluate parametric models of the system in terms of their input\u2013output transformational properties. Furthermore, the non-parametric model may suggest specific modifications in the structure of the respective parametric model. This combined utility of parametric and non-parametric modeling methods is presented in the companion paper (part II).The input\u2013output data used in this study result from the simulation of the parametric models described below. The data are analyzed according to the Volterra modeling methodology and its recent refinements (i.e. Laguerre expansions of the Volterra kernels) that are also described below.In actual experiments, the input signals at the synapses are trains of action potentials propagating along the axon and delivered at the presynaptic sites of the axon terminals. Since all action potentials have very short duration (1\u20132ms) and almost identical shapes, they are simplified for purposes of processing and analysis as sequences of discrete impulses (Kronecker delta functions) with inter-impulse intervals encoding the input information. The output signals are the quantities of neurotransmitter released from the vesicles of the presynaptic bouton in response to each action potential arriving at the presynaptic site of the axon terminal. Experimentally, the strengths of neurotransmitter release events are quantified as amplitudes of the excitatory postsynaptic currents (EPSCs). The EPSC trains are simplified as sequences of discrete impulses with varying amplitudes through deconvolution with an EPSC template (see companion paper for more details). The latencies of the EPSCs are short and approximately constant (typically a fraction of 1ms) and thus can be ignored. The EPSCs are recorded under voltage-clamp conditions and after pharmacological manipulations so that the contribution of postsynaptic processes which may introduce nonlinearities (e.g., NMDA current and nonlinear dendritic integration) are diminished. However, strong nonlinearities due to the processes of presynaptic facilitation and depression are maintained, as evidenced by the presented results (see next section) of nonlinear modeling of the input\u2013output data. The computational form of sequences of discrete impulses for the input (fixed amplitude) and output (variable amplitude) is used for the generation of the simulation data. An illustration of the input\u2013output data representation is shown in Fig.\u00a0.The parametric models of four types of CNS synapses having different characteristics of STP are included in this study: (1) the hippocampal CA3-to-CA1 Schaffer collateral synapse (SC); (2) the cerebellar granule cell parallel fiber-to-Purkinje cell synapse (PF); (3) the inferior olive climbing fiber-to-Purkinje cell synapse (CF); and (4) the excitatory synapse in layer 2/3 of the visual cortex (VC).The EPSC amplitudes for the first three synapses are calculated through simulation of the FD model based on the residual calcium hypothesis (Dittman et al. ) under conditions of random stimulation (Poisson random impulse trains) using the following equations with different sets of model parameters for each synapse. The EPSCs of the VC synapse are simulated using the FD 1 D 2 model (Varela et al. ).In Eqs. () and (), the concentrations CaX F and CaX D are modeled as two linear processes that decay exponentially with time constants \u03c4 F and \u03c4 D, after an impulsive change \u0394F and/or \u0394D of the facilitation and/or depression factor, respectively\u2014triggered by an action potential arriving at the presynaptic site at time t 0.The EPSC amplitude is modeled in Eq. () as the product of the initial EPSC amplitude A 0 with the facilitation factor F and the two depression factors D 1 and D 2. After each stimulating action potential, F is increased by a constant f [see Eq. ()] while D is multiplied by a constant factor d [see Eq. ()]. Between successive stimuli (i.e. arriving action potentials), F and D recover exponentially with time constants \u03c4 F and \u03c4 D, respectively [see Eqs. () and ()]. D 1 and D 2 have different recovery rates. Note that, in contrast to the previous FD-residual calcium model, A 0 does not represent the release probability and is set equal to 1 in the simulations for simplicity.The estimation of the equivalent non-parametric (Volterra) models requires a broadband input, such as a random point-process input like a Poisson random impulse train (RIT), to probe fully the dynamics and the nonlinearities of the parametric models of the four synapses. The inter-pulse interval (IPI) of a Poisson RIT is a random variable with an exponential distribution whose mean value equals the inverse of the exponent. The first set of simulations used RIT inputs with data-record length of 400 input/output event pairs and a mean firing rate (MFR) of 2Hz (i.e., the IPI mean value is 500ms) covering an IPI range from 2 to 5,000ms, which is consistent with the physiological firing characteristics of many types of central neurons (Berger et al. ; Barnes et al. ). This length of input/output data is adequate to obtain accurate kernel estimates of the respective non-parametric (Volterra) models following the methodology presented below. Note that it is necessary to increase the input\u2013output data-record length to 2,000 event pairs in the case of the VC synapse. In a second set of simulations, Poisson RIT inputs of the same length as before and with various MFRs (from 0.5 to 100Hz) are used to simulate the four synaptic parametric models. The resulting input\u2013output data are used to estimate the kernels of the respective non-parametric models in order to examine their ability to emulate the functional properties of the parametric models at these higher firing rates.In order to facilitate the estimation of the PV kernels from broadband input\u2013output data, we use the Laguerre expansion technique (Watanabe and Stark ; Marmarelis ; Song et al. ) which yields better estimates than the conventional kernel estimation technique of cross-correlation (Lee and Schetzen ; Sclabassi et al. ) and allows reliable kernel estimation from short and noisy datasets. This improved performance results from the utilization of the orthonormal basis of discrete-time Laguerre functions to expand the kernels and reduce the number of unknown parameters to be estimated.The Laguerre parameter \u03b1 (0 < \u03b1 < 1) determines the rate of exponential asymptotic decline of the Laguerre basis functions and in practice is selected through successive trials so that the prediction mean-square error is minimized. The utilization of basis functions for kernel expansion improves the kernel estimation because it reduces the number of free parameters to be estimated from the data (since the number L of basis functions is typically much smaller than the number M of samples within the system memory, i.e., the length of each kernel dimension.The unknown kernel expansion coefficients c  i  are estimated through least-squares fitting of the input\u2013output data in Eq. () using singular value decomposition to avoid numerical instabilities. A 1ms bin size is used to generate the discrete Laguerre basis functions. The obtained estimates of the expansion coefficients can be used to construct the PV kernel estimates according to the Eq. ().The number of basis function (L) and the Laguerre parameter \u03b1 are optimized using the criterion of out-of-sample NRMSE. For a given model order, optimal \u03b1 is searched in the range of (0, 1) for increasing L. The combination of \u03b1 and L that gives the smallest out-of-sample NRMSE is chosen to construct the PV kernels.Each of the aforementioned PV kernels quantifies interactions of a specific number of preceding input impulses that is determined by its order. However, a certain number of preceding input impulses (within the epoch of system memory) make contributions to the output through all the PV kernels of the system. For instance, a single preceding impulse makes a contribution through the second-order PV kernel (k 2) but also through all other higher-order PV kernels present in a specific system. In other words, k 2 is not the same paired-pulse facilitation/depression function measured through the popular paired-pulse stimulation protocol.The RDs may also be directly estimated from the input\u2013output data with separate consideration of each output with respect to the number of impulses within its memory. However, the current method is computationally more convenient since the estimation of PV kernels only involves combinations of v instead of the combinations of impulses in the system memory (M).The parametric models of the aforementioned four synapses (SC, PF, CF and VC) are simulated for the parameter values shown in Table\u00a0 and Poisson RIT inputs with MFR of 2Hz. Using the resulting synthetic input\u2013output data, we estimate the PV kernels of the corresponding non-parametric models employing the methodology described in the previous section.A Volterra kernel model is expressed as a functional power series of input with progressively higher-order kernels capable of describing higher-order nonlinear dynamics. Theoretically, it can replicate arbitrary system nonlinearity. However, the number of coefficients to be estimated grows exponentially with increases in model order, and thus makes it impractical to keep all the high order terms. So in practice, the first issue in kernel estimation is to find out how many terms (orders) are needed for accurate replication of the system nonlinearity and then to truncate the unnecessary high order terms. This is typically done using statistical model order selection criteria (MOSC) (Marmarelis ). However, existing noise-based statistical MOSCs are not applicable to the problem in this study since the modeled systems (parametric models) are noise-free. The residual errors are purely due to the difference between the kernel model and the parametric model, and their statistical significance thus can not be evaluated using statistical criteria. Therefore, we choose to use a simple and straightforward model order selection approach in this study: if the residual error (NRMSE) is below a certain level (5%) for a given model order, the model is considered a \u201cpractically complete\u201d (as opposed to \u201cmathematically complete\u201d) model since the majority of the system nonlinearity is accounted for.The small prediction errors imply accurate estimation of the PV kernels and RDs. In addition, the estimated RDs are also directly validated with the RDs numerically calculated through the parametric model. The estimated r 2 and r 3 well approximate the actual r 2 (Fig.\u00a0(C), dashed line) and r 3 (not shown) with arbitrary IPIs.The second column of Fig.\u00a0 shows the obtained results for the PF synapse. As indicated for the SC synapse model, r 1 (=k 1) is equal to the initial release probability (F 1 = 0.05), which is smaller than that of the SC synapse. The obtained RDs for the PF synapse have similar qualitative characteristics with the SC synapse, i.e. r 2 is positive for all IPIs (solely facilitatory) and r 3 is negative for all IPIs (solely depressive). However, the magnitude of r 2 is much larger than that of the SC synapse because of the much higher paired-pulse facilitation ratio in the PF synapse (\u03c1 = 3.1). Also, the magnitude of r 3 becomes much smaller compared to that of the SC synapse model. This confirms the NRMSE result showing that the majority of the PF synapse nonlinearity is of second order (Table\u00a0). In a triple-pulse train, e.g., the first three impulses in the 100Hz FIT (Fig.\u00a0, second row), the third response y(\u03c4 1, \u03c4 2) is strongly facilitated by the second-order contributions of each of the preceding impulses, r 2(\u03c4 1) and r 2(\u03c4 2), and only weakly depressed by the negative third-order joint contribution of the two preceding impulses r 3(\u03c4 1, \u03c4 2). The train thus shows a nearly monotonic-increasing pattern.The third column of Fig.\u00a0 shows the obtained results for the CF synapse. The value of r 1 is equal to the initial release probability 0.35, which is larger than in the previous two synapses. The values of r 2 are negative for all IPIs, and they are smaller in absolute value than the previous two synapses. The values of r 3 are also negative and small relative to r 2, indicating that the second-order nonlinearities are dominant in the CF synapse, as in the case of the PF synapse. Contrary to the SC and PF synapses, the CF synapse shows negative r 2 values that quantify the depressive effect of each preceding impulse upon the present response as a function of the IPI. The absolute values of r 2 decline rapidly for IPI < 140ms and more slowly for longer IPIs until they diminish around 2,000ms, indicating that we cannot fit the waveform of r 2 with a simple exponential function. We also observe that the form of r 3 is biphasic, crossing from negative (depressive) to positive (facilitatory) values around IPI of 100ms. The peak value of r 2 (\u221235% at the minimum IPI) is equal to the initial release probability of this synapse.Similar to the CF synapse, the VC synapse has a dominant second-order suppressive kernel/RD (Fig.\u00a0, fourth column). However, unlike the CF synapse, the small values of r 3 are positive for the VC synapse. Note that r 1 (not plotted) is equal to A 0 (=1), which is defined as the baseline response amplitude in the FD 1 D 2 model. We observe that the waveform of r 2 is comprises of an early fast-recovering component that is strongly negative and a subsequent slow-recovering component that has weaker negative values. This result shows that the two depressive components (D 1 and D 2 in Eq. ) in the parametric model of the VC synapse are captured by the second-order kernel of the non-parametric PV model. The facilitation determined by F in the parametric model of the VC synapse is masked by D 1 and D 2 in r 2, since the overall effect of D 1, D 2 and F is suppressive. The small values of r 3 indicate that the nonlinearity of the VC synapse is primarily of second order.The first three synapses (SC, PF and CF) share the same parametric model structure but have different parameter values. The corresponding non-parametric PV kernel models provide an elucidating way to appreciate how the key model parameters affect the input\u2013output transformational properties of each synapse (Fig.\u00a0). For instance, the nonlinearities of the PF synapse are primarily determined by the residual calcium-dependent facilitation mechanism. The depression mechanism is relatively weak due to the fact that PF synapse has a very small initial release probability (0.05) which limits the depression caused by vesicle depletion. These are reflected by the prominent positive r 2 and relatively small negative r 3. By contrast, the SC synapse has a much higher initial release probability (0.24) which causes a higher level of depletion, leading to larger negative values of r 3, i.e., in a triple-pulse train, the joint contribution of pairs of preceding impulses (exclusive of their second-order contributions) to the present output becomes strongly negative because of the small number of available vesicles after two consecutive releases. Nonetheless, the modulatory effect of each single preceding impulse is still facilitatory (as indicated by the positive values of r 2).The CF synapse lacks the facilitation mechanism (Dittman et al. ). In its FD model, the F factor remains constant through time regardless of the input pattern (Table\u00a0). Its nonlinearity is exclusively determined by the kinetics of recovery from vesicle depletion which is significant due to the high initial release probability (0.35), thus its r 2 values are negative. Negative also are the r 3 values for short IPIs, but become slightly positive for IPIs longer than about 100ms.The VC synapse has a different parametric model structure involving two depression factors in addition to the facilitation factor. The respective non-parametric PV model provides a general representation of its input\u2013output transformational properties and makes it possible to assess the functional similarities and differences with the different STP characteristics of the other three types of synapses\u2014even though the latter have different forms of parametric models. The main distinguishing characteristic of the parametric VC synapse model is its double-component depression (D 1 and D 2) which results in a double-exponential shape for r 2 (with time courses approximately matched by the time-constants \u03c4  D  1 and \u03c4  D  2 given in Table\u00a0). The fast facilitation factor (F) of the parametric model of the VC synapse is reflected on the positive values of r 3. Note that the FD model of the CF synapse also yields a double-exponential shape for r 2 (see third column of Fig.\u00a0) which is due to the residual calcium-dependent recovery of depletion, resulting in different recovery rates for different IPIs\u2014i.e. when the IPI is short, the residual calcium caused by a preceding impulse has a high concentration and results in a fast recovery rate; while for long IPIs, the residual calcium decays to lower levels that result in a slow recovery rate. This mechanism is modeled differently in the two parametric models but its effects on the STP are represented similarly in the respective non-parametric models, assisting the comparison of their functional characteristics.In summary, the results above show that each form of synaptic STP has its own characteristic pattern that is reflected quantitatively on the shapes, polarities and time durations of its PV kernels and corresponding RDs. It is evident that the PV kernels and the corresponding RDs contain quantitative and biologically interpretable information about the dynamics and nonlinearities underlying the STP processes in these four types of synapses.It is shown above that, for Poisson RIT inputs with MFR of 2Hz, the third-order PV models are able to highly accurately predict the outputs of all four types of synapses. For such inputs, the IPI values are random independent samples drawn from an exponential distribution with mean value of 500ms. In this study, we use an absolute refractory period of 2ms and the IPI values are generated in the range of 2\u20135,000ms. The importance of using Poisson RIT input is that it includes many of the temporal patterns typical of CNS synapses in physiological conditions. The MFR of 2Hz is selected on the basis of the firing characteristics of many central neurons, e.g., hippocampal CA1 and CA3 neurons of free-moving rats (Berger et al. ; Barnes et al. ; Deadwyler et al. ). However, different firing characteristics with higher or lower average rates and/or different distributions also exist. The effect of the different firing rates on the obtained PV models is discussed further in this section.In the fourth-order PV model, the estimated k 1 was the same as that of the third-order PV model, while k 2 and k 3 were different from those of the third-order PV model (compare Figs.\u00a0 and ). This is due to the fact that both PV models are mathematically incomplete as shown in their residual errors, although they are practically complete according to the chosen NRMSE threshold (5%). In mathematically incomplete PV models, the extra high order terms, e.g., k 4 in the fourth-order PV model of the SC synapse, are nonzero. These high order terms project to the lower-order PV kernels (k 2 and k 3) of the lower-order PV models, e.g., the third-order PV model of the SC synapse in this case, and make these lower-order PV kernels (k 2 and k 3) different in the two models. Direct comparison of the PV kernels is generally non-informative in the mathematically incomplete PV models of different model orders. By contrast, the PV kernels of the mathematically complete models do not change as the model order changes since these PV models have the same non-zero terms and the additional higher-order PV kernel in the higher-order PV model will not interfere with the lower-order PV kernels since they are equal to zero. However, it is remarkable that the RDs of the two practically complete PV models are nearly identical (Figs.\u00a0 and ). Such favorable property of the RD representation can be explained as the following: as shown in the Method section, RDs, which are derived from the PV kernels, have a simple interpretation, i.e., the ith-order RD, r  i , exclusively represents the ith-order modulatory effect of any i-1 different preceding input impulses on the present output. ith-order RD thus does not contribute to the prediction of the responses with i-1 or fewer impulses. When the MFR of the Poisson random input is low, the probability of having many input impulses in the modeled system\u2019s memory window (M) is small. The majorities of the input patterns (isolated by intervals larger than M) are single impulses, pairs and triplets. Practically complete models, e.g., third and fourth-order PV models of SC synapse, achieve low NRMSE mainly by accurately predicting those commonly encountered patterns. The first, second and third-order RDs, which correspond to the prediction of single impulses, pairs and triplets respectively, are all very close to their true values (as indicated by the small NRMSE and validated by the direct calculation) and therefore remain stable across different model orders. In the PV kernel representation, by contrast, ith-order PV kernel also contributes to the predictions of the responses with i-1 or fewer impulses. The estimates of the lower-order PV kernels, e.g., k 2 and k 3, are thus altered by the inclusion of higher-order PV kernels and consequently show different values across different model orders.In summary, the required model order to adequately characterize a system is jointly determined by both the input characteristics and the intrinsic system nonlinearity. Higher-order kernel models are more complete and robust to the change of input characteristics (e.g., MFR) in general. RDs of practically complete models are insensitive to the model order and thus are more appropriate for representations and interpretations of the system nonlinearity.In this paper, we applied a general, non-parametric modeling approach to the study of STP in CNS synapses. In this approach, the input\u2013output transformational properties of various synapses are probed with broadband stimuli (Poisson RITs). The synaptic nonlinearities, associated with STP, are represented by non-parametric models utilizing PV kernels estimated from input\u2013output data, which characterized fully the transformation of the presynaptic sequence of impulses into the postsynaptic EPSCs. Using a recently developed technique, the PV kernels can be accurately and reliably estimated from short input\u2013output datasets. In the first half of this paper, we estimated the PV kernels using the input\u2013output data simulated with established parametric models of several CNS synapses for Poisson random impulse trains (RITs) with MFR of 2Hz. It was found that third-order PV models (involving first, second and third-order PV kernels) could predict accurately the outputs of these parametric synapse models to novel input patterns (so long as the MFR did not exceed significantly 2Hz). The nonlinear dynamics defined by the specific mathematical form and associated parameter values of the parametric models were fully and reliably captured by the respective non-parametric models. The PV kernels and the associated RDs quantitatively described the transformational input\u2013output properties of the synapses and facilitated interpretation of their functional properties. In the second half of this paper, we further investigated the predictive accuracy of these non-parametric models for RIT inputs with higher MFRs and showed that this approach can be successfully applied to this case, but only if we increase sufficiently the order of the PV models to capture the higher-order dynamics and nonlinearities associated with higher input MFRs.Non-parametric method was first used to model synaptic transmission by Krausz and Friesen (Krausz and Friesen ). In their study, they estimated Wiener kernels of the lobster cardiac ganglion synapse using cross-correlation technique. The approach used in this study has several important differences and improvements compared to their approach. First, instead of Wiener kernels, we estimated Volterra kernels, which renders the kernel estimates invariant to input characteristics when a mathematically complete model is used. The corresponding RDs of the estimated PV kernels can be directly related to conventional electrophysiological measurements, e.g., r 1 is baseline postsynaptic response amplitude and r 2 is the paired-pulse facilitation/depression function. The utilization of RDs relaxes the requirement of mathematical completeness of the model in representation, since they remain invariant in practically complete models. By contrast, Wiener kernels highly depend on the input characteristic, e.g., MFRs of the Poisson input train. For example, first-order Wiener kernel is the mean of the response amplitudes during the Poisson train, which obviously depends on MFR. Second, in this study, the PV kernels of the non-parametric models are estimated using the Laguerre expansion technique that allows accurate and efficient kernel estimation with short datasets (Marmarelis ). Most of PV kernels in this paper are estimated with only 400 input\u2013output pairs, which is a significant advantage when we consider that the conventional cross-correlation technique usually requires thousands of input\u2013output pairs for comparable accuracy (Krausz and Friesen ; Berger et al. , ; Sclabassi et al. ). In addition, the Laguerre expansion technique allows the estimation of PV models of higher orders (i.e., third and fourth order) which is extremely difficult using the conventional binning method, since the latter involves much larger number of kernel coefficients to be estimated. Compared to previously obtained second-order PV kernel models (Gholmieh et al. ), higher-order models provide more accurate output predictions, because they capture the higher-order nonlinearities associated with more complex input\u2013output properties and/or higher input MFRs. This is demonstrated clearly in the SC synapse result, where there is a dramatic drop of prediction error when we increase the model order from second to third. For this specific system, only PV models of order higher than second are practically complete, in the sense that they capture the major effect of system nonlinearity. By contrast, the second-order PV model, being an incomplete model, fails to give accurate prediction. Furthermore, because of the prominent third-order nonlinearity of the SC synapse, the kernel estimates of the second-order PV model are biased by the presence of the higher-order nonlinearities. Thus, the second-order model yielded values of r 2 that are markedly different from the actual paired-pulse facilitation/depression function (Fig.\u00a0).It is important to point out again that the practical completeness of the PV model is also dependent on the MFR of the random inputs. When input MFR increases, the possibility of higher-order nonlinear interactions increases. In SC synapses, when the MFR of the Poisson RIT is 2Hz, the third-order PV model is practically complete, but when the MFR is increased to 10Hz, the fourth-order PV model is practically complete. This raises one important aspect of the non-parametric modeling approach: namely their guaranteed ability to predict the output is only for input patterns that are within the range of the inputs with which they are estimated. Although this is often true for parametric modeling as well, it is more critical for non-parametric modeling since non-parametric model is purely data-driven and in general lacks the extrapolation power. Therefore in experimental design, it is crucial to use broadband inputs that cover the patterns of physiological interest.Another form of quantitative model of STP in neuromuscular junctions of a crustacean was introduced by Sen et al. (). This model consists of a cascade of three components: a linear filter K 2, followed by a static nonlinearity F, followed by another linear filter K 1. Filter K 1 describes the shape of the postsynaptic response, e.g., excitatory junctional current/potential (EJC/EJP), and is equivalent to the \u201ctypical EPSC waveform\u201d that we used here to deconvolve the EPSC train in Part II of this study (see companion paper for details). The other filter K 2 is the first component of the cascade and serves to integrate the input activity over a longer past epoch. Sen et al. showed that it has an exponential form (with a negative exponent) extending over several seconds. The output of K 2 (i.e. the integrated input past activity) is transformed by the static nonlinearity F, which was shown to exhibit a supra-linear form (exponential-like, with a positive exponent). This model form represents the linear-nonlinear (LN) cascade model studied extensively in the 70s in connection with various sensory systems (for a review, see Marmarelis and Marmarelis ). The PV kernels estimated in this study correspond to the cascade of K 2 and F in the sense that they describe the nonlinear dynamic transformation of a point-process input (presynaptic spike train) to varying-amplitude outputs (EPSC or EJC/EJP amplitudes) which represents the combined effect of K 2 and F. It was shown that this model can be successfully applied to several crustacean neuromuscular junctions. However, it should be noted that this LN model does not constitute a complete representation of arbitrary nonlinear dynamics like the Volterra modeling approach. It relies on the strong assumption that the nonlinear dynamics of a particular system can be separated into a cascade of a single linear filter followed by a single static nonlinearity. The general Volterra representation is equivalent to the Wiener\u2013Bose model (composed of multiple parallel linear filters feeding into a multi-input static nonlinearity) or it can be decomposed into multiple parallel LN cascades termed the \u201cprincipal dynamic modes\u201d (Marmarelis ). We note that Sen et al. () adopted the LN approach for one important and valid aim, namely, developing a methodology that is \u201cas simple as possible both to apply and to describe\u201d while retaining a certain degree of generality and interpretability. This aim is different from the aim of the non-parametric modeling in this study, since we seek a general and complete representation of STP that allows us to reveal the subtle features of the nonlinear dynamical characteristics and use them to guide the modification of the parametric model.Before using experimental input\u2013output (stimulus\u2013response) data, in this paper, kernel models are estimated using input\u2013output data simulated with four parametric synapse models. There are several reasons for this: first, the parametric synapse models included in this paper represent several distinct forms of STP determined by the principle physiological mechanisms/processes. They include a variety of input\u2013output properties and make it possible to test the generality of the non-parametric approach within a broad context. Second, the parametric models are thoroughly known, stationary, nonlinear systems that allow direct and unambiguous validation of the non-parametric models. Third, the FD model capture the basic mechanisms of STP, i.e., residual calcium-dependent facilitation and depletion-based depression. As shown in Section\u00a0, many insights are gained by studying the relationships between its key parameters, which represent fundamental biological processes, and the kernels, which quantitatively describe the functional input\u2013output properties of the system.Parametric and non-parametric approaches are developed for distinct aims and have their relative strengths and weaknesses in modeling a biological system (Berger et al. ), and thus can be used in a synergistic manner. In this paper, we show that the non-parametric PV model provides a quantitative description of the synaptic input\u2013output transformation without requiring prior assumptions about the model structure. Furthermore, non-parametric model extracts the system nonlinearities and expresses them in a uniform and often simpler form. For example, the parametric FD model involves nonlinear differential equations; Analytical solution of output for a given input pattern is not obvious. On the other hand, using the non-parametric PV model, prediction of outputs for a given input pattern only requires simple arithmetical operations such as multiplication and summation. The uniformity and simplicity of model mathematical form is critical for large-scale simulations (Traub and Miles ) and hardware implementations (Tsai et al. ; Berger et al. ).Most importantly, non-parametric models estimated directly from the input\u2013output experimental data can be used to evaluate parametric models under broadband input conditions in terms of their input\u2013output transformational property. The differences between the non-parametric model estimated from experimental data and the equivalent non-parametric model of the parametric model may expose unknown physiological mechanisms/processes and suggest specific modification to the parametric model. This is shown in the companion paper (part II).", "s10827-008-0099-1": "The large number of variables involved in many biophysical models can conceal potentially simple dynamical mechanisms governing the properties of its solutions and the transitions between them as parameters are varied. To address this issue, we extend a novel model reduction method, based on \u201cscales of dominance,\u201d to multi-compartment models. We use this method to systematically reduce the dimension of a two-compartment conductance-based model of a crustacean pyloric dilator (PD) neuron that exhibits distinct modes of oscillation\u2014tonic spiking, intermediate bursting and strong bursting. We divide trajectories into intervals dominated by a smaller number of variables, resulting in a locally reduced hybrid model whose dimension varies between two and six in different temporal regimes. The reduced model exhibits the same modes of oscillation as the 16 dimensional model over a comparable parameter range, and requires fewer ad hoc simplifications than a more traditional reduction to a single, globally valid model. The hybrid model highlights low-dimensional organizing structure in the dynamics of the PD neuron, and the dependence of its oscillations on parameters such as the maximal conductances of calcium currents. Our technique could be used to build hybrid low-dimensional models from any large multi-compartment conductance-based model in order to analyze the interactions between different modes of activity.A major challenge in contemporary computational neuroscience is the analysis of high-dimensional, biophysically realistic models. Mathematical approaches in computational neuroscience have progressed from analysis of abstract neural systems at steady state (Wilson and Cowan , 1991) to more biologically realistic situations in which systems are in rhythmic or chaotic states (Terman ; Kopell and LeMasson ). A further increase in the biophysical sophistication of neural models has been driven by the availability of increasingly detailed electrophysiological and anatomical data about neuronal dynamics. Bifurcation theory (Guckenheimer and Holmes ; Strogatz ) is particularly helpful in understanding the qualitative change in the behavior of dynamical systems models as parameters are varied. However, the direct application of bifurcation theory becomes prohibitively difficult as the complexity of the model increases beyond a few dynamical variables, and heuristic arguments are commonly used to justify restricting analysis to smaller, approximate models (Fitzhugh ; Meunier ; Chow and Kopell ).In this study, we have two aims. The first is to show how a novel reduction technique (Clewley et al. ) can be extended to apply to multi-compartment biophysical neuron models. The second is to demonstrate that the technique is useful in the analysis of a real neural system that shows different qualitative behaviors in different parameter regimes. We focus on spiking and bursting behavior in a conductance-based model of the pyloric dilator (PD) neuron, a member of the pacemaker ensemble of the pyloric network in the well-characterized crustacean stomatogastric nervous system (Nusbaum and Beenhakker ; Marder and Bucher ). The PD neuron typically spikes tonically when it is synaptically isolated from the network but, in some preparations, is also capable of producing rhythmic bursts of action potentials and can thus be considered a conditional burster (Miller and Selverston ; Marder ).A recent modeling study by Soto-Trevi\u00f1o et al. () produced a biophysically realistic multi-compartment model of the PD neuron and its electrically coupled counterpart, the anterior burster (AB) neuron. We use the mathematical technique of \u201cdominant scales,\u201d developed by Clewley et al. (), to analyze this PD neuron model and systematically identify a critical time interval within the inter-spike interval, in which the dynamics of only a subset of ionic currents govern whether the neuron transitions from tonic spiking to bursting activity. Subsequently, we analyze the interactions among this select subset of ionic currents to characterize the differences in tonic and bursting activity. Our analysis elucidates the biological mechanisms underlying the change in qualitative behavior of the PD neuron model by determining local and low-dimensional approximations to a high-dimensional biophysical model. This methodology also serves as a case study in the context of a multitude of similar transitions in neuronal outputs from other models that could be investigated in a similar fashion.The model PD neuron involves two compartments to provide a spatial segregation of spike production from other ionic properties. These compartments will be referred to as the axonal and the soma/neurite (S/N) compartments. The system of ordinary differential equations (ODEs) describing the dynamics of the PD neuron are given in the Appendix, and their full description can be found in Soto-Trevi\u00f1o et al. (). Unless otherwise stated, all numerical solutions to the ODEs were calculated using the software package PyDSTool (http://pydstool.sourceforge.net).The equations for the compartmental membrane potentials take this form after a simple algebraic rearrangement described in Appendix , which reflects the conditional linearity of the equations in the Hodgkin\u2013Huxley formalism and of first-order kinetic equations in general. This view of the ODEs permits an intuitive comparison of the instantaneous target value  and time scale \u03c4  x  (t) for each variable, which may depend on the state variables.This measures how strongly the inputs influence the local structure of the vector field, and provides predictive information about the response of a compartment to a perturbation. For conductance-based neuron models, the sensitivity can be defined explicitly (Appendix ). This definition of influence extends to input terms in Eq. () arising from electrical coupling between compartments.The dominant scale method is applied to one or more known trajectories of the model system, which is typically calculated by numerical integration. The influences of input variables on a variable of focus (here, V) are computed over a trajectory. These values are ranked and normalized, and those that are larger than some small positive parameter \u03b5\u2208(0,1) at each moment in time are classified as inputs that are active; the remaining inputs are considered inactive. This is shown schematically in Fig.\u00a0 (bottom row). The time interval within which the set of active variables stays the same is defined to be an \u201cepoch\u201d (a, b and c in Fig.\u00a0). Changes in the set of active inputs through time define epoch boundaries (where changes in ranking within a set are ignored). Additionally, active variables are marked \u201cfast\u201d or \u201cslow\u201d relative to V over the duration of an epoch, according to a small time-scale parameter \u03b3\u2208(0,1): variables whose time scales are smaller than \u03b3\u03c4  V  are considered fast whereas those that are larger than \u03c4  V  /\u03b3 are considered slow. In summary, lower-dimensional reduced models approximate the full dynamics over the duration of an epoch, where the reduced model is a projection of the full system onto the space of the active variables.The dominant scale method has been implemented in an open-source Matlab code known as DSSRT (Dominant Scale System Reduction Tool), available at http://www.cam.cornell.edu/~rclewley/DSSRT.html. DSSRT requires details of the ODEs, the parameter values and a numerically computed trajectory of the system on which the analysis is performed. DSSRT does not fully automate the analysis, and relies on input from the user in order to obtain optimal results. In particular, it requires appropriate values for \u03b5 and \u03b3. The heuristic that we use for choosing the values is that they should lead to a decomposition of the inputs into as few epochs as possible over the course of the trajectory. Additionally, small changes in the values should not make a large qualitative difference to the pattern of epochs. DSSRT provides information about the relative influence strengths and time scales of variables along a trajectory that can aid in the optimal selection of these values. For the current study we chose the values \u03b5\u2009\n                        =\u20091/5 and \u03b3\u2009\n                        =\u20092/7, which robustly distinguished the most dominant scales of influence and time along the trajectories considered. Other values of \u03b5 and \u03b3 with the same order of magnitude produced similar qualitative results.The present version of DSSRT (v1.32) is unable to analyze differential equations for the calcium-dependent ion kinetics, although the trajectory used is calculated using the full set of equations. As a result, in our analysis of the model PD neuron two variables have to be approximated by constants. These are (1) the reversal potential E Ca and (2) the calcium dependence of the steady-state activation of I KCa. These approximations are not disruptive because the constants can be chosen differently for the analysis of different sub-regimes of the dynamics within which the true values remain approximately constant. By comparison to simulations of the full 16-dimensional model we will demonstrate that these approximations do not affect the validity of the qualitative low-dimensional models.The number of epochs that are automatically determined by DSSRT may be large and involve distinctions in the dynamics that are too fine-grained for our purposes. However, to elucidate the analysis of the dynamics most intuitively, DSSRT supports a heuristic amalgamation of certain epochs into what we will call \u201csub-regimes.\u201dThe two-compartment model of the PD neuron described above will be referred to as the full model (F). We use DSSRT to reduce this model to a sequence of low-dimensional sub-regimes that can be thought of as a piecewise-local model (L). Our methodological goal will be demonstrated by the successful reproduction of the distinct tonic spiking and bursting behaviors by model L. Our scientific goal is to identify the significant factors that contribute to these intrinsic output modes. We examine the dynamics in the different states of L to elucidate the details of interactions between the variables of F. We will also compare the performance and complexity of L to make a single, global reduction of the model (G)\u2014this being the traditional form of model reduction.The proper activity of the PD model neuron, both in isolation and as part of the network, is crucially based on the fact that ionic currents responsible for spike production are spatially segregated from other voltage-gated ionic currents. This conductance-based full model consists of 16 ODEs (see Appendix ) that describe two compartments, one representing the axon, and the other the soma/primary neurite (S/N). We denote by V axon and V the membrane potential of the respective compartments. The axonal compartment is responsible for spike generation via the usual fast sodium and potassium currents, I Na and I Kd, and corresponds to a specific parameter regime of a standard Hodgkin\u2013Huxley compartment. As the interactions between the variables within such a compartment are already well understood (Suckley and Biktashev ; Clewley et al. ), we will focus on reducing only the S/N compartment.In the absence of coupling to the axonal compartment the membrane potential of the S/N compartment produces large-amplitude (approx. 50\u00a0mV) slow oscillations, which are generated by three outward currents, four inward currents and a leak current I L. The outward currents are a delayed-rectified potassium current I Kd, a calcium-dependent potassium current I KCa and a transient potassium current I  A . Two of the inward currents are carried by calcium: I CaT and I CaS; the other two are a hyperpolarization-activated I H current and a persistent sodium current I NaP. We denote the total calcium current I CaS +I CaT as I Ca. The axonal and S/N compartments are intrinsically very different from each other; they interact via an axial current I axial with maximal conductance g axial. Unless otherwise stated, the values of the calcium current maximal conductances g CaT and g CaS that were used for setting the neuron in a tonic spiking regime are 60 and 22.5\u00a0\u03bcS, respectively. These will be referred to as the \u201creference values\u201d for these parameters. We increased these by 54% (to 92 and 35\u00a0\u03bcS) in order to set the neuron in a bursting regime.DSSRT computes the influence strengths \u03a8 s, defined in Eq. (), over a tonic spiking or bursting trajectory, using Eq. (). We note that, due to the strongly dissipative nature of the equations, we need only analyze a single periodic orbit for each regime, as qualitatively equivalent orbits will exhibit the same pattern of changing influences. Here, the inputs s corresponds to the activation variables of the currents in the S/N compartment and the voltage of the axon compartment, V axon. Values of \u03a8 s larger than the small positive parameter \u03b5\u2009=\u20091/5 at each moment in time correspond to inputs that are considered \u201cactive.\u201dWe begin by determining a global reduction (G) of the PD neuron model F. The passive leak term was not classified as an influential input at the chosen value of \u03b5. We performed comparative simulations (not shown) that demonstrated that the leak current has a subtle modulatory effect on the dynamics during the recovery from a burst. We chose to retain the leak term in G because it helped match the qualitative behavior to F without contributing to model complexity or computational cost. I H has a negligible influence during spiking and the active phase of bursting, but plays a role in the recovery of the cell between bursts and thus cannot be omitted. However, as noted earlier, the influence of the A and NaP currents remain smaller than \u03b5 throughout tonic spiking and bursting. Thus, we removed their associated activation (m) and inactivation (h) variables m A, h A, m NaP, and h NaP. Additionally, the observed slow time scale of h CaT and its minimal fluctuation over the tonic and bursting cycles (not shown) leads us to approximate it to a constant value of 0.65.The similarity in the bursting regime is different and requires rescaling of the variables. To explore this, we plotted the steady-state activation curves , , and  (Fig.\u00a0(b), left panel) as a function of the S/N compartment\u2019s membrane potential, V. These steady-state activations are comparable in size when V remains in the tonic regime as indicated by the white region. In the bursting regime these targets diverge as V depolarizes (left panel, gray region) as do the time scales (Fig.\u00a0(b), right panel). It is clear that a single lumped variable will not suffice for both regimes, but as part of our hand-tuned reduction using conventional methods we made two state-dependent forms of lumping in the model as described below.We lump m CaT, m CaS, and n Kd together into a single variable x depending on whether x is above or below the threshold of \u201cnonlinear activation scaling,\u201d chosen at \u03b8\u2009=\u20090.14 as the observed excitability threshold: the point where the steady state activations begin to diverge as V is increased (Fig.\u00a0(b), dashed line in the left panel). Note that our analysis is not sensitive to the precise value of this threshold. When x<\u03b8, the rescaling is linear x and is described by the equation for m CaT, the most dominant variable for that regime (although m CaS also works satisfactorily). Above this threshold, m CaT and n Kd are nonlinear rescalings of x according to a curve-fitting method (described in Appendix ). In this range, x is described by the equation for m CaS (which is now most dominant). The result of these reductions is a set of equations with nine dynamic variables. Although this model is technically \u201chybrid,\u201d in that its definition involves a discrete change to the vector field as a function of the state (Van Der Schaft and Schumacher ), this change is minor and does not modify the dimension of the system. We contrast this with the more aggressive reduction we perform in the next section.For the bursting regime we performed a similar analysis to obtain four sub-regimes, the first two of which were identical to those of tonic spiking because they involved the same active variables. The remaining sub-regimes were drawn from epochs 7\u201315 in Fig.\u00a0(b) according to the following criterion: the exact pattern of active variables need not be reproduced perfectly by the sub-regimes, but a minimal number of sub-regimes were desired. The following observations guided this choice. In epochs 7\u201315 the intracellular calcium concentration [Ca  2+ ] is in its high state (not shown). I KCa becomes active in epochs 12\u201315, but its slow time scale of activation means that it builds up slowly only when the high state of [Ca 2+ ] is reached. (Note the dominance of \u03a8 KCa at the end of the burst and during the interburst interval as shown in Fig.\u00a0(b) I & II, bottom panels.) This suggests that m KCa should be included in all the remaining sub-regimes in order that it can become a dominant variable at the right time during a burst, avoiding what can be loosely called a \u201cshadowing error\u201d (Clewley et al. ). Despite the fact that epochs 7 and 8, respectively, have the same active inputs as epochs 1 and 2, the high state of [Ca 2+ ] implies that the dynamics for the CaS and CaT activations are not the same as in epochs 1 and 2. This suggests making a sub-regime that is a high-[Ca 2+ ] analog of the first sub-regime. Thus, we chose sub-regime 3 to have the variables present in sub-regime 1 and also m KCa and [Ca 2+ ].The pattern of active variables in epochs 7\u201312 roughly follows that of epochs 1\u20136 except without the axial input. Although the influence of the axial input is not large enough to be considered dominant in epochs 7\u201312, it plays a modulatory role in creating the small S/N voltage spikes. We chose sub-regime 4 to be a high-[Ca 2+ ] analog of sub-regime 2, in symmetry with the pattern of epochs condensed into sub-regimes 1 and 2. As we are primarily interested in the differences between tonic spiking to bursting, and in the interest of simplicity, we chose not to create a separate sub-regime for the K(Ca)-dominated epochs 13\u201315, or for the remainder of the bursting trajectory. The sub-regimes associated with bursting are shown in Fig.\u00a0(a) (right panel).The four sub-regimes that constitute model L are formally defined in Appendix . Computationally, they are treated as four smooth vector fields connected by discrete events. In each sub-regime, the dynamics of the model PD neuron is dominated by a different subset of the variables. Figure (b) depicts model L as a state transition diagram, in the form of a 2\u2009\u00d7\u20092 grid where each of the four states represents one sub-regime of the vector field. The arrangement on the grid reflects the suggestion that states 3 and 4 are versions of states 1 and 2 (respectively) except with high levels of I Ca/I KCa activation. As such, states 3 and 4 require explicit dynamics for [Ca 2+ ] whereas, for states 1 and 2, DSSRT suggests that [Ca 2+ ] can set to be a constant low value. The value was set at 3.8\u00a0\u03bcM, the maximum value of [Ca 2+ ] in the tonic regime and the minimum in the burst regime. The columns of the grid provide a division whereby states 1 and 3 represent the dynamics between axonal spikes, and states 2 and 4 represent the dynamics during axonal spiking and the subsequent refractory period.From the output of DSSRT it can be observed that the transitions to sub-regimes 2 and 4 from sub-regimes 1 and 3 (respectively) occur when V axon depolarizes to approximately 5\u00a0mV above V. This provides a convenient way to define the transition event in the hybrid system specification. In the opposite direction, the transitions back to sub-regimes 1 or 3 occur after the spike refractory when V and V axon return to within 5\u00a0mV of each other. This choice of transition events reflects the intuition that the axial coupling is most influential when the difference |V\u2212V axon| is sufficiently large. The validity of these transition choices is critical to extending our method to models with two or more compartments. During the rapid changes in Vaxon during spiking, dominant scale analysis of the axonal compartment shows that it is effectively independent of the S/N compartment and therefore can be treated temporarily as an autonomous input (Clewley et al. ). This guarantees that after sodium currents become dominant in the axon, the spike will occur and we will transition from sub-regime 1 to 2 or from 3 to 4 and back as described.Based on observations of model G, the transitions from 1 to 3 and from 2 to 4 are determined by the increasing of m CaT through \u03b8. In states 3 and 4 (high I Ca/I KCa activation), I Ca increases significantly in a positive feedback loop with V. When the S/N compartment becomes persistently depolarized there is a steady increase in [Ca 2+ ] and consequently the slow I KCa becomes fully activated. This terminates the burst, returning the system to state 1. The return to low I Ca/I KCa activation occurs when m KCa decreases below the threshold \u03b8 KCa\u2009=\u20090.1, which happens later than the fall of m CaT below \u03b8 because m KCa decays more slowly. Therefore, m KCa is the more accurate indicator for the return to low I Ca/I KCa activation. Note however that the qualitative dynamics of L are insensitive to the precise choices of the event transition thresholds.Model L successfully reproduces both the tonic and bursting behaviors seen in experiments using different patterns of state (i.e. sub-regime) transitions (Fig.\u00a0(b)) without need for the hand-tuned curve fitting (as in model G). We observe that while tonic spiking, states 3 or 4 are never entered. In contrast, the bursting regime begins in the same way as the tonic regime but at some point enters state 4 (from states 1 and 2) and continues to oscillate between states 3 and 4 until it reverts to state 1 from state 3. As expected, neither the spike nor burst periods were accurately reproduced as these are influenced by NaP, A and H currents that were removed. Model L also produces intermediate bursts when g CaT and g CaS were set to 120% their reference values (inset of Fig.\u00a0). The intermediate bursts involved [Ca 2+ ] peaks between 8 and 13\u00a0\u03bcM that were comparable to those in F and G.Phase plane diagrams for analysis of high-dimensional systems have limited usefulness because the diagrams only represent projections of trajectories and cross-sections of the null-surfaces at individual time points. In general, intersections of the nullcline (cross-sectional) curves in a non-autonomous phase plane do not correspond to actual equilibria. Nevertheless, one can obtain useful information about transient dynamics from such diagrams with careful consideration for the fine structure of the dynamics at different time points along a trajectory. We will demonstrate the use of phase-plane analysis in model L to characterize the transition to bursting in the PD neuron by comparing hybrid states 2 and 4. The precisely defined domains of analysis for the sub-regimes of model L and the lower dimensionality compared to models F or G allow greater confidence in the interpretation of the phase planes. In particular, they enable us to neglect the effect of dynamics in the remainder of the system.In the tonic regime (Fig.\u00a0(a)), I axial (t) depolarizes V quickly from V(t 0) to V(t 1) but this depolarization is not enough for m CaS to cross its activation threshold \u03b8\u2009=\u20090.14 (dotted line) before I axial(t) hyperpolarizes V to V(t 2). In contrast, for state 2 in the bursting case (Fig. (b1)), the higher value of g CaS modifies the shape of the V nullclines, and therefore the location of the  values. Together with a decrease in \u03c4 CaS (V), the axonal compartment spike still acts to depolarize V, allowing m CaS to increase and cross \u03b8. This leads to an early exit from state 2 into state 4. In state 4 of the burst regime (Fig. (b1)), the V nullclines now additionally depend on n Kd, which distorts their shape compared to state 2. In particular, the V nullclines have two crossings with the m CaS nullcline, and the upper one is repelling. The axonal compartment spike no longer restores  to a hyperpolarized value. Instead, there is a net increase in  during the spike, seen by the V nullcline increasing to the right, and V continues to rise as the state is exited.The main difference between states 2 and 4 through the transition to bursting is that the effect of I axial is diminished as I Ca builds up (see Fig.\u00a0(b)II). The fact that m CaS has a slower time scale than the V axon spike rise time means that I axial (t) has an asymmetric effect: for every V axon spike up-sweep, V quickly depolarizes by an amount roughly controlled by the initial value of m CaS (i.e., through I Ca), but when the V spike down-sweep starts to hyperpolarize as a response to the V axon spike down-sweep, the slower I Ca has increased and counters the remainder of the I axial (t) peak. This process repeats over each axonal compartment spike, with the V spike getting smaller each time because of the increasing level of I Ca (a phenomenon often referred to as spike block).In the past few years there has been an explosion in the number of detailed biophysical models of neurons and networks (Hines et al. ). Due to the large number of variables involved in most biophysical models, a mathematical analysis of the dynamics of the solutions or the transitions between different modes of activity is prohibitively difficult. As a result, such models are often analyzed by simulating the model at various parameter values. This provides an understanding of the roles of different components but produces little insight into how these components interact to produce specific dynamical outputs. Our focus on local parameter variation and bifurcation is complementary to the broader search for parameter ranges in which these behavioral regimes can be found (Prinz et al. ).We have shown that the dominant scales method (Clewley et al. ) can be extended to multi-compartment models. We applied the method to the study of tonic spiking and bursting in the two-compartment model PD neuron of Soto-Trevi\u00f1o et al. () (model F), in order to discover the low-dimensional dominant dynamics in each of the two mechanisms. Guided by the output of the software tool DSSRT, we first derived a single, globally reduced, model G that qualitatively captured the tonic and bursting regimes of F. This was intended as a \u201cbest effort\u201d to use conventional reduction techniques for comparison with our method. For instance, in attempting to minimize the number of dimensions in G, ad hoc approximations were made that involved hand-fitted rescaling functions. The success of model G affirms the presence of an effective activation threshold separating low and high [Ca 2+] states\u2014corresponding to the tonic and bursting regimes, respectively. We then derived a piecewise low-dimensional description of the dynamical system (model L) by more systematically analyzing the changing patterns of influence between the variables in the full dynamics. This derivation did not require the curve fitting of any functional relationships.Simulation of model L showed that it was capable of reproducing the various behaviors seen in F despite requiring integration of only six to nine variables. The number or variables can be decreased by taking into account the results of a prior dominant scale reduction of the four-dimensional axonal compartment in Clewley et al. (), which have been omitted here. The analysis suggests that it is functionally equivalent to a one-dimensional integrate-and-fire compartment. Simulations of such a further reduced model are almost indistinguishable to the trajectories of L (not shown), and involve integrating only between three and six variables. Furthermore, an increase in g CaT and g CaS in F led to a transition from spiking to bursting via an intermediate form of bursting. Model L could also reproduce this transitional behavior with similar spike levels in [Ca 2+] to those observed in F and G, despite the fact that its derivation did not involve analysis of an intermediate bursting trajectory. These results indicate that our characterization of the tonic and bursting regimes was rich enough to support additional qualitative states present in the full system.For a dynamical system near a bifurcation point, asymptotic analysis or other rigorous methods may be used to derive an optimal normal form representation of the local dynamics. Although we have not established rigorous theory for our method, the correspondence in the results of dominant scale analysis applied to the original Hodgkin\u2013Huxley model with those of asymptotic analysis performed by Suckley and Biktashev () suggests that our method focuses on mathematically significant features in the dynamics and yields parsimonious and appropriate local models. Our method identifies temporal intervals in the dynamics of the full system within which local reductions are derived. Although much faster than doing traditional asymptotic analysis by hand, the cost of our method lies in the need to compute trajectories for hybrid dynamical systems, which involves accurate determination of zero-crossings of \u201ctest functions\u201d that define the state transition events between the individual vector fields. The changes in the pattern of hybrid states strongly suggest the presence of one or more bifurcations in the dynamics of the full system, although a rigorous bifurcation analysis is beyond the scope of this work. We fully expect that studying the hybrid model representation further could characterize other aspects of the original model PD neuron\u2019s dynamics but we did not attempt this in this study.The changing pattern of dominant inputs to the two compartments over the course of a spike shows that axial coupling has a subtler role than to promote synchronized activity. Between spikes the S/N compartment restores the axonal compartment towards its spike threshold via this coupling. Without axial coupling the axonal compartment remains quiescent and the depolarizing currents of the S/N compartment can build up without the axonal spikes to counter their effect. Thus, the high I Ca/I KCa state is inevitably entered, and the membrane potential V in F follows a trajectory that resembles the envelope of a burst without spikes. On the other hand, if the axial coupling is so strong that the two compartments are effectively unified, the cell only exhibits bursting (Soto-Trevi\u00f1o et al. , Fig.\u00a0).The finding that axial coupling plays changing roles in the PD neuron supports the view that current flowing between neural compartments acts in a more complex manner than merely as a co-promoter of activity or as a drive towards synchrony (Sherman and Rinzel ; Chow and Kopell ; Medvedev and Kopell ). For instance, it has been shown that the switch from spiking to bursting in pancreatic \u03b2-cells can be controlled by the strength of coupling between two electrically coupled cells having similar currents but different parameters (de Vries and Sherman ): stronger coupling led to spiking whereas weaker coupling led to bursting. As shown in Soto-Trevi\u00f1o et al. (), the PD neuron activity (model F), despite involving two very different compartments, can also switch from bursting to spiking as the strength of the axial coupling is increased. In light of the results described above, this transition can be interpreted as follows: the increase in axial conductance increases the influence of the axial current into the S/N compartment. The timing of the axonal spikes is such that it prevents the activation of the calcium currents from reaching the excitability threshold \u03b8, which has the resetting effect on the S/N membrane potential, as described as state 2 of model L.Our results and interpretations are also consistent with the description of the soma-dendritic \u201cping-pong\u201d mechanism for the Pinksy\u2013Rinzel model of bursting given by Bose and Booth (). The ping-pong mechanism relies on a separation of time scales between the two electrotonically coupled compartments, and a \u201cproper balance in strength or timing of interaction\u201d between them. In their analysis of a piecewise reduced model of a two-compartment neuron, Bose and Booth determined that the activation of I Ca during the active phase of the burst ultimately causes spike block and disrupts the ping-pong pattern, leading to a transition to the silent phase of the burst.Our method of dominant scales extends the principles of quasi-steady state approximation that is popularly used in modeling chemical kinetics (Murray ). While the quasi-state method focuses on separations in time scale only, our method also incorporates information about multiple scales in terms of a measure of influence of variables over each other. This method strives to bring greater objectivity to the process of model reduction through the use of an algorithmic process. However, as it is the case in other reduction methods and in the process of model development and analysis, interpretative steps may still be necessary in determining an efficient representation for a reduced model.Kepler et al. () describe a complementary method of systematic reduction known as \u201cequivalent potentials,\u201d which they apply globally to reduce the number of differential equations representing a system of ODEs. Golomb et al. () applied a variant of this method to their reduction of an STG model, which took into account calcium dependence and also the significant separation of time scales in the activation variables. Their reduced model showed qualitatively similar dynamics and bifurcation structure. The mathematical nature of lumping using equivalent potentials is rigorously determined in comparison to the hand-tuned curve fitting used for deriving model G. However, here we attempt to objectively measure which variables are dominant in a particular sub-regime before lumping any together, and we retain the original units for all the variables. As observed by Golomb et al., activation variables whose time constants were significantly dissimilar over the critical range of membrane potential values could not be lumped together. It was for this reason we pursued the locally reduced, hybrid model L, which makes the use of lumping more powerful. Model L included lumped variables in sub-regimes 1 and 2 where their time constants were similar, while the other sub-regimes used individual representations for those variables.Butera et al. () used a similar technique in their study of low-order reductions of an 11-dimensional bursting model neuron based on a multiple time scale analysis. Their reductions were most powerful when a heuristically determined interaction between the fast and slow sub-systems was included, making the model effectively hybrid. We also note that our separation of time scales in the reduced analysis of bursting is equivalent to the \u201ceffective leak\u201d method of Guckenheimer et al. ().The hypothesis that the maximal conductance parameters g CaT and g CaS can control a transition from spiking to bursting (or vice-versa) could be tested by applying neuromodulators that have a specific impact on these conductances, or by using a dynamic clamp protocol (Prinz et al. ) to add or remove calcium conductances. We have explored the variation of other parameters in order to control similar transitions, and have found qualitatively similar dynamics. In this study, we focused on calcium conductances in the S/N compartment, but changes that affect the effective structure of the axon compartment can also modify the behavior of both biological and model PD neurons. This would be consistent with experimental results which show that antidromic spiking activity in the axon can disrupt bursting in the de-afferented PD soma (Bucher et al. ), and switch it to tonic activity.The differential equations for the activation and inactivation variables of all the ionic membrane channels are already in this form.Each current is determined by the same equations as for the full model. The axonal compartment is the same as in F.The I CaT inactivation variable h CaT is set to the constant value of 0.7.The passive leak current is retained only in the axonal compartment.Currents that are missing in some states are held constant.[Ca 2+] is held constant at 3.8\u00a0\u03bcM for states 1 and 2. m CaS=n Kd=x\u2261m CaT  n Kd=y\u2261m CaS, axial coupling.CaT, CaS, Kd, K(Ca), [Ca 2+].CaS, Kd, K(Ca), [Ca 2+], axial coupling.Different states have different sets of lumped or un-lumped variables. Therefore, when a state transition was made some of these variables required initialization from the available activation values of the previous state. Transition event from state 1 to 2 and from 3 to 4: (V axon\u2212V\u2265E (\u201caxonal spike threshold\u201d) assuming initially that V axon\u2212V<E. Transition event from state 2 to 1 and from 4 to 3: V\u2212V axon \u2264 E (\u201crecovery threshold\u201d) assuming initially that V\u2212V axon>E. Notice that the left-hand sides of these inequalities are not absolute values, and therefore these two conditions are not inverses of each other. The first condition is met when V axon becomes more depolarized than V by at least an amount equal to E. The second condition is met when V axon depolarizes to become close to V, by an amount less than E. We used E\u2009=\u20095\u00a0mV. Transition event from state 1 to 3 and from 2 to 4: x \u2265 \u03b8. Transition event from state 3 to 1 and from 4 to 2: m KCa < \u03b8 KCa, where \u03b8 KCa=0.10.", "s10827-008-0103-9": "As described by others, an extracellular calcium-sensitive non-selective cation channel ([Ca2+]o-sensitive NSCC) of central neurons opens when extracellular calcium level decreases. An other non-selective current is activated by rising intracellular calcium ([Ca2+] i ). The [Ca2+]o-sensitive NSCC is not dependent on voltage and while it is permeable by monovalent cations, it is blocked by divalent cations. We tested the hypothesis that activation of this channel can promote seizures and spreading depression (SD). We used a computer model of a neuron surrounded by interstitial space and enveloped in a glia-endothelial \u201cbuffer\u201d system. Na+, K+, Ca2+ and Cl\u2212 concentrations, ion fluxes and osmotically driven volume changes were computed. Conventional ion channels and the NSCC were incorporated in the neuron membrane. Activation of NSCC conductance caused the appearance of paroxysmal afterdischarges (ADs) at parameter settings that did not produce AD in the absence of NSCC. The duration of the AD depended on the amplitude of the NSCC. Similarly, NSCC also enabled the generation of SD. We conclude that NSCC can contribute to the generation of epileptiform events and to spreading depression.A TTX resistant non-selective cation current (NSCC) that is activated by low [Ca2+]o was first described by Hablitz et al. () in chick dorsal root ganglion cells. Hablitz et al. () suggested that a similar current in central neurons could play a role in the generation of seizures and spreading depression (SD). MacDonald, Xiong, and collaborators (Chu et al. ; Xiong and MacDonald ; Xiong et al. ) detected in mammalian central neurons an NSCC that is activated by the lowering of [Ca2+]o. This channel is maximally open when extracellular calcium concentration is zero, and at the normal [Ca2+]o of 1.2\u20131.5\u00a0mM it permits the flow of a small current which, presumably, adds to the resting conductance of the cell. This channel is not voltage dependent and it is about equally permeable to Na+ and K+ but not to Ca2+ or other divalent and multivalent ions. It is, however, blocked not only by Ca2+ but also by other divalent cations (Xiong et al. ). With view of the depolarization caused by the activation of the [Ca2+]o-sensitive NSCC, Xiong et al. () also suggested that it could contribute to the generation of epileptiform seizures.Swandulla and Lux () discovered a non-selective cation channel in snail neurons that is activated by elevated intracellular Ca2+ concentration ([Ca2+] i ). A [Ca2+] i  dependent NSCC (often denoted ICAN) has also been recorded in hippocampal pyramidal neurons (Caeser et al. ; Crepel et al. ) and substantia nigra inhibitory neurons (Lee and Tepper ). The TRPM4b channel, which is expressed in mammalian neurons, has been characterized as a [Ca2+] i -activated, Ca2+-impermeable, monovalent cation-permeable channel (Fleig and Penner ; Launay et al. ). Since uptake of Ca2+ into neurons results simultaneously in an increase in [Ca2+] i  and a decrease in [Ca2+]o, if the two Ca2+ sensitive currents co-exist in the same cell, they are expected to reinforce one another and in fact could perhaps be generated by the same channel (see Section\u00a0).It has been known for a considerable time that low [Ca2+]o can induce seizure-like spontaneous discharges in intact brain (Sabbatani ) as well as in brain tissue slices (Jefferys and Haas ; Konnerth et al. ). Besides, at the onset of seizures, no matter how induced, [Ca2+]o drops precipitously (Heinemann et al. ,; Pumain et al. ,). Taken together, these two sets of observations suggest that the drop in [Ca2+]o, as also the increase in [K+]o, may be a link in one of the parallel feedback loops that can promote epileptic seizure discharges. The induction of spontaneous burst discharges by low [Ca2+]o has usually been attributed to reduced screening of neuron membrane surface charges leading to enhancement of voltage dependent inward Na+ current and hence reduced firing threshold (Hille ). Discovery of the [Ca2+]o-dependent NSCC adds another mechanism by which depletion of extracellular Ca2+ can promote neuron excitation even more powerfully.In a study of the ion currents that are responsible for hypoxic spreading depression-like depolarization (HSD) in hippocampal tissue slices M\u00fcller and Somjen () found that blockade of voltage gated Na+ as well as glutamate-controlled channels by a mixture of TTX, CPP and DNQX, while it postponed HSD, it did not usually prevent it. Adding Ni2+ to the blocking cocktail did, however, reliably suppress HSD (M\u00fcller and Somjen ). This lead to the conclusion that HSD is generated by the cooperation of several inward currents, one of which could be a non-specific cation current (NSCC) that is insensitive to TTX but is blocked by Ni2+ (M\u00fcller and Somjen , ). This as yet unidentified current could be the [Ca2+]o sensitive current, which is indeed inhibited by divalent cations.Earlier we have simulated seizure-like and SD-like processes (Kager et al. , , ; Somjen et al. ) in a neuron model based on the NEURON simulation environment of Hines, Moore and Carnevale (Hines and Carnevale ). It appeared that SD and HSD could be simulated in the presence of either a persistent Na+ current (INa,P) or one that resembled NMDA-controlled current (INMDA). When both were activated, SD occurred sooner and lasted longer than with either alone (Kager et al. ). Importantly, both simulated AD and SD required a substantial increase in [K+]o, illustrating pathological positive feedback where shifts in ion concentration influence the very channels through which the ions flowed in the first place (review in Somjen ).We now report computer simulations that demonstrate that, similarly to altered [K+]o/[K+] i  ratio, low [Ca2+]o can also promote seizure-like firing and SD-like depolarization due to activation of [Ca2+]o -sensitive NSCC. Some of the data appeared in abstract form (Somjen et al. ). A companion paper (Somjen et al. ) examines neuron-glia interaction mediated by ion flux.The morphology of the computer model used here is similar to the one described in Kager et al. (, , ) as the \u201csimplified\u201d cell, and it is illustrated in Kager et al. ().The model was created in the simulation environment \u201cNeuron\u201d of Hines and Carnevale (). A neuron and a glia-endothelial buffer space were aligned in parallel and communicated with each other through a limited interstitial space (IS). The neuron consisted of a soma with unbranched basal and apical dendrites attached to the opposite ends. The electrical currents were carried by the appropriate ions according to the Goldman\u2013Hodgkin\u2013Katz current equations (Hille ) and the ions accumulated in the relevant compartments. Diffusion between compartments was also implemented and charge was conserved and electroneutrality maintained in all compartments. The ions taken into account were Na+, K+, Ca2+, Cl\u2212 and A\u2212, the latter representing non-permeating anions in the neuron and in the glia cytosol. Ionic pumps were implemented that exchanged 3 Na+ for 2 K+, that exchanged 3 Na+ for 1 Ca2+ and ones that extruded Ca2+, all under the assumption of unlimited energy. (See Kager et al. , ,  and Somjen et al.  for details).Linear leak currents of Na+, and K+ plus a small NSCC (see below) and the pump currents controlled the resting potential of around \u221270\u00a0mV. Resting calcium concentration was set by adjusting the calcium extrusion mechanisms to reach a stable resting value around 55\u00a0nM at \u221270\u00a0mV.The glial membrane contained leak conductances and pump currents (corresponding to the \u201cpassive\u201d glia as defined in Somjen et al. () but no Ca2+ conductance. Calcium was not represented in the glial cytosol. The leak potassium conductance was much more dominating than the one of the neuron membrane, yielding a resting membrane potential in the glial cell of \u221288\u00a0mV (for details see Kager et al. , Somjen et al. ).The invariant Cl\u2212 conductance in both neuron and glial compartments allowed maintenance of ion balance during unequal redistribution of cations and of osmotic pressure with associated volume changes. To avoid excessive complication of the model we did not include a Cl\u2212 pump. This is justified because GABA induced or voltage dependent Cl\u2212 currents were not represented. The resting Cl\u2212 concentration gradient was in equilibrium at resting membrane potential. Impermeant anion (A\u2212) concentrations were chosen to balance the intracellular electrical charges. Changes in ionic strengths were translated into \u201cvirtual\u201d osmotic pressure and resulted in volume changes of the relevant compartments with resulting concentration changes, according to the set of equations described in detail in Kager et al. ().The main expansion of our model compared to its previous versions was the implementation of the Ca-sensitive nonselective cation conductance, gNSCC, (based on data given in Chu et al. ; Xiong et al.  and Xiong, personal communication). The nonspecific cation channel senses the extracellular calcium concentration (Fig.\u00a0(a)) and changes its conductance gNSCC for the monovalent cations K+ and Na+.We assumed gNSCC to be independent of voltage (Xiong et al. ) and gave it equal permeability for Na+ and K+. We implemented the Ca2+ gating as a fourth order gating process which yielded an empirical opening time constant of around 166\u00a0ms and a closing time constant of 25\u00a0ms (Fig.\u00a0(b)). These time constants were independent of current magnitude. The NSCC contributed to the resting conductance of the neuron. Whenever the conductance of the NSCC was altered, the leak conductances had to be slightly adjusted to maintain the balance of the total (net) membrane current and hence keep Vm steady at rest. The code for a representative selection of simulations presented in this paper will be added to the NEURON data base.The role of the currents generating the voltage response of Fig.\u00a0(b) are plotted in Fig\u00a0 (g\u2013i). As [Ca2+]o decreased in the interstitial space around the soma (Fig.\u00a0(d,e)), the NSCC was turned on (Fig.\u00a0(g,h)). During each action potential the NSCC showed brief positive spikes. Since the non-specific current is generated by the opposing simultaneous fluxes of Na+ and K+, the reversal potential of the NSCC is slightly negative relative to zero, (similalrly to EPSCs), and therefore during the positive overshoot of the action potentials the NSCC briefly reversed from inward to outward. In between spikes, the NSCC provided a sustained inward current. While the amplitude of the NSCC was small compared to the large surges of the transient Na+ current, INa,T, the NSCC provided just enough depolarization between action potentials to enable re-activation of the INa,T and hence the continuation of the AD (Fig.\u00a0(h)). The cooperative action of the INSCC and the INa,T generated the pacemaker potentials that kept the firing going. It should be noted that stronger stimuli could trigger AD even in the absence of the NSCC, assisted in that case by INa,P (see Kager et al. ), but the presence of the NSCC lowered the threshold for AD considerably. The AD was self-limiting (Fig.\u00a0(i)) because as EK subsided (Fig.\u00a0(b)), the neuron began to repolarize, while at the same time [Ca2+]o rose toward its rest level (Fig.\u00a0(i)) de-activating the NSCC, and these two shifts prevented re-activation of INa,T. EK began to recover as the ratio [K+]o/[K+] i  was being restored by the combined actions of the neuron 3Na/2K pump and the glial buffer (see also Somjen et al. ).We also simulated cerebral hypoxia. In order to explore whether INSCC by itself is capable of supporting hypoxic SD-like depolarization (HSD) we imitated the actions of TTX and glutamate antagonists (cf. experiments by M\u00fcller and Somjen , ) by setting the conductances of I Na,T, I Na,P and I NMDA to zero. To simulate severe hypoxia, the 3Na+/2K+ pump of the neuron and the glial cell as well as the Ca pump of the neuron were turned off; similarly to our earlier study of HSD (Kager et al. ). In this condition SD-like depolarization was generated without applying a depolarizing current stimulus, provided that g NSCC was present (not illustrated). In a subsequent simulation, I NSCC was also eliminated, imitating a condition in which all major Na+ currents were blocked, and now turning off the pump simulating the \u201chypoxic\u201d condition did not induce SD. Instead, the cell depolarized slowly due to the gradual increase of the [K+]o/[K+] i  ratio while the dendritic membrane current (I mem) remained weakly outward, carried mainly by I K,DR and I K,SK. (not illustrated). This behavior is similar to that seen in hippocampal tissue slices after oxygen deprivation in the presence of TTX, glutamate receptor antagonists plus Ni2+ (M\u00fcller and Somjen ).The feedback loops operating in these pathological conditions are schematically shown in Fig.\u00a0. With view of the inherent potential for runaway excitation one can ask, as Jung and T\u00f6nnies () have, how it is that all of us do not convulse every time we get out of bed. The processes that normally preserve the well controlled operation of cerebral tissue and those that upset its stability in diseased brains fall outside the present study.The screening of negative surface charges by Ca2+ (see Hille ) was not represented in the model. When [Ca2+]o decreases, removal of the screen has the same effect as a decrease in the membrane potential and hence it augments the excitability of neurons and adds to the epileptogenic effect. Incorporating surface screening in a computer model requires an other, major investigation. Other variables remaining equal, decrease in [Ca2+]o would also diminish all inward Ca2+ currents. When, as in our simulations, the decrease in [Ca2+]o is the consequence of influx of Ca2+ into neurons, the accompanying increase in [Ca2+] i  is expected to influence a number of calcium dependent processes. For our discussion the most important is the seemingly distinct NSCC that appears to be activated in hippocampal neurons, not by low [Ca2+]o, but by high [Ca2+] i  (Swandulla and Lux ; Caeser et al. ; Crepel et al. ; Lee and Tepper ). Removing Ca2+ from the bathing fluid depresses the [Ca2+] i -sensitive NSCC, presumably because less Ca2+ is available to flow into cells and raise [Ca2+] i , whereas low [Ca2+]o enhances the [Ca2+]o-sensitive current. In intact brain, however, during high frequency firing or SD, whenever Ca2+ flows into cells [Ca2+] i  increases at the same time as [Ca2+]o decreases and, apparently, both signals activate a current carried by monovalent cations. It is an open question whether they flow through two different channels or through the same one. Our simulations of the NSCC did not take into account the effect of the changes in [Ca2+] i .Had we incorporated the added effect of [Ca2+] i , it would presumably have changed only the scaling of gNSCC.Besides AD, the NSCC also facilitated the ignition of simulated SD and hypoxic SD-like depolarization. HSD was induced even when the transient and persistent Na+ currents (I Na,T and I Na,P) as well as the NMDA-controlled current were turned off. Eliminating these major inward currents imitated the administration of TTX, CPP and DNQX to brain tissue slices. In this condition withdrawing O2 from hippocampal slices induced HSD of greatly delayed onset indicating the operation of an unidentified TTX insensitive and glutamate-independent current. Adding Ni2+ to the \u201cblocking cocktail\u201d did prevent HSD (M\u00fcller and Somjen , ). The simulations supports the idea that the Ni2+-sensitive \u201cmissing\u201d current could be the Ca2+ sensitive NSCC.According to Xiong et al. () the Ca2+ dependence of the NSCC can be described by a function with Hill coefficient close to unity. This value was derived from data recorded in neurons cultured from the brains of newborn rats. When we tested the model using 1.4 for this parameter, we found that at the resting potential of \u221270\u00a0mV a rather large NSCC was flowing. Since there is no published evidence for a large NSCC in normal adult cerebral neurons at rest, we used a larger value, 3.4, for the Hill coefficient for most of the simulations (Fig.\u00a0). With this setting the NSCC in the un-stimulated neuron was small. Since, however, this choice was arbitrary, we also performed a number of simulations with a Hill coefficient of 1.4. The results were qualitatively similar with both settings of the Hill coefficient, but the less steep calcium-dependence resulted in greater readiness for AD generation (Fig.\u00a0). Incidentally, the Hill coefficient of the [Ca2+] i -activated NSCC generated by TRPM4b channels was either 4 or 6 depending on the treatment of the preparation (Launay et al. ).The model was, of course, simplified not just morphologically but also functionally. Some known ion currents were missing, as were the effects of pH change and of metabolic and other biochemical processes. We ignored these factors, because the questions asked were limited to a possible role of the [Ca2+]o sensing NSCC. Also absent were voltage controlled currents in the glial membrane. We did explore the effect of glial voltage gated K+ currents in a companion paper (Somjen et al. ). Lian and Stringer () found some experimental support for extracellular calcium being regulated by astrocytes, and this also deserves further exploration.Epileptic conditions can have many different causes, and by no means all have been discovered. It is possible that a shift in the [Ca2+] dependence, or increased conductance, or changing steepness of the activation function of either the [Ca2+]o- or the [Ca2+] i -sensitive NSCC, due either to genetic or to acquired anomaly, is one of them.This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.", "s10827-008-0101-y": "Deep brain stimulation (DBS) of the internal globus pallidus (GPi) is an established surgical technique for the treatment of movement disorders. The objective of this study was to propose a computational stereotactic model of the electrical distribution around the electrode within the targeted GPi in order to optimize parameter adjustment in clinical practice. The outline of the GPi can be defined precisely by using stereotactic magnetic resonance imaging (MRI) and from this it is possible to model its three-dimensional structure. The electrode and the distribution of the patient-specific parameters can then be co-registered with the GPi volume. By using this methodology, it is possible to visualize and measure the relationship between the electrical distribution of patient-specific parameters and the morphology of the GPi. The model could be applied in clinical practice to help determine the threshold for achieving a therapeutic effect and consequently may aid in optimizing parameter settings for individual patients.Deep brain stimulation (DBS) of the basal ganglia represents an effective surgical technique for the treatment of movement disorders, notably Parkinson\u2019s disease (PD; Benabid et al. ) and dystono-dyskinetic syndromes (DDS; Coubes et al. ; Coubes et al. ; Coubes et al. ; Krauss et al. ). Numerous studies have examined the use of DBS in treating other neurological disorders including epilepsy (Hodaie et al. ) and multiple sclerosis (Wishart et al. ), as well as psychiatric disorders including depression (Mayberg et al. ) and obsessive compulsive disorder (Nuttin et al. ). Continuous bilateral electrical stimulation of the internal globus pallidus (GPi) is now a commonly used treatment for DDS. A wide range of factors can influence outcome including clinical, surgical and electrical variables. To date, selection of stimulation parameters (i.e. contact configurations, frequency, pulse width and voltage) during follow-up is time consuming and based on clinical observation and physician experience. One way to establish optimal parameters could be to measure and visualize them in the context of the anatomy of each individual patient.The surgical protocol developed in our center for locating the anatomical target includes pre- and postoperative magnetic resonance imaging (MRI) with a Leksell Frame under general anaesthesia (Coubes et al. ). This procedure have been shown to have a very low complication rate (Vasques et al. ; 0% bleeding) and to be highly precise (Hariz ; Coubes et al. ; Vayssiere et al. ). The objective of the present study was to propose a computational anatomical model based on stereotactic MRI coupled with an electrical distribution model of DBS [i.e. isopotential (ISP), isofield (ISF) and current density] that can be applied both pre- and postoperatively to patients already implanted, in the most controlled environment possible. Thus, the model aims to optimize target localization at the time of surgical planning, allowing electrode localization after surgery and adjustment of electrical parameters in clinical practice. The model could also facilitate volumetric studies of target nuclei in movement disorders.With this aim in mind, we decided to monitor stereotactic points along the edges of the GPi directly on MRI (Starr et al. ). These coordinates were then used to build up a 3D model of the GPi, allowing calculation of the volume of the target. The target volume could then be correlated with the electrical parameters which were recorded from the internal pulse generator (IPG) during stimulation. This methodology allowed the individual structural spatial variations between patients to be taken into account. Electrical field values were correlated with the GPi anatomy in one reference patient.Subsequent to the design of the 2D-electrical field model (Hemm et al. ), a three-dimensional model of the in vivo stimulation system was developed which included the lead (four contacts numbered from 0 to 3, radius: 0.635\u00a0mm, electrodes height: 1.5\u00a0mm, separation: 0.5\u00a0mm; ref: 3389, Medtronic, Reuil-Malmaison, France) and the IPG (Model 7424, 7425, 7426, Medtronic).Brain tissue in the vicinity of the stimulating lead was assumed to be isotropic and homogeneous due to the low density of neurons in the GPi (Yelnik ) compared to other structures including the subthalamic nucleus (STN). The brain tissue around the lead was represented by a cylinder (with a radius of 40\u00a0mm and a height of 80\u00a0mm). The size of the cylinder was determined by the electrical field strength at its boundary. The maximum electrical field strength on the boundaries for monopolar stimulation is ~0.001\u00a0V/mm depending on the voltage used and which contacts are activated.The height of the cylinder was calculated from the mean length of the trajectory from the entry point on the cortical surface to optic tract (on the inferior surface of the brain). Calculations demonstrate that this value was necessary and sufficient because superior values do not modify the final result. Different stimulation modalities were applied: monopolar stimulation with single or multiple electrodes activated to serve as the negative pole (with the IPG serving as the positive pole); and bipolar, in which the current flows between two or more electrodes. The borders of the cylinder were considered to be insulators in the bipolar mode. In the monopolar mode, the IPG was modelled as an additional perfect conductor disk with a radius of 20\u00a0mm located at the bottom of the cylinder. The DBS model displays and calculates the electrical current distribution from the patient\u2019s electrical parameters recorded by telemetry. Furthermore, the software allowed the electrical distribution within the GPi to be displayed preoperatively by using theoretical impedance values. The radius of the modelled IPG was calculated by comparing and adapting the magnitude of the ratios of impedances in monopolar and bipolar modes in the model to the mean values measured in patients using the Medtronic radiofrequency console programmer. Preoperatively, the model impedance is representative of the clinical impedance (For example, contact 1 as negative pole: 1,185\u00a0\u03a9, contact 1 and 2 as negative poles: 811.6\u00a0\u03a9, contact 1 as negative pole and contact 2 as positive pole: 1,478.8\u00a0\u03a9).Possible interactions between the two implanted electrodes in bilateral stimulation were not taken into account.Boundary conditions were defined according to given potentials imposed at the metal contacts (depending on the specific mode of stimulation) and to zero values of the normal electrical field component (En\u2009=\u2009\u2212dU/dr) at the insulating surfaces of the central electrode and of the external cylinder. The latter condition is a direct consequence of the requirement that the component of the current density, normal to the surface of an insulator, is zero. The non-stimulated contacts are electrical conductors that can thus present a non-zero potential. The initial value for these contacts is an unknown constant potential to be determined from the fact that the total current is zero. A finite differences method was used for numerical analysis. An analytical description near the metallic edges was incorporated into the finite-difference scheme. The space step size is variable; it is smallest near the electrode and largest near the cylinder. For example, the distance between successive points near the contact is ~0.05\u00a0mm (z and r), whereas it is ~1\u00a0mm (z and r) near the cylinder. The singularities near the extremities of the contacts were approached by a Taylor dominant order expansion (order\u2009=\u20090). The electrical field and the current density were deduced from the potential.The stimulus waveform generated by the Medtronic IPG is biphasic, and the technical amplitude of the stimulus waveform is defined by the peak-to-peak voltage of the cathodic and anodic phases (Butson and McIntyre ). The true stimulus waveform generated by the Medtronic IPG was used rather than the amplitude registered on the clinical programmer in order to improve the accuracy of the DBS model. The stimulation waveforms of the electrical settings currently used in our center were recorded from the Soletra IPG on a digital storage oscilloscope (Tektronix, France) and the results were integrated on the computer program. The measurements were comparable with those of the Butson and McIntyre study (Butson et al. ).The stereotactic protocol used for the localization of the anatomical target in our centre involves a pre- and postoperative stereotactic 1.5-T MRI (slice thickness of 1.5\u00a0mm for both T1- and T2-weighted images) under general anaesthesia (Coubes et al. ). The efficacy and reproducibility of the therapy are based on a precise delineation of the GPi borders (Vayssiere et al. ; Vayssiere et al. ).The system Eq. () was solved by using the LU factorization. Once the weights d  i  had been determined, the implicit equation could then define the surface to be reconstructed.The theoretical models were computed by using a C/OpenGL computer program that applies the Marching Cubes algorithm of Lorensen and Cline (Lorensen and Cline ).Two steps were required for correlating the anatomical information with the electrical distribution in a given patient. Firstly, C-points and target coordinates including the trajectory angles were registered on the software. Secondly, the electrical parameters of the lead were selected for each patient and were then recorded from the IPG, in order to visualize, manipulate and measure the correlation between the electrical distribution and the GPi anatomy of the patient. The software displayed an interface where the electrical settings of the lead could be changed (voltage, electrode configuration and impedance). It was possible to extrapolate the line representing a specific ISF value (or ISP value or current density value) from a group of lines (from 0 to 1\u00a0V/mm) and to calculate its surface area and the volume enclosed within it. Zoom, translation and rotation tools were also implemented.The volumes stimulated by each ISF line were studied at 3 and 24\u00a0months. Thereafter, three ISF values were compared more specifically: 0.1, 0.2 and 0.4\u00a0V/mm.Application of this model to the present study has been possible using standard PCs, with several working systems. The compilation and execution of the program take 1\u00a0min and 45\u00a0s for visualization with a grid of 256\u2009\u00d7\u2009256\u2009\u00d7\u2009256 (6\u00a0s with a 100\u2009\u00d7\u2009100\u2009\u00d7\u2009100 grid), without taking into account the time necessary to define the C-points on MRI (which is operator dependant). The surface was defined with a mathematical curve representing the GPi which could be calculated at different levels of detail by increasing the size of the grid. Preoperatively, the software allowed the display of the electrical distribution within the GPi of the patient by using theoretical impedance. The display and calculation of the surface and volume of both the GPi and the intersection between the electrical distribution and the GPi were possible (Fig.\u00a0) i.e. the intersection between the volume of each electrical field value and the GPi volume. In clinical practice, taking into account the patient electrical parameters recorded by telemetry, the physician can thus visualize, manipulate and calculate the electrical distribution around the DBS lead in the anatomy of patients.Electrical field distribution was correlated with the patient\u2019s anatomy by using the postoperative stereotactic MRI, which gave the anatomical information, the target coordinates and the trajectory angles of the leads.At 2\u00a0years of follow-up, with an improvement of more than 90% in the clinical part of the BMFDRS, the ISF lines from 0.025 to 0.4\u00a0V/mm reached the postero-lateral part of the right GPi and from 0.025 to 0.425\u00a0V/mm on the left GPi.Three electric field values were compared more specifically: ISF0.1, ISF0.2 and ISF0.4 (Fig.\u00a0).At 24\u00a0months, on the right side, the volume intersection (and surface intersection) with the GPi was 58.4\u00a0mm3 (22.8\u00a0mm2), 20.5\u00a0mm3 (7.29\u00a0mm2) and 10.35\u00a0mm3 (1.39\u00a0mm2) respectively (Fig.\u00a0). In contrast to the electrical parameters at 3\u00a0months, the ISF0.4 reached the postero-lateral border of the GPi. The lateral part of the GPi was also reached by the ISF0.1 and ISF0.2 and no electrical field value reached the postero-medial section. There were no significant changes in the left GPi.The objective of this study was to develop a computational MRI-based stereotactic anatomical model coupled with a DBS model in order to aid surgical planning, contact localization after surgery and parameter adjustments during the course of DBS.It is a commonly used technique in MRI studies to depict the outlines of structures of interest on brain slices and then to digitize them to create a stack of contours. Promising segmentation (automatic or manual) and energy minimization methods exist for modelling the basal ganglia. However, segmentation from MR images remains difficult because of the low levels of contrast and the absence of easily visible contours due to unstable anatomical limits between structures. Segmentation of the ventricular system can be performed relatively easily because there is a definite contrast between cerebrospinal fluid and the surrounding brain (Colliot et al. ; Ciofolo and Barillot ; Collins et al. ). However, up to now, segmentation of grey nuclei including caudate nuclei, thalami and GPi, the surgical target in DDS, seems to be more difficult. These difficulties are anatomically determined by the small size of the structures, and the heterogeneity and irregularity of their interfaces with the surrounding white matter. Particularly in the case of a disorder secondary to a lesion seated in the grey nuclei, their shapes can be irregularly modified and there is also a lower level of contrast between structures (similar histological structures). Segmentation methods commonly use atlases (Bloch et al. ; Xue et al. ) and deformable templates (Pitiot et al. ) based on statistical shape training. The drawback of these techniques is the inter-individual anatomical variation which has major impact on the final precision. This inter-individual variation has been studied previously (Vayssiere et al. ). The differences between the target coordinates obtained with direct visualization by MRI and those based on atlases prompted the present study. The actual location of the target might differ significantly from the theoretical statistical target location due to individual variation. The advantage of the approach put forward here is that the individual anatomy of the patient is taken into account for the surgical planning. Each C-point can be localized within the stereotactic frame by direct visualization of its coordinates under visual control, which is an important feature of this model compared to previously published models. The resulting unorganized collection of scattered data points allows a semi-automated reconstruction of the GPi surface under visual control. Field-based implicit surface methods have become increasingly popular in recent years with Blobby Molecules (Blinn ), R-functions (Pasko et al. ) and convolution surfaces (Bloomenthal and Shoemake ), all with different applications: shape morphing, natural phenomena simulation and space deformation. The method used in this study for interpolating the scattered points was chosen because of its robustness when scattered points are non-uniform. The interpolation of the implicit surface using compactly supported radial basis functions (Morse et al. ) was appropriate for generating the complex organic shapes and has an advantage in terms of computational expense.One limitation of this study is the use of a homogenous model. Many groups use anisotropic brain models, which take into account the grey/white matter distribution and fibre direction using diffusion tensor imaging (DTI) to try to understand the mechanisms of action of DBS (Butson et al. ; McIntyre et al. ; Miocinovic et al. ). Tissue inhomogeneities such as small lacunar cavities can modify the shape of the electric field distribution (Astrom et al. ; Butson et al. ). McIntyre et al. (McIntyre et al. ) described a neural model for subthalamic nucleus (STN) stimulation. They determined the conductivities of the STN from diffusion tensor magnetic resonance data in one patient and correlated them with neuron cable models taking into account axon diameter and internodal spacing. This model (McIntyre et al. ) gives a more faithful representation of real brain structures by reflecting the heterogeneity of the STN. Astr\u00f6m et al. (Astrom et al. ) studied the effects of cystic cavities on electrical field distribution in the GPi by modelling a grey matter nucleus with and without cerebrospinal fluid filled cystic cavities to mimic Virchow\u2013Robin perivascular spaces (VRPS). The GPi and the VRPS were modelled as spheres and not in their irregular shapes. Butson et al. (Butson et al. ) developed a patient-specific model of STN for Parkinson\u2019s disease to predict the volume of tissue activated by DBS. The model showed that the volume of tissue activated by DBS differs between isotropic and diffusion tensor MRI. These approaches are promising because they take into account the heterogeneity of the brain. Their model gives an important indication of how the surrounding structures may influence the shape of the electrical field and subsequent neural response to stimulation.Up to now, even though highly specific, models using DTI are difficult to apply to neurosurgical and clinical practice where individual brain anatomy is the most important factor. The precision of the surgical procedure is an issue of major importance given the submillimeter accuracy of lead insertion needed to hit the target. DTI is not currently appropriated due to limited image resolution. An accurate measurement of small structures or interfaces between structures like GPi/GPe requires high spatial resolution, but high submillimeter DTI cannot be currently achieved. To create a model using DTI, it is necessary to use multiple image co-registration and atlas representations which involve spatial variability and increase the time of surgery. Furthermore, DTI could not be applied to the entirety of our population already implanted with a DBS system (Vasques et al. ). In this study, the risk of error due to multiple image co-registration and anatomical variability has been limited by using the stereotactic coordinates of targets selected directly on MRI. The model can also be applied in routine clinical practice in patients who already have a DBS system implanted by using the postoperative stereotactic MRI and the electrical parameters recorded by telemetry. Preoperatively, the model allows the display and the calculation of the electrical distribution within the GPi of the patient using theoretical impedance. Based on the evaluation of the in vivo impedance and current values measured in patients (Hemm et al. ), the theoretical impedance seems to be an acceptable approximation for the preoperative surgical planning. Hemm et al. (Hemm et al. ) showed that for patient with a greater than 80% improvement on the BFMDRS, the mean impedance with a single contact activated as the negative pole is 1,283\u2009\u00b1\u2009215\u00a0\u03a9; close to the calculated impedance in our model (1,185\u00a0\u03a9). However, prediction of impedance preoperatively is difficult because the electrical brain impedance of each individual patient is unique (Hemm et al. ). Given the model parameters and the electrode configuration of the patient in the present study at 3\u00a0months, the calculated impedance is 1,185\u00a0\u03a9, which is close to the recorded impedance on the left side (1,208\u00a0\u03a9) and differs from the recorded impedance on the right side (1,505\u00a0\u03a9) by 420\u00a0\u03a9. Preoperatively, depending on the impedance for a given patient, the results may represent an overstimulation of the stimulated volume.For the purposes of this study, the brain tissue in the vicinity of the stimulating lead was assumed to be homogeneous and to have an isotropic resistance due to the low density of neurons in the GPi compared with the STN (Yelnik ). This would seem to be acceptable especially if it is assumed that only the ISF lines within the GPi have a therapeutic effect. It has been reported that the most effective position for the lead is in the posterior, medial and ventral part of the GPi (Coubes et al. ; Tisch et al. ); This has been shown to be the sensori-motor area (Laitinen et al. ) and close to the output pathway i.e. the ansa lenticularis. If these fibres must be stimulated to obtain optimum therapeutic effect, it can be concluded that these fibers must be within the volume to be stimulated. The GPi somatotopic organization (Vayssiere et al. ) favours the hypothesis that different neuron populations within the GPi are responsible for the control of different body parts. These populations would be organized in several fascicules rather than a single one containing all output fibres. The actual target is a network-specific target probably influencing remote structures and representing only a subsection of the stimulated volume. The application of the model to one clinical case indicates how this combination of electrical and anatomical information could be used to quantify the electric field necessary to produce the physiological effect. The activation of neurons is triggered by membrane depolarisation. This is due to a variation in the potential difference influenced by several factors including electrical and anatomical properties of neurons, distance and orientation relative to the electrode, tissue resistivities, stimulation waveform and configuration mode. The electrical field (voltage gradient parallel to the fiber) is probably the most important factor responsible for triggering the action potential because it varies with fiber orientation relative to the electrode. The visualization and quantification of specific isofield lines facilitates the determination of the stimulated region around the electrode. Depending on the position within the target, the trajectory angles of the lead, the patient electrical parameters, and the volume of the target and its morphology, an electric field value could emerge as the threshold value of the stimulated volume necessary to produce a therapeutic effect when applying this methodology to our entire population of patient with dystonia. For example, the electric field value of 1\u00a0V/mm can probably be considered to be insufficient as the resulting target (~1\u00a0mm width around the stimulated contact) seems too limited to be responsible for the clinical effect. In contrast, the value of 0.1\u00a0V/mm covers a large part of the GPi, even extending beyond the postero-lateral part of the GPi. We propose a method of modelling the electric field lines with the aim of identifying a threshold value and, consequently, the critical volume to be stimulated within the target to produce the optimum therapeutic effect. Its application to our dystono-dyskinetic patients in clinical practice could help to determine the threshold of the electrical field necessary to obtain a therapeutic effect, and possibly define the target as specifically as possible.", "s10827-008-0096-4": "Neurons in the superior colliculus (SC) are known to integrate stimuli of different modalities (e.g., visual and auditory) following specific properties. In this work, we present a mathematical model of the integrative response of SC neurons, in order to suggest a possible physiological mechanism underlying multisensory integration in SC. The model includes three distinct neural areas: two unimodal areas (auditory and visual) are devoted to a topological representation of external stimuli, and communicate via synaptic connections with a third downstream area (in the SC) responsible for multisensory integration. The present simulations show that the model, with a single set of parameters, can mimic various responses to different combinations of external stimuli including the inverse effectiveness, both in terms of multisensory enhancement and contrast, the existence of within- and cross-modality suppression between spatially disparate stimuli, a reduction of network settling time in response to cross-modal stimuli compared with individual stimuli. The model suggests that non-linearities in neural responses and synaptic (excitatory and inhibitory) connections can explain several aspects of multisensory integration.In the natural environment, stimuli of different modalities occur at various locations in space and time, initiated by different events. Integration of information from multiple senses is fundamental to determine the relationships among different sensory signals and to enhance detection and identification of external events to trigger the correct responses (Stein and Meredith ; Welch and Warren ).Evidence for multisensory convergence at neural level has been found in various structures of the mammalian brain outside the primary sensory areas (Stein and Meredith ). The most studied locus of multisensory interaction is a layered midbrain structure, the superior colliculus (SC). This structure plays a critical role in the generation and control of orienting movements of the head and eyes towards external events (Sparks ; Stein and Meredith ). Many neurons in the deep layers of the SC receive converging visual, auditory and somatosensory afferents from various subcortical and extraprimary cortical sources (Edwards et al. ; Huerta and Harting ; Wallace et al. ). Responses of such neurons to a combination of stimuli delivered in multiple sensory modalities differ significantly from those evoked by any of their unisensory inputs in a way that substantially facilitates the role of the SC in controlling attentive and orientation behaviour (Frens et al. ; Schroger and Widmann ; Stein et al. ). Several studies have been carried out in order to identify the principles ruling multisensory integration in SC neurons and a consistent amount of physiological data has been gathered.A multisensory SC neuron has multiple receptive fields (RFs), one for each of its sensory modalities. Modality-specific RFs are in spatial register, i.e. they represent similar regions of sensory space, so that they overlap. This receptive field alignment is critical for normal multisensory processes in SC (Meredith and Stein ). According to this organization, when two different sensory stimuli (e.g., auditory and visual) are present at close spatial proximity (as it occurs when they derive from the same event), they fall within the overlapping receptive fields of the same neuron. The combination of these inputs is typically synergistic producing a neuron\u2019s response which is significantly greater than that evoked by the most effective of the two unimodal inputs individually (multisensory enhancement; Bell et al. ; Kadunce et al. ; Meredith and Stein ,; Wallace et al. ). In some neurons, the response enhancement may even exceed the sum of their individual unimodal responses (superadditivity; Kadunce et al. ; Meredith and Stein ; Perrault et al. , ; Wallace et al. ). On the other hand, when the two stimuli are presented at different locations (i.e. they likely derive from different events) so that one is within and the other outside the RF of the target neuron, two alternative results can be observed: either no interaction occurs or the neuron\u2019s response to the within-field stimulus is considerably depressed (multisensory depression; Kadunce et al. ,; Meredith and Stein ; Wallace et al. ). Multisensory depression is assumed to derive from the presence of an inhibitory region which surrounds the excitatory receptive field (Kadunce et al. ). Globally, these properties of cross-modal integration are known as the spatial rule. Overt behaviour responses have been found to follow this property of SC neurons: the probability of a correct orientation response to a visual target is increased, and the reaction time reduced, by a spatially coincident auditory stimulus, whereas the orientation response is degraded when the auditory stimulus is spatially disparate (Frens et al. ; Schroger and Widmann ; Stein et al. ; Wilkinson et al. ).The spatial rule is accompanied by another well known integrative principle called inverse effectiveness, according to which the magnitude of the multisensory enhancement is inversely related to the effectiveness of the individual unimodal stimuli: combinations of weakly effective unisensory stimuli produce proportionally greater multisensory enhancement than more effective unisensory stimuli (Meredith and Stein ; Perrault et al. , ; Stanford et al. ; Stein and Meredith ; Wallace et al. ). Inverse effectiveness has functional sense in behavioural situations: the probability to detect a weak or ambiguous stimulus benefits more from multisensory enhancement than a high-intensity stimulus which is easily detected by a single modality alone (Jiang et al. ; Stein et al. ; Wilkinson et al. ).These principles provide a highly influential framework to explain the multisensory interactions observed in SC neurons and to predict how some characteristics of the stimuli (such as spatial location, time of occurrence and intensity) influence the integrative response, both at neuronal and behavioural level.Some efforts have been made to derive implications for neuronal mechanisms underlying multisensory integration starting from phenomenological results. For example, consistency between the inverse effectiveness principle and a neuronal model including threshold and saturation properties has been indicated by Stanford et al. (Stanford et al. ).A fundamental contribution to investigate the mechanisms by which modality-specific inputs achieve integration in SC can be obtained by using neural network models and computer simulations. Anastasio, Patton et al. (Anastasio et al. ; Anastasio and Patton ; Patton et al. ; Patton and Anastasio ) proposed some models based on information theory, in which neurons use the Bayes rule to compute the probability that a target is present in their receptive field. With these models, the authors accounted for the existence of cross-modal enhancement and within-modality suppression, and for the existence of both multimodal and unimodal neurons. However, we are not aware of any model which considers the intensity of the inputs, their spatial disposition, and competition among neurons with different receptive fields to detect a target.The aim of this work is to develop a neural network model, based on neurobiologically plausible mechanisms, which is able to reproduce and explain several in vivo results in anesthetized animals. The model includes three neural networks, which communicate via synaptic connections. Two of them are unimodal and represent neurons coding visual and auditory stimuli, respectively; these networks may represent unisensory cortical areas projecting afferents to SC; a downstream network, representing multimodal neurons in the SC, receives information from the upstream unisensory networks via feedforward synapses and integrates this information to produce the final response. Furthermore, neurons in each network are interconnected via lateral synapses. The activity of each neuron is described by a sigmoidal relationship with a threshold and a saturation. By adopting the previous structure the model is able to reproduce a number of within- and cross-modality interactions.In this section we will first describe the general structure of the model. Then all equations are presented and justified. Finally, parameter assignment is discussed, on the basis of previous neurophysiological data.The model is composed of three areas. Elements of each area are organized in NxM dimension matrices, so that the structure keeps a spatial and geometrical similarity with the external world: neurons of each area respond only to stimuli coming from a limited zone of the space (see Fig.\u00a0). Neurons normally are in a silent state (or exhibit just a mild basal activity) and can be activated if stimulated by a sufficiently strong input. Furthermore, each neuron exhibits a sigmoidal relationship (with lower threshold and upper saturation) and a first order dynamics (with a given time constant). The two upstream areas are unimodal, and respond to auditory and visual stimuli, respectively. A third downstream area represents neurons in the SC responsible for multisensory integration. These three areas have a topological organisation, i.e., proximal neurons respond to stimuli in proximal position of space.Each element of the unisensory areas has its own receptive field (RF) that can be partially superimposed on that of the other elements of the same area. The elements of the same unisensory area interact via lateral synapses, which can be both excitatory and inhibitory. These synapses are arranged according to a Mexican hat disposition (i.e., a circular excitatory region surrounded by a larger inhibitory annulus).The elements of the multisensory area in the superior colliculus receive inputs from the two neurons in the upstream areas (visual and auditory) whose RFs are located in the same spatial position. Moreover, elements in the SC are connected by lateral synapses, which also have a Mexican hat disposition.The multimodal neurons in the SC send a feedback excitatory input to the unimodal neurons whose RFs are located in the same spatial position; in this way, detection of a multimodal stimulus may help reinforcement of the unisensory stimuli in the upstream areas.The receptive fields of unisensory areas.According to Eq. (), a stimulus presented at the position x,y excites not only the neuron centered in that zone, but also the proximal neurons whose receptive fields cover such position.The activity in the unisensory areasThis is the sum of three components: , that represents the external sensory input; , coming from the intra-area synapses; and , the contribution of the feedback coming from the superior colliculus.where \u03d1  s  defines input value at which neuron activity is half the maximum (central point) and p  s  sets the slope at the central point.The activity in the multisensory areaThen, the activity of a multisensory neuron is computed from its input by using equations similar to Eqs. () and (). and  have been given so that the receptive fields of the visual neurons are approximately 10\u201315\u00b0 in diameter, and those of acoustic neurons approximately 20\u201325\u00b0 in diameter, according to data reported in (Kadunce et al. ).  and  are set to 1, to establish a scale for the inputs generated by the external stimuli.Parameters which establish the extension and the strength of lateral synapses in the unimodal areas (i.e., , ,  and  for the visual area, and ,,  and  for the acoustic area) have been assigned to simultaneously satisfy several criteria: (1) the presence of an external stimulus produces an activation bubble of neurons which approximately coincide with the dimension of the receptive field; (2) according to data reported in Kadunce et al. (Kadunce et al. ) we assumed that the surrounding inhibitory area is much larger than the activation bubble; (3) inhibition strength must be strong enough to avoid instability, i.e., an uncontrolled excitation which propagates to the overall area.Parameters which establish the extension and the strength of lateral synapses in the SC areas (i.e., , ,  and ) have been assigned to warrant cross-modal and within-modal integration in agreement with data reported in Kadunce et al. (Kadunce et al. ,).For the sake of simplicity, these parameters have been chosen equal for all neurons, independently of the respective area. The central abscissa, \u03d1  s , has been assigned to have negligible neuron activity in basal condition (i.e., when the input is zero). The slope of the sigmoidal relationship, p  S , has been assigned to have a smooth transition from silence to saturation in response to unimodal and cross-modal inputs (Perrault et al. ). The time constant agrees with values (a few milliseconds) normally used in deterministic mean-field equations (Ben-Yishai et al. ). In particular, this value can be chosen significantly smaller than the membrane time constant (Treves ).The parameters of feedforward connections from the unisensory areas to the superior colliculus (i.e., k  v  and k  a ) have been set in order to: (a) have a significant multisensory enhancement; (b) have a greater dynamical range of multisensory neurons in response to cross-modal stimuli, compared with unimodal stimuli (i.e., a single stimulus cannot lead the SC neuron to saturation; Perrault et al. ). Furthermore, in this work we assumed that the effect of a visual stimulus on the SC neuron is moderately greater compared with the effect of an auditory stimulus. For the sake of simplicity, the feedback connections from the SC neuron to the upstream visual and auditory neurons (i.e., parameters F  a  and F  v ) have been set to small values in this work, assuming that this effect is normally modest. These parameters may be the subject of a sensitivity analysis in future works.Figure  shows an example of the activation bubbles in the three areas, simulated by the model in presence of a unisensory visual input (upper panel), a unisensory auditory input (middle panel) and two simultaneous visual and auditory stimuli (bottom panel).Figure  shows the response of a multisensory SC neuron evoked by a stimulus (or a pair of stimuli) located at different positions within and outside the receptive field. A bell shape is obtained, with the maximum located at the centre of the RFs. When the stimulus is placed at the border of the RF, neuron activity decreases down to ~9% of the bell peak. It is worth noting the strong enhancement of the multisensory response compared with the response to individual stimuli. Results displayed in Fig.\u00a0 resemble those experimentally obtained in SC cells by changing the stimulus locations in modality-specific and multisensory tests (see Fig.\u00a0 in Kadunce et al. ). In these tests, a bell shape was obtained with the best point (both for unisensory and multisensory stimulation) approximately positioned at the center of the overlapping region between the auditory and visual RFs. Multisensory response always overcame unisensory response. Neuron activity decreased down to approximately the 10% of the peak activity when the stimulus was applied at the border of the overlapping region, in agreement with model predictions.The dynamical ranges of a SC neuron are reported in Fig.\u00a0, for an auditory (dotted line), a visual (dashed line) and a multisensory (continuous line) stimulation. These responses have been obtained by using either a single stimulus (auditory or visual) of increasing strength or two paired stimuli (visual + auditory) located at the centre of the RF. The dynamical range is defined as the difference in neuron activity at saturation and at threshold. Furthermore, by way of comparison, the sum of the two unisensory responses is also presented in the same figure (dash-dotted line). Two aspects of these curves are of interest: first, the dynamical range to multisensory stimulation is much greater than that to a single stimulus. Second, the neuron exhibits a superadditive response at low values of the input stimuli (just above threshold), while the response becomes additive/subadditive at high stimulation levels.Figure  displays the interactive index computed at different values of the input stimuli. In particular, the top panel shows the response of a SC neuron to an individual auditory or visual stimulus (ranging from above threshold to saturation) located at the center of the RF, and to a combination of both paired stimuli. The multisensory response is always higher than the individual responses. From these data, the interactive index has been computed (bottom panel). According to the principle of inverse effectiveness, this index decreases from about 500% in case of weak unisensory stimuli (just above the threshold) down to 60\u201350% in case of strong stimuli (input values = 30 and 40 in the figure). It is worth noting that for input values above 25 neuron behaviour becomes subadditive (see Fig.\u00a0). Values of interactive index obtained with the model fall in the ranges reported in the literature. Perrault et al. () found values of interactive index between 1,000% and 300% in case of superadditivity, and between 100% and 20% in case of additivity/subadditivity. In Kadunce et al. (), the interactive index was 94% and 55% in case of subadditivity, and above 200% in case of superadditivity. Wallace et al. (), reports an average enhancement of 122% (with values even up 270%) for poorly effective stimuli, and of 31% (range between \u20136% and 67%) for highly effective stimuli.Figure  analyzes the inverse effectiveness principle by using the multisensory contrast. The upper panel shows contrast as a function of the auditory stimulus, computed when the visual stimulus is set to a small value (i  v \u2009=\u200912; just above threshold). Neuron behaviour is superadditive (i.e., contrast is greater than zero) for all values of the acoustic input. The bottom panel shows the same figure, computed using a high value for the visual input (i  v \u2009=\u200930; close to saturation). In this condition, neuron behaviour changes from superadditive to additive (contrast almost zero) or to subadditive by increasing the second (auditory) stimulus.Figure  shows the role of the distance between two stimuli on the integrated response. The panels (a) show the case in which a visual stimulus is located at the center of the RF, and either a second visual stimulus (within-modality interaction) or a second auditory stimulus (cross-modality interaction) is moved from the center to the periphery. The activity in the visual unimodal area, at the central position of the RF, is shown in the left panel, while the middle panel shows activity in the corresponding multimodal neuron. The panels (b) consider a central auditory stimulus (again with within-modality and cross-modality interaction with a second stimulus). The activity in the unimodal auditory area at the central position of the RF is shown in the left panel, while activity in the same position of the SC is shown in the middle panel. The results confirm that a second stimulus of a different modality located within the receptive field causes significant cross-modal enhancement, whereas in the case of a within-modality stimulus the enhancement is mild (i.e., a second stimulus of the same modality, located inside the RF does not evoke a significantly greater response). The absence of significant within-modality enhancement agrees with experimental data (Stein and Meredith ; Wallace et al. ). If the second stimulus is moved away from the RF, one can observe significant within-modality suppression as well as significant cross-modality suppression. Within modality suppression is strong in both modalities (auditory and visual) leading to almost 70% reduction in the SC response. This model result is consistent with the study by Kadunce et al., reporting a magnitude of within-modality depression greater than 50% for the majority of visual and auditory responsive neurons. Moreover, in the same study, no significant differences in the magnitude of response suppression were observed between within- and cross-modality suppression, as predicted by the model. Finally, the suppressive regions are quite large (25\u201330\u00b0) in the model, in accordance with physiological data (Kadunce et al. ).In the model, cross-modality suppression is a consequence of long-range lateral inhibition within the SC area. By contrast, within-modality suppression may depend on the concurrent action of two mechanisms: lateral inhibition within the unimodal (visual and auditory) areas and lateral inhibition within the SC area. In order to identify the specific role of these two mechanisms, the previous simulations have been repeated by assuming the absence of lateral interactions within the multimodal area (right columns). In these conditions, both cross-modality suppression and within-modality suppression completely disappear. This result suggests that, with the basal values of parameters (see Table\u00a0), lateral unimodal synapses play a negligible role in generating within-modality suppression, and lateral inhibition among SC neurons is fully responsible for both within-modality and cross-modality suppression. This assumption is confirmed by the activity in unimodal areas shown in the left panels of Fig.\u00a0, showing that lateral inhibition in unimodal areas, with basal parameter values, is negligible.Figure  displays the same simulations as in Fig.\u00a0 assuming stronger lateral inhibition within unimodal areas (see the legend to the figure for the modified values of parameters). In this case, when all mechanisms are included, within-modality suppression results slightly greater than cross-modality suppression (middle column). If lateral interactions in SC area are set to zero (right column), cross-modality suppression vanishes, while within modality suppression still survives. In fact, as shown in the left panels, a significant within-modality suppression is now evident in the unimodal areas. Hence, a different balance between lateral inhibition in the unimodal areas and in the multimodal area may explain the existence of within-modality suppression without cross-modality suppression as documented in the literature (Kadunce et al. ).Results in Figs.\u00a0 and  were obtained by performing the same parameter changes in all locations of the network, i.e., all neurons in the network behave in the same manner. However, in a real set up, neurons with different behavior can be observed within the same network. In order to show this possibility, we repeated a few simulations concerning within- and cross-modality suppression, starting from the basal network (that is the network with the same parameters as in Table ) but we modified the local value of synapses only at some specific positions. The results are illustrated in Fig.\u00a0, where three exemplary positions are considered: (a) position 29.25\u00b0, 29.25\u00b0: we assume that all neurons in this position (multimodal and unimodal) receive the basal value of synapses. The SC neuron exhibits both within-modality and cross-modality suppression (top panel). (b) Position 58.5\u00b0, 45\u00b0: we assume that the multimodal neuron in this position does not receive any lateral connection within the SC, but the unimodal neurons at the same position receives basal values of synapses. The SC neuron exhibits neither within nor cross-modality suppression (middle panel). (c) Position 29.25\u00b0, 58.5\u00b0: the multimodal neuron does not receive any lateral connection within the SC, while the neuron in the acoustic unimodal area at the same position receives lateral inhibitory synapses stronger than basal (see legend for details). In this case, the SC neuron exhibits within-modality suppression (using acoustic stimulation) without cross-modality suppression (bottom panel). These results demonstrate that the present model can easily be extended to build networks in which all types of behavior co-exist.In order to provide a deeper description of the different factors affecting the settling time, in Fig.\u00a0(a) we also show the output of the SC neuron and the input to the SC neuron, distinguishing between the visual input, the acoustic input, the lateral input (i.e., that due to lateral synapses in the SC) and the net input (i.e., the sum of the previous factors). Results show that the settling time is particularly high when the neuron works close to the central region and its net input (due to the sum of the input coming from the unimodal areas and the lateral inputs from other neurons in the SC) is close to zero, or just a little positive. When excitation becomes much higher than inhibition, the settling time decreases rapidly. Substantially, settling time dramatically falls when the overall lateral excitation becomes comparable to the lateral inhibition, and lateral excitation and inhibition balance.Many studies in the last two decades described the physiological properties of neurons in the superior colliculus which integrate stimuli from different sensory modalities, in order to produce efferent motor commands, namely head and eyes orientation (Kadunce et al. ,; King and Palmer ; Meredith and Stein , ; Perrault et al. , ; Populin and Yin ; Stanford et al. ; Wallace et al. ). Results from these studies contributed to individuate some general principles ruling the integrative properties of these neurons. First, two stimuli are strongly integrated when they occur in close spatial and temporal register. On the other hand, stimuli which do not overlap in space and time may exert a reciprocal inhibitory influence (cross-modal and within-modal suppression). Finally, multisensory integration is much stronger for stimuli individually less efficient in inducing a unisensory response (principle of inverse effectiveness).All these experimental data provide a clear and coherent scenario on the properties of multimodal neurons. This scenario is further supported by behavioural experiments both on animals (Stein et al. ,) and humans (Frassinetti et al. ), showing facilitation or suppression of attentive/orientation responses in the presence of multimodal stimuli compared with unimodal stimuli (Amlot et al. ; Bermant and Welch ; Frens et al. ; Hughes et al. ; Perrott et al. ).These properties of multisensory integration depend not only on neuronal individual characteristics, but also on the organization of the circuitry that processes unimodal stimuli and conveys these stimuli toward multi-sensory neurons. A deeper insight into these mechanisms and into the possible topology of the neural network involved can be provided by mathematical models and computer simulation techniques. Mathematical models allow the formulation of hypotheses in rigorous quantitative terms, the validation/rejection of these hypotheses on the basis of available experimental data and the synthesis of multiple knowledge into a coherent structure. In particular, in the case of multisensory SC neurons, there are presently enough data to attempt an accurate analysis and validation with mathematical models, and the synthesis of these data into a comprehensive, although simplified, theoretical scenario.In recent years, Patton et al. () and Anastasio et al. () developed mathematical models, based on a Bayesian approach, to study the properties of neurons in the deep superior colliculus. They postulated that these neurons compute the posterior probability that a target is present in their receptive field, and showed that this hypothesis can explain cross-modal enhancement. In a subsequent work (Patton and Anastasio ), the same authors proposed some neural implementations, based on modified perceptron models, and showed that these can explain cross-modal enhancement and within-modality suppression. Although the latter models share some aspects with our (especially in the use of sigmoidal non-linearities) there are also fundamental differences. First, the authors do not explicitly consider the spatial arrangement of the input stimuli, not their intensity, but modify the covariance of the input channels (assuming that inputs of the same modality have greater spontaneous covariance than inputs of different modality). This assumption may be true if the two unimodal stimuli come from neurons with overlapping receptive fields. Furthermore, the authors use a multiplicative interaction in their models. Our model adds several new aspects: it considers the spatial position and the intensity of the input stimuli explicitly, and simulates the effect of competitive interactions involved in target detection. It analyzes dynamical ranges to stimuli of increasing amplitude. Finally, it explains enhancement and suppression without assuming multiplicative interaction at the synaptic level, but considering a Mexican-hat disposition of synapses between adjacent neurons.In a further version of their model, Anastasio and Patton () separately considered the ascending and descending inputs to SC neurons, and trained the connection weights from these inputs with different rules, to have both unimodal and multimodal neurons in the same theoretical model. This differentiation is not considered in our work, but may be the subject of future extensions (see also discussion below).The present work was designed to elucidate possible neural mechanisms involved in multisensory integration in SC by using a mathematical model. To this aim, we developed a model of a simple neural circuit which encompasses several mechanisms, still maintaining a moderate level of complexity. Actually, the model aspires to represent a good compromise between completeness, on one hand, and conceptual (and computational) simplicity on the other.The basic idea of this model is that multimodal neurons in the superior colliculus receive their inputs from two upstream unimodal areas, one devoted to a topological organisation of visual stimuli and another devoted to a topological organisation of auditory stimuli. For the sake of simplicity, in this model somatosensory stimuli are neglected, i.e., we consider only the problem of audio-visual integration. Moreover, the exact location of these areas is not established in our model, i.e., we did not look for a definite anatomical counterpart. Experimental data suggest that multisensory neurons are created by the convergence of modality-specific afferents coming from different sources (Edwards et al. ; Huerta and Harting ; Wallace et al. ). Moreover, results of recent experiments (Jiang et al. ; Jiang and Stein ) indicate that SC neurons respond to these inputs in different ways: the nature of multisensory integration is altered depending on the considered input sources and cortical deactivation. Limitations of our model in fitting these experiments, and lines for future improvements, will be discussed at the end.Several mechanisms have been included in this simple basal circuit, each with a specific significance and a possible role in affecting final responses: (1) non-linearities in the activation function of single neurons (i.e., a lower threshold and upper saturation, expressed with a sigmoidal relationship). As will be commented below, these non-linearities are essential to understand some important properties of multisensory integration, such as the inverse effectiveness, and the possibility of superadditive, additive or subadditive integration. (2) Lateral synapses (excitatory or inhibitory) among neurons in the same unimodal area. They have been modelled with a classical \u201cMexican hat\u201d disposition, i.e., a close facilitatory area surrounded by an inhibitory annulus. These synapses play a fundamental role in producing the receptive field of multimodal neurons. Moreover, they contribute to the within-modality suppression documented in many experiments in the absence of cross-modal suppression (Kadunce et al. ,). (3) Feedforward connections from unimodal to multimodal neurons. The strength of these synapses affects the sensitivity of multimodal neurons and their unisensory dynamical range (i.e., the maximum response to a single stimulus of a given modality). (4) Lateral synapses among multisensory neurons. These synapses are necessary to obtain a significant cross-modality suppression between spatially separated auditory and visual stimuli, as documented in recent experiments (Kadunce et al. ). Moreover, they significantly affect the settling time of the response. (5) Excitatory backward connections from multimodal neurons to unimodal neurons at the same spatial position. Inclusion of these connections considers the possibility that the response by a multimodal neuron reinforces the response at an earlier unimodal area (for instance, that a strong visual stimulus may help perception of a weak auditory stimulus in the same position, and vice versa). In the present simulations the strength of these backward connections has been maintained quite low, hence they do not play a major role in simulation results. However, it may be interesting to investigate the possible effect of a reinforcement of these synapses in further studies.In summary, although some important properties in the model (for instances, cross-modal enhancement and inverse effectiveness) derive from sigmoidal non-linearities, lateral synapses in the unimodal and multi-modal areas also play a significant role, explaining the suppression (either within-modality or cross-modality) between two distal stimuli and affecting the settling time of the response. All elements included in the model are necessary to account for the variability of in vivo SC cell behaviour.Inverse effectiveness\u2014As it is evident in Figs.\u00a0 and , the capacity of multisensory neurons to integrate cross-modal stimuli strongly depends on the intensity of unisensory inputs. As in Perrault et al. (), in the present work the facilitatory interaction has been quantified using two alternative metrics, namely interactive index and contrast. The first relates the multisensory response to the stronger unisensory response. The second relates the multisensory response to the predicted sum of the two unisensory responses. Both metrics are affected by the intensity of the unisensory inputs, with cross-modal response exhibiting a significant decrease if stimulus intensity is progressively raised. This behaviour, which is know as \u201cinverse effectiveness\u201d, is a consequence of the non-linear property of neurons, and depends on the position on the sigmoidal relationship after application of the more effective input. To explain this mechanism, let us consider the case in which, after application of the more effective unisensory stimulus, the SC neuron works at the lower portion of its sigmoidal relationship, close to the threshold. Here, application of a second stimulus may move the working point into the linear portion of the curve, thus causing a disproportionate increase in the response compared with that evoked by the first input (superadditivity, enhancement greater than 100%). By contrast, if the neuron works in the central (quasi-linear) region, the effect of a second stimulus is simply additive. Finally, if the upper saturation region is approached one can have sub-additivity, since a second stimulus can induce only a minor increase in neuron activity. The last case is not simulated in this work since, with the present value of feedforward synapses, a single stimulus cannot move the working point close to the upper saturation region. Sub-additivity, however, can be mimicked by increasing the feedforward synapses.Dynamic range\u2014The multisensory dynamic range of multimodal neurons is greater than the unisensory dynamical range (Perrault et al. ). This means that the maximal response evoked by a combination of auditory and visual stimuli in close spatial and temporal register is greater than the maximal response evoked by a single stimulus of either modality (see Fig.\u00a0 in Perrault et al. ). Such a property is explained in our model by the presence of two sigmoidal relationships, disposed in a series arrangement. Let us consider a single stimulus and progressively increase its intensity: in our model, the maximal response in the SC (see Fig.\u00a0) is determined by the upper saturation of neurons in the upstream unimodal area, and by the strength of the feedforward synapses linking this unimodal neuron to the downstream (multimodal) neuron [synapses k a  or k v  in Eq. ()]. This input does not lead multimodal neurons to saturation. Consequently, if we apply a combination of a visual and an auditory stimulus, and progressively increase their intensity (multisensory dynamic range), the downstream multimodal neuron can be driven closer to its upper saturation and exhibits a greater response.Cross-modality vs. within modality integration\u2014According to the literature (Stein and Meredith ) in our model a combination of two cross-modal stimuli within the RF results in significant enhancement of the SC response, but the same effect is not visible when the two stimuli are presented as a within-modality pair. A second within-modality stimulus applied within the RF causes just a mild enhancement (Fig.\u00a0).Spatial relationship between two (within-modal or cross-modal) stimuli\u2014In agreement with experimental data (Kadunce et al. ,), our model shows that, as the spatial distance between two stimuli increases, multisensory integration in SC layer shifts from enhancement to suppression both using within-modality and cross-modality stimuli. However with our choice of basal parameter values (Table\u00a0), synapses in unimodal areas do not play a relevant role and do not affect suppression properties of superior colliculus: in these conditions both within-modality and cross-modality suppression depend mainly on the presence of lateral inhibition in the multimodal area (see Fig.\u00a0). Hence, within-modality and cross-modality suppression cannot be decoupled. Previous results in the literature (Kadunce et al. ) show the existence of different types of SC multimodal neurons: some exhibit both cross-modality suppression and within-modality suppression (as in the exemplum in Fig.\u00a0); others exhibit within modality suppression without cross-modality suppression (as in Fig.\u00a0 right panels). The model suggests that these differences can be ascribed to a different balance between lateral inhibition in the unimodal and multimodal areas. In fact, increasing lateral inhibition in the unimodal area with poor lateral inhibition in the multimodal area may explain within-modality suppression without cross-modality suppression.Temporal dynamics\u2014As illustrated in Figs.\u00a0 and , the temporal response to a combination of two stimuli in different sensory modalities is much faster than the temporal response to a single stimulus. Looking at Fig.\u00a0, we can say that the settling time evoked by two large stimuli is less than half the settling time evoked by a single stimulus. Furthermore, sensitivity analysis (Fig.\u00a0) demonstrates that the observed settling time is affected not only by the inputs and time constants, but also by the lateral feedback in the multimodal area. Hence its value is an emergent property of the network which depends on the time required for feedback mechanisms to reach a steady state level. This time is significantly decreased by two large cross-modal stimuli compared with a unimodal stimulus. In particular, as shown in Fig.\u00a0, the settling time falls dramatically when lateral inhibition is overcome by lateral excitation. However, it is important to stress that the settling time in our model does not replicate the temporal pattern of neuron response to real (auditory and visual) input stimuli, as measured in vivo, but only represents the network dynamics. Actually, the temporal pattern of neuron response during in vivo experiments may depend on additional factors, such as the time dynamics of the peripheral receptors (such as the retina and the cochlea), as well the latency of the neural pathways from the receptor to the SC. Analysis of these factors is well beyond the aim of the present work. However, we think that the 20\u201325\u00a0ms difference in settling times, evident in Figs.\u00a0 and a may be of interest, and may in part explain the difference in overt behaviour between multimodal and unimodal stimulation observed during behavioural experiments (Frens et al. ; Perrott et al. ).An interesting aspect of our simulations (see Fig.\u00a0) is that the behaviour of a neuron in response to a second stimulus can shift from entirely superadditive to superadditive\u2013additive depending on the intensity of the first stimulus applied. This signifies that, contrarily to what frequently claimed in the literature (Perrault et al. ), the behaviour of a neuron in terms of its multisensory contrast is not an intrinsic property, but depends on its particular operative conditions.In summary, the results in Figs.\u00a0, , ,  and  and in Fig.\u00a0 have been obtained only by changing input stimuli, (i.e., modality, intensity and spatial position), without altering any parameter of the model. Therefore, they mimic results which can be obtained on a single neuron, not on a group of different neurons. The purpose was to show that the model, designed on the basis of few principles, with a single set of parameters (hence, representing a single case) can explain and summarize several data, characterized by different properties of the input.An example of possible individual variability among neurons and classes of neurons has been investigated in Figs.\u00a0 and , by changing the value of lateral and feedforward synapses in multimodal and unimodal areas. These simulations emphasize the possibility to have within-modality suppression without cross-modality suppression, as observed in some SC neurons (Kadunce et al. ).Finally, we wish to comment on possible model limitations, which may the subject of future improvements. First, the structure of the model is drastically simplified compared with the reality. In our model, we assumed that the receptive fields of acoustic and visual neurons, converging to the same multimodal neuron, have a circular shape and are exactly centered at the same position in space, with the visual neuron having a smaller RF compared with the acoustic neuron (hence, the RF of visual neurons is entirely contained inside the RF of acoustic neurons). By contrast, as clearly documented in the literature, the two RFs do not exhibit a 100% overlap (although, as reported in Kadunce et al. , in many multisensory neurons more than 70% of the visual RF is contained within the area of the auditory RF). Moreover, in most simulations we assumed that synapses within a single area are perfectly symmetrical and that there is no difference in the properties of neurons within the same area, except for the position of their RF. Of course, in real networks, neurons and synapses exhibit a random variability, and two proximal neurons in the same area can exhibit different properties and disparate responses. Of course, these simplifications have been adopted to have a more straightforward model, and to make the analysis of results easier.A first step to overcome this limitation was presented in Fig.\u00a0, where we locally modified the values of synapses and displayed the activity of three neurons in the same network with different properties. These results show that the network can be easily changed, to account for the simultaneous existence of neurons with different properties, as experimentally observed.Another limitation of our model is the absence of direct synapses between neurons in the two unimodal areas. Indeed, a unimodal auditive neuron (area A) and a unimodal visual neuron (area V) communicate only indirectly, through the backward synapses coming from the multimodal SC neuron (area SC). This choice has been adopted according to a principle of parsimony, i.e., to limit the number of mechanisms in the model. It is possible, however, that neurons in unimodal areas communicate also directly via lateral synapses. The possible effect of these links on model response may be the subject of future extensions.Jiang et al. () and Jiang and Stein () demonstrated that the capacity of SC neurons to integrate cross-modal sensory stimuli is strongly dependent on influences from two cortical areas [the anterior ectosylvian sulcus (AES) and the rostral lateral suprasylvian sulcus (rLS)]. However, the response to unimodal stimuli remains largely intact even if these cortices are temporarily deactivated (Jiang et al. ; Jiang and Stein ). This aspect raises additional problems for a mathematical model, which might be solved including more unimodal areas and/or a more complex topology for the network. For instance, the two unimodal areas in the present model might represent the visual and auditory inputs from the AES (and perhaps rLS), allowing multisensory integration (either enhancement or suppression). In particular, the AES contains distinct sensory representations (a somatosensory, a visual and an auditory region) and also has many multisensory neurons. Yet, only unimodal neurons in the AES send inputs to the SC (Stein ; Wallace et al. ). Additional inputs from other cortical and subcortical structures might be responsible for the responses observed after deactivation of the AES. Hence, a more complete model should include at least four distinct unimodal inputs (two visual and two auditory) to SC neurons, reflecting descending inputs from cortico-collicular regions (responsible for multisensory integration) and ascending inputs coming from a variety of other sources (which do not produce multisensory integration). The present model considers only the first (descending) inputs, hence cannot simulate SC behaviour after AES and rLS deactivation. Of course, development of the more complex model might be the subject of future refinements and extensions.Finally, it is important to stress that most of the data mentioned in this work have been obtained in anesthetized animals, hence the model presented here aspires to simulate these conditions. In recent years, controversial results have been published on the possible effect of anesthetics on bimodal enhancement in the superior colliculus. While some authors report multisensory integration and cross-modal enhancement in alert untrained cats (Wallace et al. ) others observed depressed enhancement in behaving cats compared with anesthetized animals (Frens and Van Opstal ; Peck ; Populin ; Populin and Yin ). There are two main aspects which can in part explain these differences. First, some authors (Populin and Yin ) used a different metrics to quantify multimodal enhancement in alert animals. By this measure, the authors consider multimodal enhancement only in case of superadditivity. In our model, superadditivity may be converted to simple additivity, or even to subadditivity by changing some model parameters. Second, the SC receives a vast intrinsic inhibitory network (Mize et al. ), and receives both ascending subcortical inputs and descending inputs from cortical regions (such as AES and rLS). It is thus possible that anesthesia modifies some parameters in the model, alters the balance among the inputs, and/or the balance between inhibition and excitation. These effects may be studied in future versions of the model, including additional inputs and using a sensitivity analysis on parameter changes and/or on input changes.", "s10827-008-0106-6": "Spike trains are unreliable. For example, in the primary sensory areas, spike patterns and precise spike times will vary between responses to the same stimulus. Nonetheless, information about sensory inputs is communicated in the form of spike trains. A challenge in understanding spike trains is to assess the significance of individual spikes in encoding information. One approach is to define a spike train metric, allowing a distance to be calculated between pairs of spike trains. In a good metric, this distance will depend on the information the spike trains encode. This method has been used previously to calculate the timescale over which the precision of spike times is significant. Here, a new metric is constructed based on a simple model of synaptic conductances which includes binding site depletion. Including binding site depletion in the metric means that a given individual spike has a smaller effect on the distance if it occurs soon after other spikes. The metric proves effective at classifying neuronal responses by stimuli in the sample data set of electro-physiological recordings from the primary auditory area of the zebra finch fore-brain. This shows that this is an effective metric for these spike trains suggesting that in these spike trains the significance of a spike is modulated by its proximity to previous spikes. This modulation is a putative information-coding property of spike trains.It is not known how the information that is propagating in the sensory pathways is encoded in spike trains. One approach to this question is to use a spike train metric (Victor and Purpura ) to cluster responses to repeated presentations of a set of stimuli. If a metric measures a short distance between responses to the same stimulus and a longer distance between responses to different stimuli then the distance measured between two spike trains must be related to the information they encode. A metric can be evaluated by performing distance-based clustering and then calculating how accurately the metric clusters together responses to the same stimulus. Using this comparison to optimize a parameter in a family of metrics then gives a measurement of how information is coded in the spike trains.This approach was used by Victor and Purpura (). This paper reports the analysis of spike trains collected from areas V1, V2 and V3 of awake monkeys during the transient, 256 ms long, presentation of visual stimuli which differ in texture and contrast level. The spiking responses were clustered using, what is now called, the Victor-Purpura metric: an edit-distance metric which has a free parameter q. An indicative time scale for the metric is given by 2/q; roughly speaking, spikes from two different spike trains can be thought of as being related by jitter if their timing difference is less than 2/q. It is a measure of the timescale for which spike timing is significant. In fact, the spike trains are clustered most accurately for 2/q in the range c. 10\u2009\u2212\u200930 ms for stimuli which differed in contrast levels and c. 100 ms for stimuli which differed in texture. This shows that information is encoded in the spike timing, even though the individual visual stimuli are not time-dependent.This same approach was applied to the neuronal responses of neurons in field L of zebra finches (Narayan et\u00a0al. ; Wang et\u00a0al. ). In the ascending auditory pathway, area field L is afferent to the song system and is considered the oscine analogue of the primary auditory cortex (Zaretsky and Konishi ). However, a different metric was used in this study. This study was similar to the one discussed above in that the metric had a timescale parameter and, again, optimizing this parameter showed that information is encoded in spike times.The metric used in (Narayan et\u00a0al. ; Wang et\u00a0al. ) first converts a spike train into a function using a filter (Hunter et\u00a0al. ; van Rossum ). A metric on the space of functions then induces a distance measure on the space of spike trains. Provided the map between the space of spike trains and the function space is injective, as it is here, the induced distance measure will also be a metric.In Narayan et\u00a0al. () it was found that for zebra finch field L data, setting the time-scale \u03c4 to 12.8 ms gives the most accurate clustering of field L responses by song stimulus. Thus, 12.8 ms is an indicative time-scale for the encoding of information in these spike times.In this paper, a number of different metrics will be compared. For convenience, the metric discussed above, based on a filter, will be refered to as the f-metric. The term van Rossum metric will be used to mean any metric induced on the space of spike trains by mapping individual spike trains to functions and then using a standard metric on the function space. Thus, the f-metric is a van Rossum metric. However, other van Rossum metrics will be considered, for which the map from spike train to functions is different.Using stimulus-based clustering, it was found in both the examples discussed that information is carried by spike times. Furthermore, optimizing the metric timescale parameter gave an indicative timescale for the spike timing precision. Here, this investigation is continued. The f-metric is extended by changing the way the spike train is mapped to functions. The new mapping has an additional free parameter, one modelled on the short-term depletion of available binding sites in the dendritic spine. Varying this parameter improves the stimulus-based clustering of the zebra finch field L responses previously considered in Narayan et\u00a0al. (). This indicates that this new van Rossum metric measures another property of information coding in the spike trains. Roughly speaking, the van Rossum timescale parameter measures the precision at which spike timing is significant. The new parameter measures how the significance of a spike varies depending on its proximity to previous spikes.At the synapses, an arriving spike causes the exocytosis of vesicles containing neurotransmitters. The neurotransmitters diffuse across the synaptic cleft and binds with receptors on the dendritic spine. This triggers the opening of ion channels, changing the conductance of the dendritic membrane and leading to a post-synaptic potential. There is a rapid breakdown and reuptake of unbound neurotransmitters. The bound neurotransmitters detach from their binding sites, terminating the post-synaptic potential.In Fig.\u00a0(a), the optimal parameter values for each site are used for each of the two metrics.  for the b-metric shows an average improvement of 14.5% over  for the f-metric. For three sites, the improvement is over 30%: the biggest improvement is 47%. Most of the sites that showed little improvement have values of  very close to one. This behaviour is not significantly different for the six sites which are classified as single neuron recordings: their average improvement is 12.0%. The smaller value is likely to reflect the greater reliability of the single neuron sites. The average  value is 0.868 using the f-metric whereas the average for all the sites is 0.802.The average parameter values are \u03c4\u2009=\u200912.9 ms and \u03bc\u2009=\u20090.72 for the b-metric and \u03c4\u2009=\u200912.8 ms for the f-metric. In Fig.\u00a0(b), the performance is compared using these average values for all sites. Obviously, for most sites, the  value is reduced for both metrics. The average improvement is also reduced slightly to 12.9%; however, the fact that the effect is not significantly reduced seems to indicate that the improvement is not the result of over-fitting. Only one site has a lower  value using the b-metric rather than the f-metric.The biological situation motivating the d-metric is very different from the one motivating the b-metric discussed above. One models vesicle depletion, the other, the depletion of neurotransmitter binding sites. The time scales for these two biological phenomena are very different. Nonetheless, mathematically, the d-metric is a generalization of the b-metric. If \u03c4  d \u2009=\u2009\u03c4, then setting p\u2009=\u20091\u2009\u2212\u2009\u03bcf and identifying \u03d5 with 1\u2009\u2212\u2009\u03bc reduces the synaptic depression metric to the b-metric. For this reason, optimizing the three-parameter d-metric for each site will produce results which are superior to the b-metric. However, this improvement is modest: 2.9%. Using average values, performance is actually degraded: the relative change is \u2212\u20093.2%. Synaptic facilitation has also been studied and does not appear to improve clustering performance for these data.It seems that the performance gain exhibited by the b-metric is significant when compared to other biologically-motivated van Rossum metrics. In fact, the d-metric seems to only show superior performance because it over-fits the data. Of course, real synapses do not choose between binding-site depletion and synaptic depression; they have very complicated dynamics involving many different time scales. Nonetheless, it is interesting that modelling binding site depletion is particularly useful for clustering these data. These data are recorded from an area which is early in the auditory pathway where, perhaps, shorter timescales are particularly important. It is probable that the d-metric will demonstrate a larger improvement over the b-metric when applied to other data.It is also interesting that this parameter is suggested by a simple model of synaptic conductance. The information processing step that is novel to the metric, the modulation of the significance of spike times, could occur naturally in real neuronal systems in the precisely the mechanism that originally inspired the metric, through the depletion of binding sites in synapses. It would not be surprising to find, as indicated here, that aspects of the neural code are apparent in the post-synaptic voltage response, it is striking, however, that the depletion of binding sites appears to be a significant detail. Of course, the real dynamics of synapses are much more complicated and it would be interesting to understand binding site depletion, synaptic potentiation and depression and spike rate adaptation (Fairhall et\u00a0al. ) as components in an information process.", "s10827-008-0100-z": "We investigated by a computational model of the basal ganglia the different network effects of deep brain stimulation (DBS) for Parkinson\u2019s disease (PD) in different target sites in the subthalamic nucleus (STN), the globus pallidus pars interna (GPi), and the globus pallidus pars externa (GPe). A cellular-based model of the basal ganglia system (BGS), based on the model proposed by Rubin and Terman (J Comput Neurosci 16:211\u2013235, ), was developed. The original Rubin and Terman model was able to reproduce both the physiological and pathological activities of STN, GPi, GPe and thalamo-cortical (TC) relay cells. In the present study, we introduced a representation of the direct pathway of the BGS, allowing a more complete framework to simulate DBS and to interpret its network effects in the BGS. Our results suggest that DBS in the STN could functionally restore the TC relay activity, while DBS in the GPe and in the GPi could functionally over-activate and inhibit it, respectively. Our results are consistent with the experimental and the clinical evidences on the network effects of DBS.The basal ganglia system (BGS) is a group of four cerebral nuclei that receive information from the whole cortex and project mainly to the frontal cortex through the thalamus. The nuclei are the striatum, the substantia nigra (further divided in pars compacta, SNc, and pars reticulata, SNr), the subthalamic nucleus (STN), and the globus pallidus (further divided in pars interna, GPi, and pars externa, GPe). The striatum and the STN are considered the input ports of the BGS, while the GPi and the SNr are the output port of the BGS projecting to the thalamus and brainstem targets (DeLong and Wichmann ).Parkinson\u2019s disease (PD) is a degenerative disease with cardinal motor symptoms that correlate with the loss of dopaminergic cells in the SNc. PD is characterized by akinesia, bradykinesia, rigidity, resting tremor at frequencies between 4 and 7\u00a0Hz, and other motor and postural impairments (Bergman and Deuschl ).Ablative surgery of the thalamus or the GPi, pharmacological therapy based on levodopa (l-dopa), and deep brain stimulation (DBS) have been proved to be effective therapies for PD (Perlmutter and Mink ). DBS is a chronic electrical stimulation through electrodes implanted in the BGS or in the thalamus. DBS electrodes provide an high frequency (usually >100 pulses/s), continuous train of electrical pulses (Perlmutter and Mink ).The GPi and the STN have been identified as sites in the BGS where DBS improves many axial and cardinal symptoms in patients affected by advanced PD (Burchiel et al. ; Yokoyama et al. ; Ostergaard and Sunde ). Quantitative studies showed that STN- and GPi-DBS improve advanced PD patients\u2019 stability in quiet stance (Rocchi et al. ; Maurer et al. ; Rocchi et al. ) and increase the step length and the speed of steady-state gait (Faist et al. ; Defebvre et al. ; Rizzone et al. ; Ferrarin et al. ). STN-DBS is assumed to present a higher risk of dyskinetic side-effects than does GPi-DBS (Krack et al. ); thus, patients suffering predominantly from l-dopa-induced dyskinesia are commonly directed to GPi-DBS (Perlmutter and Mink ).A recent analysis of patient outcomes (Weaver et al. ; Follett et al. ) highlighted how PD patients who had undergone STN-DBS and those who had undergone GPi-DBS experienced comparable improvements both in motor function and in performance of activities of daily living following surgery. In a multicenter study with a 4-year follow-up, PD patients who had undergone STN-DBS and those who had undergone GPi-DBS exhibited significant improvement in many cardinal features of PD, such as tremor, rigidity, bradykinesia, and tremor (Rodriguez-Oroz et al. ).The GPe has been recently proposed as a DBS target for PD. Vitek et al. () have shown that patients undergoing GPe-DBS improved more in terms of bradykinesia, akinesia, and rigidity than did patients undergoing GPi-DBS. GPe-DBS induced more dyskinetic events than did GPi-DBS.Despite the paucity of data directly comparing different targets for DBS, many clinicians already consider the STN to be the preferred target site for PD, even though GPi (GPe) may be similarly effective (Vitek et al. ; Anderson et al. ). While comparative, blinded experimental studies are underway to compare the different DBS targets, we meant in this paper to give a contribution to elucidate the issue by means of a computational modelling approach of the BGS.So far, most of the theoretical and computational models have been proposed to interpret both the role of the BGS in the CNS and the pathological behaviour of the BGS (mostly in PD) (see Humphries et al. ; Leblois et al. ; and DeLong and Wichmann  for the most recent and complete modelling frameworks and for a brief review of the previously presented models).The BGS plays a key role in the executive functions of the central nervous system (CNS). Briefly, the most accepted theory (action selection theory) about BGS states that it behaves as a selection system between competing motor and/or cognitive plans (Mink ; Beiser et al. ; Berns and Sejnowski ; Redgrave et al. ; Brown et al. ; Nambu ; DeLong and Wichmann ). In most of the models of the action selection mechanism in the BGS, the selection is performed in the striatum and the subsequent activation of the selected plan and the inhibition of the competing plans are performed through the so called \u201cdirect\u201d (striatum \u2192 GPi \u2192 thalamus) and \u201cindirect\u201d (striatum \u2192 GPe \u2192 STN \u2192 GPi \u2192 thalamus) pathways.In subjects affected by PD the neural activity in the nuclei of the BGS is severely altered, regarding both the mean firing rates and the discharge patterns of the neural populations (Raz et al. ; Brown et al. ; Bergman and Deuschl ); Several theoretical and computational models for PD have been proposed. Most of these models offer an explanation for PD in terms of: (a) alteration of the mean activities of the direct and of the indirect pathway (Delong ; Albin et al. ); (b) alteration of the discharge patterns of the BGS nuclei (Terman et al. ; Rubin and Terman ), (c) impairment of the striatal selectivity (Mink ; Humphries et al. ); and (d) a mix of the previous frameworks (Bar-Gad et al. ; Humphries et al. ; DeLong and Wichmann ).The therapeutic action of DBS acts through mechanisms not yet completely clear (see Montgomery Jr and Baker ; Benabid et al. ; Dostrovsky and Lozano ; Vitek ; McIntyre et al. ; and Perlmutter and Mink  for complete reviews of all the hypothesized mechanisms). Briefly, the problem involves three issues: (1) how does DBS influence the neuronal activity in the target site (local DBS effects)? (2) how do the local DBS effects influence the neuronal activity in the other structures in the CNS (network DBS effects)? and (3) why do the network DBS effects provide therapeutic effects?BGS models were never used, to authors\u2019 knowledge, to compare the network effects of DBS in different targets in the BGS. To achieve this aim we needed a convincing BGS model having: (a) a theory about the physiological and the pathological (PD) functioning of the BGS and (b) a theory about the local DBS effects.The model presented in this paper follows the seminal model by Rubin and Terman () (RTM). The RTM is a model of the BGS that includes the STN, the GPi, the GPe, and the thalamus. Every nucleus is represented by a population of Hodgkin\u2013Huxley cell models.Rubin and Terman, through the RTM, provided a theory about the physio-pathological functioning of the BGS and about the network effects of the STN-DBS. In RTM, the thalamus is a simple relay station whose physiological role is to faithfully respond to the inputs arriving from the sensorimotor cortex (SMC). Thalamic cells receive also inhibitory inputs from the GPi cells. In the physiological state of the RTM the GPi activity is tonic and uncorrelated among subpopulations of the GPi cells and do not disrupt the thalamic relay activity. In the PD state of the RTM the GPi activity is phasic and correlated among subpopulations of the GPi cells and disrupt the thalamic relay activity. The PD GPi activity arise from upstream PD STN and GPe activities. The STN and GPe populations are arranged in a mutually coupled architecture. In the PD state of the RTM, a weakened intra-GPe connectivity and an augmented striatal inhibition to the GPe bring the GPe and the STN activities to be: (a) synchronous oscillatory (with GPe and STN cells showing bursts of action potentials (APs) at 4\u20137\u00a0Hz) and (b) correlated among GPe and STN subpopulations.The theory about the physio-pathological functioning of the BGS provided by Rubin and Terman originated from the experimental work by Plenz and Kitai . In this work, Plenz and Kitai demonstrated that, in an organotypic culture, the mutually coupled GPe and STN populations could generate oscillatory and synchronized activity. The theory agrees with in-vivo physiological studies, that proved an abnormal, high level of synchronization in and between the BGS nuclei in PD (Raz et al. ; Bergman and Deuschl ).Rubin and Terman assumed a very simple representation of the local DBS effects. In RTM, every DBS pulse elicits an AP in every STN cell. The limits of this assumption will be further addressed in Section\u00a0.During DBS, the pathological, phasic STN activity is replaced by an abnormal, regular activity that is synchronized with the DBS pulses train. The regularization of the STN activity brings to a regularization of the GPe and GPi activities. Rubin and Terman hypothesized that the main network effect of STN-DBS in the RTM is to stabilize the GPi inhibitory input to the thalamus and to restore the thalamic relay activity. This conclusion is consistent with the theories on the network effects of DBS proposed by McIntyre et al. () and DeLong and Wichmann ().While modelling in detail the indirect pathway of the BGS, the RTM does not provide any representation of the direct pathway. No striatal input to the GPi is considered. Consequently, no functional striatal control of the GPi activity and, subsequently, of the thalamic relay activity is modelled. In the RTM, the role of the thalamic cells in the physiological state is simply to respond to SMC inputs. On the contrary, the action selection theory of the BGS states that the thalamic relay activity is controlled by the BGS and is dynamically released only for the execution of the selected motor or cognitive plan. Therefore, we decided to improve the RTM by including a representation of the striatal input to the GPi and, consequently, of the direct pathway of the BGS. In our opinion, this more complete model of the BGS may allow a better understanding of the network effects of DBS.The aims of this study were: (a) to better understand the differential network effects of STN-, GPi-, and GPe-DBS by using an extended version of the RTM, the eRTM, and (b) to compare results with clinical and experimental evidences regarding STN-, GPi-, and GPe-DBS.The eRTM has been implemented with Matlab 7.04 and Simulink 6.1, on a PC platform (Pentium IV 2\u00a0GHz, 1\u00a0GB RAM).The eRTM models the STN, the GPe, the GPi, and the thalamus by four populations of Hodgkin\u2013Huxley, mono-compartmental cell models. Therefore, each cell is represented by a set of differential equations. The eRTM consists in a network of 16 STN, 16 GPe, 16 GPi, and two thalamocortical (TC) relay cells. The eRTM considers: (a) the inhibitory connections striatum \u2192 GPi (representing the direct pathway), striatum \u2192 GPe, GPe \u2192 GPe, GPe \u2192 STN, GPe \u2192 GPi, and GPi \u2192 thalamus and (b) the excitatory connections STN \u2192 GPe, STN \u2192 GPi, and cortex \u2192 thalamus.Each STN, GPe, and GPi population is divided in two groups of eight cells each. Each GPe cell receives inhibitory input from three cells of its own group, randomly chosen, and receives excitatory input from three STN cells, randomly chosen among the entire STN population. Every STN cell receives inhibitory input from three GPe cells chosen among a matched group in the GPe. It follows that, from a functional point of view, the GPe population (but not the STN population) is divided into two parallel channels. This connectivity design aims to reflect the real connectivity observed (GPe) or hypothesized (STN) in the nuclei of the BGS (Bar-Gad et al. ). Every GPi cell receives inhibitory input from a corresponding GPe neuron and excitatory input from a corresponding STN neuron. Every TC cell receives inhibitory input from eight GPi cells. A single realisation of the connectivity scheme described above was considered in the following analyses.The striatal inputs to the GPe and to the GPi are represented by constant currents directly injected into GPe and GPi cells.The cortical excitatory inputs to TC cells are represented by a train of positive, rectangular current pulses directly injected into TC cells. Time intervals between subsequent pulses are selected from a Poisson distribution, with an enforced minimum interval of 10\u00a0ms, and determined as 10\u2009\u2212\u2009log(ran(1))/0.03\u00a0ms, where ran(1) is a random number selected from a uniform distribution on [0, 1] (as in Rubin and Terman ).See Rubin and Terman () and Appendix for details on equations of the eRTM and parameter values.Two parameters were used to model the physiological and the PD state in the eRTM, according to Rubin and Terman (): (1) the indirect striatal current to GPe cells (I Striatum\u2192GPe\u2009=\u20090\u00a0pA/\u03bcm2 for the normal state, and I Striatum\u2192GPe\u2009=\u2009\u22124\u00a0pA/\u03bcm2 for the PD state) and (2) the intra-GPe inhibitory synaptic conductance (g GPe\u2192GPe\u2009=\u20090.3\u00a0nS/\u03bcm2 for the normal state, and g GPe\u2192GPe\u2009=\u20090\u00a0nS/\u03bcm2 for the PD state). The parameter changes turned out to be a good phenomenological model of dopamine depletion\u2014by putting the model in the \u201ccorrect\u201d dynamic state for PD (see Section\u00a0 and Fig.\u00a0)\u2014but for reasons unknown, as we currently have no data to support the changes. The increase in striatal current to the GPe for the PD state follows the models of PD that propose an \u201cincreased response to excitatory input\u201d in the indirect pathway of the BGS due to the decreased activation of D2 receptors in the striatum (DeLong and Wichmann ). The decrease in the intra-GPe inhibitory synaptic conductance in the PD state was motivated in Rubin and Terman () by some experimental results with rats (Stanford and Cooper ; Ogura and Kita ). These results showed in fact that enkephalin and dynorphin act pre-synaptically on GABAergic terminals in GP (the functional homologous of the GPe in rodents) to reduce GABA release, while Rubin and Terman implicitly assumed that an increased level of inhibition from the striatum to GP after PD-related dopaminergic denervation could be positively correlated with the release of enkephalin and dynorphin. However, it should be pointed out that no results has been so far provided to sustain the cited assumption.It is worth mentioning that we did not include the striatal current to the GPi (I Striatum\u2192GPi) in the set of parameters used to switch the eRTM between the normal and the PD states. This neglect could seem controversial, given the models of PD that propose a \u201cdecreased response to excitatory input\u201d in the direct pathway of the BGS due to the decreased activation of D1 receptors in the striatum. However, since we aimed to include the action selection theory in the eRTM and the current I Striatum\u2192GPi was the key parameter in doing that (as shown in following sections), we preferred to define the \u201cPD state\u201d of the eRTM as a state showing just the pathological activities arising in the indirect pathway. Pathological features arising in the direct pathway and involving I Striatum\u2192GPi were introduced and discussed later, in Sections\u00a0 and .In the eRTM, the DBS was modelled as a train of rectangular, positive-current pulses directly injected into the cells belonging to the target site. This DBS modelling is rather simplistic, since in the reality the response of a neuron to extracellular stimulation is very different from its response to intracellular stimulation (see Section\u00a0). The pulse amplitude in DBS was set to 400\u00a0pA/\u03bcm2, while pulse length was set to 0.15, 0.3, and 1.2\u00a0ms for STN, GPi and GPe stimulation, respectively. These values ensured a 1:1 ratio between DBS pulses and APs in the target structures.DBS was always tested on the PD state of the eRTM.The mean firing rate (MFR) of the GPi, calculated as the average firing rate of the 16 GPi cells (APs revealed by a thresholding method; threshold set to \u221220\u00a0mV).The percentage effectiveness score (ES%), defined as the percentage of correct thalamic responses to SM cortical inputs (averaged over the 2 TC cells). A thalamic AP was considered correct if it occurred within 6\u00a0ms after a SMC input. Summary ES% was the average of the ES% computed in 25 runs of the simulation.We defined also an additional parameter to monitor the degree of synchrony amongst the activities of the GPi cells in the physiological and PD states, the Synchrony Index (SI). For every GPi cell i (i\u2009=\u20091,\u2026, 16), we detected the APs by a thresholding method (threshold set to \u221220\u00a0mV) and we derived an \u201cactivation function\u201d simply by counting the APs in consecutive, non-overlapping windows of 30\u00a0ms each. Then, we calculated the correlation coefficient for each pair of activation functions (number of meaningful pairs N P\u2009=\u2009n (n\u2009\u2212\u20091)\u2009/\u20092, n\u2009=\u200916) and tested for significant correlation with \u03b1\u2009=\u20090.05 (function corrcoef in Matlab; see Matlab documentation for the details on the statistical test for correlation). The SI was calculated as the ratio between the number of pairs showing significant correlation (N S) and the total number of pairs N P. It has to be pointed out that when testing for significant correlation, we did not correct for false positives (i.e. Type I errors), of which one would naively expect to find about 0.05\u2009*\u2009120\u2009=\u20096 for each simulation. However, the expected false positive rate ((false N S)\u2009/\u2009N P) is a constant since: (a) all simulations involved the same number of comparisons, from time-series with the same length (30\u00a0ms windows over 1\u00a0s), and (b) we already assumed a normal distribution for action potential counts by taking the p-values from the corrcoef function in Matlab. So, if the expected false positive rate is a constant, then the SI scores are comparable between, e.g., normal and PD simulations because both SI scores should have roughly the same error. Anyway, future applications of the SI score will take into account a correction for false positives.The role of the direct striatal input to GPi, in the physio-pathologic states and during DBS. Different values of I Striatum\u2192GPi (from \u221213 to 3\u00a0pA/\u03bcm2, see ) were tested for 1\u00a0s. Positive, depolarizing values for an inhibitory current have no physiological meaning. We included them in our analysis just for the sake of completeness and as a way to control if the resting membrane potential of GPi cells (set arbitrarily by the parameter I moreGPi, see ) was calibrated at a plausible level to further allow the inclusion of the AS theory in the model (I Striatum\u2192GPi directly adds up to I moreGPi, see ). In these simulations, DBS frequency was set to 120\u00a0Hz.The role of the DBS frequency. Different DBS frequencies (30, 60, 90, 120, 150, 180\u00a0Hz) and different sites of stimulation (GPi, GPe, and STN) were tested for 1\u00a0s. I Striatum\u2192GPi was assumed equal to 0, \u22125, and \u221211\u00a0pA/\u03bcm2 (in order to sample the range for this variable).Outcome measures were computed discarding the first 300\u00a0ms of the simulations, to allow extinction of the initial transient phases.Qualitative network effects of STN-, GPe-, and GPi-DBS in the eRTM are illustrated in the following subsections. The activity of representative STN, GPe, and GPi cells were considered. In these simulations we left unchanged from Rubin and Terman () the parameters regarding connectivity to GPi and to Thalamus: g STN\u2192GPi\u2009=\u20090.3\u00a0nS/\u03bcm2, g GPe\u2192GPi\u2009=\u20091\u00a0nS/\u03bcm2, g GPi\u2192Thalamus\u2009=\u20090.08\u00a0nS/\u03bcm2, and I Striatum\u2192GPi\u2009=\u20090\u00a0pA/\u03bcm2.For 30\u00a0Hz DBS, STN cells responded in a 1:1 ratio. GPe cells, excited by STN cells, fired at the DBS frequency. The resulting network contribution to GPi cells lead them to fire in burst mode, in triplets of APs, at a burst frequency of 30\u00a0Hz.For 120\u00a0Hz DBS, STN and GPe cells still fired in a 1:1 ratio with the DBS stimuli; note that the duration and the amplitude of STN APs diminished. The total inhibition from GPe activity overrode the total excitation from the STN activity and GPi cells were completely inhibited by the inhibitory GPe activity.For 180\u00a0Hz DBS, STN cells fired in a 1:1 ratio, but one in every two DBS stimuli occurred during the refractory period of the AP arising from the previous DBS pulse. The consequent \u2018reduced\u2019 AP was not capable of driving a GPe AP and GPe cells fired at 90\u00a0Hz. In any case, STN APs did not drive GPi APs, and the GPi cells were still inhibited as for 120\u00a0Hz DBS.The network effects of GPe-DBS are shown in Fig.\u00a0(b).For 30\u00a0Hz DBS, GPe and GPi cells regularized their activities at 90\u00a0Hz, while driving STN activity at 15\u00a0Hz.DBS at 120 and 180\u00a0Hz drove GPe cells to fire in a 1:1 ratio with DBS stimuli. STN and GPi cells were therefore completely inhibited by the tonic, high-frequency GPe activity.Upstream, pathological STN and GPe activities were not changed by GPi-DBS.DBS at 30 and 120\u00a0Hz led GPi cells to respond in a 1:1 ratio to stimuli. Note the residual phasic patterns that resulted from the upstream, pathological GPe and STN activities.DBS at 180\u00a0Hz led to a complete override of the GPi phasic activity resulting from the upstream GPe and STN activities.In the physiological state, the GPi inhibitory contribution to the TC cells acts like a \u201cwhite noise\u201d, whose level does not corrupt the TC relay activity.In the PD state, the phasic activity of GPi cells leads to a phasic inhibitory contribution to the TC cells. The amplitude of the high-activity phases of this contribution is high enough to inhibit TC relay activity and to enhance TC rebound bursting during the following low-activity phases (see Rubin and Terman , for the explanation of rebound bursting phenomena in TC cells).STN- and GPe-DBS stabilize the GPi \u2192 Thalamus contribution at a low level (a single set of outputs was provided since the GPi activity was equal to 0 for both STN- and GPe-DBS in these simulations). A functional restoration of TC relay activity follows.GPi-DBS stabilizes the GPi \u2192 Thalamus contribution at a high level. A functional inhibition of TC relay activity follows.It should be pointed out that we found that the effect of STN-DBS on GPi-MFR and TC activity level is rather sensitive to DBS frequency between 100 and 150\u00a0Hz, even if, for the sake of clarity, we did not mention this point so far. It will be further discussed in Section\u00a0.Qualitative results on the network effects of STN- and GPe-DBS indicate that the relative weights of the STN \u2192 GPi and GPe \u2192 GPi connections may play a role in modulating the downstream effects of STN- and GPe-DBS. In fact, in a recent study on the RTM model (Pascual et al. ), the researchers eliminated the GPe \u2192 GPi connection to allow the STN activity during STN-DBS to drive GPi activity. Rubin and Terman () showed that STN activity during STN-DBS drives GPi cells activity, but at a lower frequency than DBS frequency.Another critical point emerging from qualitative results presented above is that the TC relay activity in the physiological state is substantially activated with I Striatum\u2192GPi\u2009=\u20090\u00a0pA/\u03bcm2. This result is not consistent with the AS scenario, since an activation of the striatum (negative values for I Striatum\u2192GPi) would inhibit the GPi and further enhance the TC relay activity. For the eRTM to be consistent with the AS theory (primarily in the physiological state), the TC relay activity should be inhibited with I Striatum\u2192GPi\u2009=\u20090\u00a0pA/\u03bcm2 and progressively enhanced along with an increase of |I Striatum\u2192GPi|. It should be noted that, in this context, the GPi \u2192 Thalamus inhibitory conductance may play an important role.Having these issues in mind, we decided to perform a preliminary sensitivity analysis to the parameters g GPe\u2192GPi and g GPi\u2192Thalamus on the input\u2013output relationships between the striatal input to GPi (input) and the TC relay activity (output) (leaving unchanged the upstream STN/GPe module of the eRTM and the g STN\u2192GPi parameter). We tested the following states of the model: (a) physiological, (b) PD, (c) STN-DBS at 120\u00a0Hz, (d) GPe-DBS at 120\u00a0Hz, and (e) GPi-DBS at 120\u00a0Hz. The tested values for the two parameters were: (a) 0, 0.3, 0.5, 0.7, and 1\u00a0nS/\u03bcm2 for g GPe\u2192GPi, and (b) 0.08, 0.16, and 0.24\u00a0nS/\u03bcm2 for g GPi\u2192Thalamus (original values in the RTM: g GPe\u2192GPi\u2009=\u20091\u00a0nS/\u03bcm2; g GPi\u2192Thalamus\u2009=\u20090.08\u00a0nS/\u03bcm2). The aim of this analysis was to determine the optimal values for the two parameters allowing (a) a proper integration of the AS theory in the eRTM, and (b) a consequent better explanation of the network effects of DBS on different targets. States of the model, simulations, values for I Striatum\u2192GPi, and outcome measures were defined, run, and determined as described in Sections\u00a0 and .In the action selection theory, the role of the direct pathway is to drive, in synergy with indirect and hyperdirect pathways, an optimal and selective activation of TC relays. Thus, the relay activity in a particular TC cell is absent until an action involving this TC relay is selected in the striatum. Striatal inhibitory input to GPi then increases, leading to a decrease of GPi activity and to a subsequent release from inhibition of the selected TC relay.The variable I Striatum\u2192GPi (see ) has been introduced in the eRTM to model the striatal input to the GPi due to the direct pathway, thus becoming the input variable of the control system modelled by the eRTM. GPi-MFR and ES% could then be considered as the output variables of the eRTM. The method used here to model the striatal input to the GPi (a simple constant current to GPi cells) could not entirely capture its local effects on GPi, but could approximate its network effects on BGS.GPi-MFR (panels (a)) increased monotonically for all the tested configurations of the eRTM and for all the tested values of g GPe\u2192GPi when I Striatum\u2192GPi was changed from \u221213 to 3\u00a0pA/\u03bcm2.Mean SI (panels (a)) for the PD state was much higher than in the physiological state for all the tested values of g GPe\u2192GPi. This result shows that the ability of the STN-GPe module of eRTM to spread PD patterns to the GPi is robust against changes in the GPe \u2192 GPi connectivity strength.Increasing g GPe\u2192GPi from 0 to 1\u00a0nS/\u03bcm2 translated the GPi-MFR curves toward lower values in all the tested states of the eRTM, accordingly to the inhibitory nature of this connection. This effect was weaker (a) in the GPi-DBS state (GPi-DBS ensuring an a-priori activation of GPi at DBS frequency) and (b) in the GPe-DBS state (GPe-DBS ensuring a stable and robust inhibition of GPi\u2014except for the case g GPe\u2192GPi\u2009=\u20090) as compared to the physio-pathological states. In the physiological state, the GPi-MFR range moved from [12 130] to [4/80] APs/s, while in the PD state the GPi-MFR range moved from [18 135] to [15 100] APs/s. The effect was much stronger in the STN-DBS state, as already partially seen in Section\u00a0 and discussed in Section\u00a0. Moving g GPe\u2192GPi from 0 to 1\u00a0nS/\u03bcm2, the indirect GPe-driven inhibition of GPi subsequent to STN-DBS increasingly overrode the direct STN-driven excitation.ES% (panels (b), (c), (d)) decreased monotonically (or remained stable) for all the tested configurations of the eRTM and for all the tested values of g GPe\u2192GPi and g GPi\u2192Thalamus when I Striatum\u2192GPi was moved from \u221213 to 3\u00a0pA/\u03bcm2.Looking at the physiological state of eRTM, the value of 0.16 and 0.24\u00a0nS/\u03bcm2 for g GPi\u2192Thalamus were appropriate values, since with these values the direct current I Striatum\u2192GPi modulated the ES% from \u223c10% (I Striatum\u2192GPi\u2009=\u20090\u00a0pA/\u03bcm2) to \u223c90% (I Striatum\u2192GPi\u2009=\u2009\u221213\u00a0pA/\u03bcm2) for most of the g GPe\u2192GPi values, in agreement with the AS theory. In contrast, the value of 0.08\u00a0nS/\u03bcm2 for g GPi\u2192Thalamus was not appropriate to allow AS theory to be included in the model (ES% for I Striatum\u2192GPi\u2009=\u20090\u00a0pA/\u03bcm2 were too high, for every tested value of g GPe\u2192GPi).The control mechanism described in point (e) was partially disrupted in the PD state, where ES% increased much more slowly than in the physiological state. The PD curves showed a decreased slope for a wide range of the I Striatum\u2192GPi values compared to the physiological curves, leading to (1) an incomplete activation of the TC relay for high |I Striatum\u2192GPi| values and to (2) an incomplete inhibition for low |I Striatum\u2192GPi| values. Our results showed that, in the eRTM framework: (a) the disruption of the control mechanism in the PD state is not, per se, a consequence of an increase of the GPi-MFR values compared to the physiological state (indeed, ES% curves for physiological and PD states crossed each other in most of cases, while GPi-MFR curves did not) and (b) the disruption of the control mechanism in the PD state could arise also from an increase in the synchrony amongst the GPi cells (reflected in an increase in SI values), thus confirming the results of Rubin and Terman () in a broader scenario.The control mechanism described in point (e) was completely disrupted in the GPi-DBS state (ES% curves were almost always close to 0 for every value of I Striatum\u2192GPi) and in the GPe-DBS state (ES% curves were close to 100 for almost every value of I Striatum\u2192GPi). ES% values in the GPi-DBS state were different from 0 only for low g GPi\u2192Thalamus values. ES% values in the GPe-RTM state were different from 100 only for low |I Striatum\u2192GPi| values and for low g GPe\u2192GPi values (both factors allowing GPi-MFR being different from 0). Thus, GPi-DBS led to a functional inhibition of the TC relay activity, while GPe-DBS led to a functional over-activation.The control mechanism described in point (e) was potentially restored during STN-DBS. The condition for this functional restoration was to choose an appropriate value for g GPe\u2192GPi (the position of the strictly decreasing portion of the ES% curves in the STN-DBS were very sensitive to g GPe\u2192GPi, for the reasons already explained in point (c)). Note also that ES% curves in the STN- and GPe-DBS states were steeper than in any other state.For the reasons explained in points (e) and (h), we chose for the following analysis and discussions g GPe\u2192GPi\u2009=\u20090.5\u00a0nS/\u03bcm2 and g GPi\u2192Thalamus\u2009=\u20090.16\u00a0nS/\u03bcm2. As can be seen in Fig.\u00a0(c), this choice allowed (a) a proper integration of the AS theory in the eRTM and (b) a suitable explanation for the clinical effectiveness of STN-DBS (the latter being further discussed and justified in the following sections).GPe- and GPi-DBS modulated the GPi-MFR at levels that could lead to a functional over-activation (GPe-DBS) or to a functional inhibition (GPi-DBS) of the TC relay activity. This \u2018polarization\u2019 effect toward complete or absent TC responsiveness to SMC stimuli (1) increased monotonically with the DBS frequency and (2) was only partially modulated by the level of striatal inhibition to GPi.The network effects of frequency STN-DBS on GPi-MFR and TC responsiveness are not so easy to interpret, due to the disynaptic (STN \u2192 GPi and STN \u2192 GPe \u2192 GPi) connectivity from STN to GPi.STN-DBS allowed the overall recovery of the striatal control on the TC responsiveness for every tested DBS frequency (setting I Striatum\u2192GPi to 0 and \u221211\u00a0pA/\u03bcm2 led to ES% curves close to 0 and 100, respectively, thus qualitatively recovering the relationship between ES% and I Striatum\u2192GPi found in the normal state). However, the recovery was heavily influenced by the DBS frequency, as can be inferred by the GPi-MFR and ES% curves with I Striatum\u2192GPi\u2009=\u2009\u22125\u00a0pA/\u03bcm2. During 30\u00a0Hz DBS, STN cells responded in a 1:1 ratio with DBS stimuli. GPe cells, excited by STN ones, fired at the DBS frequency. The resulting network contribution to GPi cells lead them to fire at 60\u00a0Hz (a couple of APs for every STN AP). During 60, 90, and 120\u00a0Hz, STN and GPe cells still fired in a 1:1 ratio with the DBS stimuli. However, the duration and the amplitude of STN APs diminished as the DBS frequency increased, leading (a) the total inhibition from GPe activity to progressively override the total excitation from the STN activity and (b) GPi cells to be completely inhibited by the GPe activity at 120\u00a0Hz. During 150\u00a0Hz DBS, one out of two DBS stimuli occurred during a refractory period of STN cells, leading to reduced STN APs that were strong enough to drive GPi APs but not GPe APs, thus allowing a GPi activity at 75\u00a0Hz. During 180\u00a0Hz DBS, STN and GPe cells still fired in a 1:1 and in a 1:2 ratio with the DBS stimuli, respectively. However, the duration and the amplitude of STN APs diminished allowing the GPe activity to be more effective in inhibiting GPi cells, that fired at 45\u00a0Hz.The DBS frequency plays an important role in PD treatment. Usually, frequencies less than 100\u00a0Hz are thought to worsen PD symptoms, while frequencies greater than 100\u00a0Hz have been proven to have therapeutic benefits (Perlmutter and Mink ). Simulation results on the eRTM may provide a partial explanation of these clinical findings Low-frequency GPe- and GPi-DBS (e.g., 30 and 60\u00a0Hz) were not sufficient to drive the striatal control over the TC relay activity toward a functional over-activation (GPe-DBS), or a functional inhibition (GPi-DBS). Low-frequency STN-DBS was instead sufficient to drive a functional restoration of the control mechanism, but, interestingly, only high-frequency STN-DBS (90, 120, and 180\u00a0Hz, with the exception of 150\u00a0Hz) was able to restore the control mechanism in a limited range of I Striatum\u2192GPi ([0\u20135] pA/\u03bcm2, instead of [0\u201311] pA/\u03bcm2). A limited range of I Striatum\u2192GPi could be associated with PD, as explained in Sections\u00a0 and . However, we did not deepen further this point in the present work.Simulation results agree with experimental results about the DBS network effects achieved in human subjects and monkeys by microdialysis and extracellular recording procedures. Microdialysis evidences in PD subjects suggest that STN-DBS increases the STN activity (Stefani et al. ) and augments STN-driven excitation of GPi, while simultaneously decreasing GABA extracellular concentrations in the anteroventral thalamic motor nucleus, one of the thalamic nuclei targeted by GPi (Stefani et al. ). These speculations are confirmed by our simulation results, that suggest a simultaneous activation of STN and inhibition of GPi during STN-DBS. Indeed, GPi-MFR values were lower in the STN-DBS state than in the normal or PD ones for the physiological range of I Striatum\u2192GPi (Fig.\u00a0(a)). Note that values for g GPe\u2192GPi lower than 0.5\u00a0nS/\u03bcm2 would not allow this result (Figs.\u00a0(a) and (a)).Our findings are consistent also with results obtained by Kita and colleagues (Kita et al. ) on non-MPTP treated monkeys. In their work, single pulses and high-frequency stimulations (110\u00a0Hz) of STN evoked powerful excitatory responses in GPe neurons, while evoking a predominantly inhibitory response in GPi neurons. Their data suggest that the STN \u2192 GPe excitatory response dominates the STN \u2192 GPe \u2192 GPe recurrent inhibition in the GPe, whereas the STN \u2192 GPe \u2192 GPi inhibitory response dominates the STN \u2192 GPi excitatory response in the GPi.Hashimoto et al. () and Kita et al. () discussed the role of di-synaptic connectivity between STN and GPi, and both observed alternating phases of excitation and inhibition of GPi cells after every STN-DBS pulse. Simulation results on the network effects of STN-DBS at 120\u00a0Hz by using eRTM differed significantly from those obtained by Hashimoto et al. (). Their results suggested that GPi-MFR significantly increased during STN-DBS at 136\u00a0Hz. Results comparable to those from Hashimoto et al. () were obtained in the eRTM by increasing the DBS frequency from 120 to 150\u00a0Hz (Fig.\u00a0). Even if we did not conduct a complete input\u2013output analysis for that DBS frequency, the GPi-MFR value for STN-DBS with I Striatum\u2192GPi\u2009=\u2009\u22125\u00a0pA/\u03bcm2 (Fig.\u00a0) suggests that GPi-MFR values in the STN-DBS state could be higher than in the normal or PD ones for a wide range of I Striatum\u2192GPi values.Simulation results showed that GPi-DBS acted to: (a) stabilize the GPi \u2192 thalamus inhibition at a level that is as high as the DBS frequency and (b) progressively inhibit TC cells as DBS frequency increases (complete inhibition at 180\u00a0Hz). These results are consistent with those from Anderson et al. () in an extracellular recording study in MPTP-treated monkeys, where GPi-DBS at 100\u00a0Hz inhibited or decreased the activity of most of the downstream thalamic cells.It has been argued that a decrease in nigrostriatal dopamine after PD onset could affect the direct pathway of BGS in two ways: (1) by a net decrease in striatal input to the GPi mediated by striatal D1 receptors (Delong  and Albin et al. ) and/or (2) by a lack of striatal selectivity, with a concomitant spreading of uncontrolled activation to BGS channels not involved in the selected action. The first hypothesis (modification of the basal level/range of striatal input to GPi in PD) is compatible with the eRTM, as shown below (the eRTM already takes into account two channels, with eight GPi cells and one TC cell each). The second hypothesis (lack of striatal selectivity) would require further improvements in the model by adding a striatal module to the eRTM.Figures ,  and  qualitatively show in a dynamical experiment: (a) the full compatibility of the AS theory with the eRTM, (b) the static results already shown in Fig.\u00a0(c), and (c) the short duration of the transient phases in the GPi-MFR and the TC relay activities against dynamical changes of the striatal input to GPi.A full understanding of the therapeutic effects of DBS should take into account concomitant pharmacological treatment with l-dopa. The eRTM allows for the modelling of l-dopa consumption, by assuming that l-dopa simply acts to increase the basal level/range of striatal input to GPi (qualitative elements for a discussion on this issue are presented in the next section). The effects of l-dopa on the striatal selectivity would need further improvements in the eRTM to be modelled.In clinical practice, the preferred surgical target for DBS in the treatment of PD is the STN (Weaver et al. ; Follett et al. ). However, results from randomized studies (Burchiel et al. ; Anderson et al. ; Rodriguez-Oroz et al. ; Weaver et al. ; Follett et al. ) recently showed that both STN and GPi are effective DBS targets, leading to a renewed interest in GPi as a DBS target for PD (Okun and Foote ) and to the creation of other randomized studies (Follett et al. ; Weaver et al. ).As stated before, our simulation approach: (a) outlines three different network effects that could be associated with STN-, GPe-, and GPi-DBS and (b) qualitatively allows the modelling of the AS mechanism (in both normal and PD state of the eRTM) and of the l-dopa consumption by assuming they act by modifying the basal level/range of striatal current to the GPi. Consequently, within the eRTM we could relate: (a) bradykinetic and akinetic aspects of PD\u2014to the poor thalamic responsiveness (due to the PD state plus a reduced range of striatal current to the GPi); (b) dyskinetic events in PD\u2014to the non-zero thalamic activation in the OFF state of a given channel and to the phasic noise induced through the indirect pathway on the GPi activity; (c) benefits from l-dopa consumption\u2014to a static increase of the range of striatal current to the GPi; and (d) dyskinetic events related to l-dopa consumption\u2014to dynamical, uncontrolled modifications in the basal level/range of striatal current to the GPi (leading to uncontrolled activation of the thalamus).We will read here our results from simulations using eRTM in the light of the few elements that the clinical literature provides as distinctive of STN, GPe, and GPi targets or as part of the common expert clinical judgement in the decision about the preferred surgical target for a specific PD patient.The functional inhibition of TC relay activity that we observed as a result of the high frequency GPi-DBS could be the responsible for the larger control of dyskinesia observed by skilled surgeons in GPi-DBS. Are wide and rapid fluctuations in striatal dopamine (and, consequently, in striatal input to GPi) due to l-dopa consumption in the advanced stages of PD neutralized by this functional inhibition? Our results could suggest that GPi is an ideal target for patients mostly impaired by hyperkinetic signs of PD.The eRTM does not clarify why a complete inhibition of the TC relay activity due to GPi-DBS would bring to clinical benefits for PD patients. However, the cortex \u2192 BGS \u2192 thalamus \u2192 cortex loop is not the unique anatomical and functional circuit in the central nervous system devoted to the movement control. Other circuits, and primarily the cortex \u2192 cerebellum \u2192 thalamus \u2192 cortex, are involved in the motor control and co-control the thalamic activity. Lewis et al. () have recently shown in a fMRI study on twins clinically discordant for PD that, despite some degree of functional segregation, these two circuits are functionally-related and task-specifically influenced by PD and l-dopa consumption. We could speculate that a regular, tonic output of the GPi (forced by GPi-DBS) is less disrupting the other circuits that control the thalamic activity than an irregular, bursty output, thus leading to clinical benefits. However, this hypothesis remains to be tested in a more complete model for the motor control, which should include the other circuits cited above.Several comparative studies of STN- and GPi-DBS reported that patients undergoing GPi-DBS still received a larger daily l-dopa dose than those undergoing STN-DBS (Anderson et al. ; Ostergaard and Sunde ). The functional restoration of the striatal control of the TC relay activity that we observed as a result of the high-frequency STN-DBS could be the responsible for this reduced need of l-dopa in patients undergoing STN-DBS.Bradykinesia tends to improve more with STN- than with GPi-DBS (Ostergaard and Sunde ). Based on our results, we hypothesize that STN stimulation speeds up the execution of movements and that STN could be an ideal target for patients mostly impaired by hypokinetic signs of PD.Finally, it has been observed that GPe-DBS improves bradykinesia and akinesia compared to GPi-DBS and that GPe-DBS induces more dyskinetic events than GPi-DBS (Vitek et al. ). This observation is consistent with the \u201calways on\u201d activation of the thalamus due to GPe-DBS.In order to allow the building of more controllable, larger network models of the BGS, we think that future improvements of the RTM and of the eRTM should consider simpler models of single cells, eventually derived from those that have been used in the eRTM so far.The eRTM incorporates a mono-compartmental cell representation. Consequently, there is a wide range of phenomena that has been proposed (see Benabid et al. ; Lozano et al. ; and McIntyre et al. ) to explain the local effects of extra-cellular stimulation that cannot be further investigated with the eRTM. These phenomena include: (1) differential effects varying electrode-to-neurons distance, (2) differential effects on somas, dendrites, and axons, (3) synaptic activation/inhibition effects, (4) neurotransmitters depletion, and (5) antidromic propagation of AP.Rubin and Terman assumed that the main local effect of DBS is that DBS pulses enhance firing in the target nuclei, and modelled DBS pulses by injecting a current in the cells of the target nuclei. We kept this simplification, even if it was found that, in most DBS configurations (varying across polarity and type of stimuli), DBS pulses may inhibit somas and dendritic trees while exciting efferent axons (see McIntyre et al. ). Since the main network effects of DBS are mediated by efferent axons in the eRTM, we considered acceptable the theory about the local effects of DBS proposed by Rubin and Terman. However, the findings by McIntyre et al. () substantially limit any sensitivity analysis regarding variation in duration and amplitude of DBS pulses in the eRTM framework, since there are not direct relationships between these parameters in extracellular and intracellular stimulation protocols (McIntyre et al. ). This consideration explains why we did not repeat the sensitivity analysis on these DBS parameters performed in the original paper by Rubin and Terman and in a recent work on RTM by Feng et al. ().The RTM does not consider any possible functional encoding of information in the physio-pathological functioning of BGS. An improvement has been achieved in this work, by including some features (the direct pathway) of the action selection theory in the eRTM. The action selection mechanism was modelled just in the direct pathway, while it could involve also the indirect and the hyperdirect ones (as modelled in Humphries et al.  and Leblois et al. ). The direct pathway was the first to be identified as a candidate neural substrate of the action selection mechanism, since it shows clear, funnel-like channel segregation among the nuclei it involves. Future improvements are needed, in order to include also functional representations of: (1) the indirect pathway (that is present in the eRTM, but only to explain the transition from the physiological to the PD states), (2) the hyperdirect pathway, and (3) the striatal selectivity.The results shown in this paper only pertain to a single realisation of the connectivity scheme in the eRTM, as in (Rubin and Terman ). Whereas we suspect (based on the analysis and results provided in Terman et al. ) that the qualitative properties of the results obtained in the physiological and in the PD states of the eRTM are robust to changes in its underlying network (especially in the mutually connected STN and GPe populations), nevertheless we should corroborate this hypothesis by further analysis.We set a computational model of the BGS, the eRTM, to integrate the action selection theory by the modelling of the direct pathway and some adjustments in two connectivity parameters. Then, we used the eRTM to compare the different network effects of STN-, GPe-, and GPi-DBS for PD. Our findings suggest that (a) STN-DBS, (b) GPe-DBS, and (c) GPi-DBS could have three completely different network effects: (a) a functional restoration, (b) a functional over-activation, and (c) a functional inhibition of the TC relay activity, respectively.Our results were consistent with the experimental evidences regarding STN- and GPi-DBS and with the few comparative clinical evidences regarding STN-, GPe-, and GPi-DBS. We further showed that even a simple improvement (the modelling of the direct pathway) of the seminal RTM toward the action selection theory framework significantly increased the power of the model in terms of (a) ability to interpret the clinical and the experimental evidences and (b) possible further usability as a unique framework to explain not only the network effects of DBS but also the mixed network effects of DBS and l-dopa.Basic sets of equations of the eRTM are presented here. See Rubin and Terman () and Terman et al. () for more details on parameters and equations used.The model features: potassium and sodium spike-producing currents I K e I Na; a low-threshold T-type Ca2+ current (I T); a high-threshold Ca2+ current (I Ca); a Ca2+ activated, voltage-independent after-hyperpolarization K+ current (I AHP); and a leak current (I L). These currents (for STN, GPe, GPi and thalamic cells) are described by Hodgkin\u2013Huxley formalism. Constant, depolarizing current I MoreSTN were introduced in Rubin and Terman () to simulate diffuse excitatory input from cortex to STN cells.Constant, depolarizing currents I MoreGPe and I moreGPi were introduced to simulate more diffuse excitation from STN to pallidal cells than that reproducible by synaptic connections. I striatum\u2192GPe and I striatum\u2192GPi represent the constant inhibitory current input from striatum to pallidal cells. I SM represents a train of rectangular depolarizing current pulses from the cortex, identified by pulse amplitude (6\u00a0pA/\u03bcm2), pulse length (6\u00a0ms), and pulse repetition frequency.The summation is taken over the presynaptic \u03b1 cells.The input currents to the cells are negative if inhibitory, positive if excitatory under the sign convention used here.", "s10827-008-0098-2": "This paper presents a synergistic parametric and non-parametric modeling study of short-term plasticity (STP) in the Schaffer collateral to hippocampal CA1 pyramidal neuron (SC) synapse. Parametric models in the form of sets of differential and algebraic equations have been proposed on the basis of the current understanding of biological mechanisms active within the system. Non-parametric Poisson\u2013Volterra models are obtained herein from broadband experimental input\u2013output data. The non-parametric model is shown to provide better prediction of the experimental output than a parametric model with a single set of facilitation/depression (FD) process. The parametric model is then validated in terms of its input\u2013output transformational properties using the non-parametric model since the latter constitutes a canonical and more complete representation of the synaptic nonlinear dynamics. Furthermore, discrepancies between the experimentally-derived non-parametric model and the equivalent non-parametric model of the parametric model suggest the presence of multiple FD processes in the SC synapses. Inclusion of an additional set of FD process in the parametric model makes it replicate better the characteristics of the experimentally-derived non-parametric model. This improved parametric model in turn provides the requisite biological interpretability that the non-parametric model lacks.Synaptic transmission is a nonlinear dynamic process that plays a critical role in signal transmission and information processing in the nervous system (Zucker and Regehr ). The term \u201cdynamic\u201d implies that the causal effects of a presynaptic event spread into the future values of the postsynaptic events and are not limited to the present time. The term \u201cnonlinear\u201d means that the combined effect of two or more presynaptic events is different from the simple concatenation of the postsynaptic events that would have been caused by each of the presynaptic events separately. Thus, the process of synaptic transmission can also be viewed as a nonlinear dynamic input\u2013output transformation of a sequence of presynaptic events into postsynaptic events in a manner that is use-dependent (i.e., depends on the specific temporal pattern of presynaptic events) and is also termed short-term plasticity (STP).STP can be modeled parametrically or non-parametrically for different aims (see Introduction of the companion paper). Parametric models are often developed to explain the underlying biological mechanisms and thus have a predictive power in terms of how the biological processes determine the systems behavior, e.g., input\u2013output transformational property of the system (Dittman et al. ). By contrast, non-parametric models are built to quantitatively describe such input\u2013output transformational property in a model-free manner (Krausz and Friesen ; Marmarelis and Marmarelis ; Berger et al. ,; Sclabassi et al. ; Berger et al. ; Bishop ; Marmarelis ). The advantages of a non-parametric model for such aim are the following: in terms of model configuration, a non-parametric model takes a general model form that is applicable to almost all causal systems, thus avoids potential errors in the postulation of the model structure. In terms of parameter estimation, a non-parametric model is estimated from input\u2013output data collected under broadband condition and inherently valid for such condition.The main aim of this study is to combine both parametric and non-parametric modeling methods in a synergistic manner to study STP in CNS synapses. In the first half of this study (see companion paper), non-parametric models of synaptic STP are estimated from input\u2013output data simulated with several parametric STP models. Results show that non-parametric model (in the form of Poisson\u2013Volterra kernels) can accurately and efficiently capture the nonlinear dynamics defined by those parametric models. Volterra kernels provide a general and quantitative representation of the synaptic STP. Furthermore, by relating the kernel shapes of the non-parametric model to the key parameters of the parametric model, many insights are gained on how the biological processes (represented by the parameters) shape the input\u2013output functional properties (described by the kernels) of the synapse.The non-parametric model constitutes a canonical and complete representation of the system (nonlinear) dynamics that is derived directly from the broadband data. Thus, the obtained model is not restricted by any prior assumptions and can be used as the \u201cground truth\u201d to evaluate the parametric models of the system in terms of its input\u2013output transformational property. In this paper, we estimate the non-parametric Poisson\u2013Volterra kernel (PV) models of the Schaffer collateral to hippocampal CA1 pyramidal neuron (SC) synapse under two extracellular calcium conditions. Results show that the PV models more accurately capture the synaptic nonlinear dynamics than a parametric facilitation/depression (FD) model under both conditions. On the other hand, since these non-parametric models are descriptive representations of synaptic nonlinear dynamics, they can accurately predict the synaptic output under each condition but lack the ability to explain the obtained synaptic nonlinear dynamics in a physiologically-interpretable manner. To get better understanding to the underlying mechanism, we combine the parametric and non-parametric models again\u2014we validate and modify the parametric FD model using the non-parametric PV model. Several significant discrepancies between the experimentally-derived non-parametric model and the equivalent non-parametric model of the parametric model are found in the PV kernels. These discrepancies suggest the presence of multiple FD processes in the SC synapses. Inclusion of an additional FD process in the parametric model makes it replicate better the characteristics of the experimentally-derived non-parametric model. The modified parametric model provides in turn the requisite biological interpretability of the model components\u2014whereby the advocated synergistic use of the two modeling approaches.Hippocampal slices were prepared from young adult male Sprague\u2013Dawley rats (80\u2013200\u00a0g). Animals first were anesthetized with 5% halothane, and then were decapitated and the hippocampi were rapidly dissected. Both hippocampi were sectioned into blocks while being washed with cold, oxygenated medium and slices of tissue (350\u00a0\u03bcm thick) then were cut perpendicular to the longitudinal axis using a vibratome (VT-100S; Leica). Slices were incubated with medium consisted of (in mM): 128 NaCl; 2.5 KCl; 1.25 NaH2PO4; 26 NaHCO3; 10 glucose; 2 CaCl2; 3.0 MgSO4, aerated with 95% O2/5% CO2. Hippocampal slices were maintained at 32\u00b0C for 30\u00a0min and then at room temperature throughout the entire experiments. During the recording session, slices were transferred to the recording chamber and perfused at flow rates of 2\u20133\u00a0ml/min; the perfusion medium was changed according to the experimental purposes.A bipolar stimulating electrode (a pair of twisted, insulated nichrome wires) were placed in the Schaffer collateral to orthodromically activate CA1 pyramidal cells. A cut was made between the CA3 and CA1 regions to prevent epileptiform activity in the CA3 region from affecting the recording in CA1. Biphasic current impulses (200\u00a0\u03bcs in duration) controlled by a stimulator (Master-8; AMPI) were delivered to the tissue via stimulation isolation units (ISO-Flex; AMPI). The stimulation intensity varied from 100 to 700\u00a0\u03bcA. Since the cells were under voltage-clamp and the synapses could be taken to be independent current sources, the strict control of stimulation intensity was relaxed. Instead of using stimulation/response function (I/O curve), stimulation intensities were chosen to elicit roughly constant EPSC amplitudes reflecting approximately the same number of activated synapses. Under 2\u00a0mM [Ca2+]o condition, the baseline EPSC amplitudes were in the range of 150\u2013600\u00a0pA; under 1\u00a0mM [Ca2+]o condition, the baseline EPSC amplitudes were in the range of 80\u2013300\u00a0pA. Too small responses tended to have large variations and too big responses might cause imperfect voltage clamp and spurious EPSCs.Poisson random impulse trains were generated with an STG 1002 stimulator (MultiChannel Systems) and then sent to Master-8. The inter-impulse intervals were determined offline by a Poisson distribution with a mean interval of 500\u00a0ms and a range of 6\u20135,000\u00a0ms. The minimal inter-impulse interval (6\u00a0ms) is longer than the reported refractory period of the Schaffer collaterals (Stevens and Wang ; Dobrunz et al. ). Fixed-interval impulse trains with four frequencies (10, 20, 30 and 40\u00a0Hz) were generated with Master-8. Each train was comprised of ten impulses.Whole-cell recordings of EPSC were performed with an HEKA EPC-9 patch-clamp amplifier from CA1 pyramidal cells visually identified with an infrared microscope (Olympus BX50WI). The glass micropipette was filled with internal solutions containing (in mM): 95 caesium gluconate, 20 TEACl, 10 NaCl, 5 QX-314, 4 Mg-ATP, 0.4 Na-GTP, 0.1 EGTA and 10 HEPES (pH 7.0, titrated with gluconic acid). Some of the experiments were performed with a reduced formula containing (in mM): 130 caesium gluconate, 5 CsCl, 0.1 CaCl2, 1 BAPTA and 10 HEPES (pH 7.0, titrated with gluconic acid). Most of the cells were voltage-clamped at a holding potential of \u221270\u00a0mV. Some cells were held at \u221290 or \u2212120\u00a0mV. Glass pipettes were pulled using a horizontal puller (Model P-80 PC; Sutter Instrument Co.). They had a resistance of 2\u20134\u00a0M\u03a9 and were not polished or coated. The serial resistance varied between 5 and 30\u00a0M\u03a9. The input resistance of the cells was higher than 1.0G\u00a0\u03a9 (on-cell mode). Responses were sampled at 10 or 20\u00a0kHz with pulse data acquisition software (HEKA). Recorded data were exported in ASCII format and then imported to pClamp 9.0 (Axon Instruments) and Matlab (The MathWorks, Inc.) for further analysis.Both fixed-interval and random-interval impulse trains were applied to each cell. In fixed-interval train experiments, each train was repeated 16 times and then averaged. Totally 640 input\u2013output pairs were recorded. In random-interval train experiments, 1,200\u20133,600 input/output pairs were collected.In addition to cutting the CA3-CA1 connection, high concentration of Mg2+ (3\u00a0mM) was used in the incubating and recording mediums to minimize multi-synaptic activity (Mody et al. ). The Ca2+ concentration was altered to manipulate the release probability of the synapses (1 or 2\u00a0mM). To simplify postsynaptic mechanisms and get more reliable measurements to the presynaptic release, NMDA receptors and GABAA receptors were blocked with DAP-V (25\u00a0\u03bcM) and picrotoxin (100\u00a0\u03bcM), respectively. In some experiments, GABAB receptors were blocked by CGP55845a (2\u00a0\u03bcM). AMPA receptor desensitization blocker cyclothiazide (CTZ) was made daily as a 10\u00a0mM stock in dimethyl sulfoxide (DMSO) and diluted to 100\u00a0\u03bcM final concentration in the perfusion medium immediately prior to application.Note that, although synaptic transmission involves both presynaptic and postsynaptic mechanisms, most STP studies to date focus on the presynaptic mechanism (Zucker and Regehr ). However, since the neurotransmitter release (which is the real output of the presynaptic region) is difficult to measure, researchers typically use the postsynaptically recorded signal (e.g., EPSC) to infer the strengths of synaptic release. In order to do this, it is necessary to simplify the postsynaptic mechanisms by voltage-clamping in the postsynaptic region and by removing postsynaptic voltage-dependent channels (e.g. NMDA). These pharmacological manipulations were used to isolate the presynaptic mechanisms and, therefore, the study of the STP process refers to the presynaptic transformation of a sequence of action potentials arriving at the bouton (input) to the EPSCs (output) recorded from the soma.The model order is determined by means of the predictive accuracy of PV models of ascending order (1st, 2nd, 3rd and 4th), which is quantified by the normalized root-mean-square error (NRMSE) of the output prediction for Poisson RIT inputs. The 3rd-order model is found consistently to be adequate for capturing the dynamic nonlinearities of the system as reflected on the broadband input\u2013output data (i.e., the obtained 4th-order model decreased NRMSE only marginally (<1%) that does not justify the inclusion in the model of the 4th-order kernel). This is in agreement with the results of the companion computational study (Part I).All PV kernels for this synapse are estimated with a system memory of 2,000ms and a sampling interval of 1ms. The number of Laguerre basis functions is chosen to be 4, which is the value allowing the most accurate prediction without overfitting, as determined by the model-order selection criterion for Volterra-type models (Marmarelis ). The expansion coefficients of the kernels are estimated through ordinary least-squares using singular value decomposition. The optimal value of the Laguerre parameter \u03b1 was searched in the range from 0 to 0.999 and the average value was found to be 0.920 over the available input\u2013output datasets.These defining relations indicate that the 1st-order RD, r 1, is equal to the baseline EPSC amplitude; the 2nd-order RD, r 2(\u03c4), represents the total change (including both 2nd and 3rd-order) in the present EPSC amplitude caused by a single preceding impulse, as a function of their time interval \u03c4; the 3rd-order RD, r 3(\u03c4 1, \u03c4 2), represents the joint effect (exclusive of their individual effects described by r 2(\u03c4 1) and r 2(\u03c4 2)) of two preceding impulses on the present EPSC amplitude, as a function of their time intervals \u03c4 1 and \u03c4 2. Without loss of generality, both r 2 and r 3 are normalized with r 1.The experimental data were collected under two different [Ca2+]o conditions (2 and 1\u00a0mM). Under each condition, both fixed-interval trains (FIT) and random-interval trains (RIT) of stimuli were applied. EPSC amplitudes were extracted using the deconvolution technique described in Section . Third-order Poisson\u2013Volterra (PV) models of STP in SC synapses were estimated from the RIT datasets that contain PV kernels from which the RDs were computed in order to study the input\u2013output transformatrion characteristics of the SC synapse.In the high-calcium condition (shown in the first row of Fig.\u00a0), the computed r 2 rapidly increases to the peak value of 27 \u00b1 9% in the short IPI range of 6\u201340\u00a0ms; while in the intermediate IPI range of 40\u2013440\u00a0ms, r 2 decreases from the peak to a minimum value of \u22129 \u00b1 2%, crossing into negative values for an IPI around 200\u00a0ms. In the long IPI range beyond 440\u00a0ms, r 2 returns asymptotically to zero, practically vanishing after an IPI of approximately 1,600\u00a0ms. The computed r 3 is primarily negative, reflecting the dominant 3rd-order suppressing effects of pairs of preceding impulses, especially for IPIs shorter than 200\u00a0ms, as it is evident in the diagonal slice of r 3 which exhibits a fast initial exponential phase with approximate time constant of 11\u00a0ms and a slower phase that can be approximated by the difference of two exponentials with the time constant being approximately 87\u00a0ms. These results suggest that the STP dynamics in the SC synapse involve at least two processes with different time constants.The biphasic form of the computed r 2 and the negative monophasic form of the computed r 3 under the high-calcium condition indicate that the SC synapses have both facilitative and depressive processes. To explore further the transmission characteristics and the underlying mechanisms of SC synapses, the experiments were repeated with lower [Ca2+]o (1\u00a0mM). Under low-calcium conditions, the initial release probability of the synapse is suppressed and the synaptic depression caused by vesicle depletion is consequently decreased (Rahamimoff ; Creager et al. ). The response dynamics are expected to be dominated by the facilitation processes.Indeed, this dominant facilitation is evident in the FIT responses. As shown in Fig.\u00a0, the EPSC amplitudes increase more in the first four responses (relative to the first response) and then form a broad summit (resembling almost a plateau) over the subsequent responses. The observed ratios of the second responses to the corresponding first responses are 125 \u00b1 8%, 138 \u00b1 4%, 150 \u00b1 25% and 148 \u00b1 18%; and the ratios of the third responses to the corresponding first responses are 142 \u00b1 8%, 171 \u00b1 10%, 204 \u00b1 23% and 197 \u00b1 17%, for the 10, 20, 30, and 40Hz trains, respectively (n = 5).In the case of RIT stimulation, the computed RDs (Fig.\u00a0, bottom row) indicate that r 2 remains positive for all IPI values and practically vanishes beyond an IPI value of 500\u00a0ms (n = 8). A fast rate of decline is observed in the IPI range up to 100ms, where r 2 decays from 178 \u00b1 13% to 126 \u00b1 3% (corresponding to an approximate exponential time-constant of 21\u00a0ms), while the rate of decline in the subsequent IPI range of 100\u2013500\u00a0ms is much slower (corresponding to an approximate exponential time constant of 197\u00a0ms). This result suggests the presence of (at least) two processes of facilitation in the STP dynamics of SC synapses. We also observe in the bottom row of Fig.\u00a0 that r 3 is triphasic and remains negative for most IPI values (i.e., it is slightly positive only for IPI values between 30 and 100\u00a0ms). Comparing the absolute values of r 2 and r 3, we observe that the latter is much smaller and has shorter duration. The results show that under the low-calcium condition, the synaptic nonlinearity is primarily a 2nd-order facilitation (Fig.\u00a0, bottom row)\u2014unlike the high-calcium condition where the synaptic nonlinearity appears to be primarily a 3rd-order depression (Fig.\u00a0, top row).In this study, several strategies were used to isolate the presynaptic mechanisms of STP: postsynaptic active membrane conductances were voltage-clamped; nonlinear NMDA receptors were blocked by D-APV and high Mg2+ in addition to voltage-clamp; GABAA receptors were blocked by picrotoxin; temporal summations of EPSCs were eliminated by the deconvolution procedure, etc. The purpose is to leave only the AMPA receptors and use them to infer the amounts of presynaptic transmitter releases. However, it is known that the AMPA receptors are subject to desensitization that could potentially influence the estimation of presynaptic release, especially in the short IPIs (Trussell and Fischbach ). To eliminate the contribution of AMPA receptor desensitization, a potent AMPA receptor desensitization inhibitor, cyclothiazide (CTZ, 100\u00a0\u03bcM), was used (Yamada and Tang ). After the aforementioned FIT and RIT experiments, CTZ was applied to the perfusion medium and the experiments were repeated after 20\u00a0min.Consistent with the FIT results, the RDs (r 2 and r 3) are not significantly changed by CTZ application, as illustrated in the middle row of Fig.\u00a0 (n = 4). This result of negligible CTZ effect on the STP dynamics of SC synapses under 2\u00a0mM [Ca2+]o condition indicates that the observed STP dynamics are almost exclusively due to presynaptic mechanisms.In order to test the adequacy of the parametric FD model (Dittman et al. ) for Poisson RIT inputs (i.e., a broad repertoire of operating conditions\u2014and not simply periodic inputs), we estimate the 3rd-order PV models of the SC synapse using two types of broadband input\u2013output data: (1) the data resulting from the simulation of the parametric FD model for a Poisson RIT input, and (2) the experimental data collected from the actual synapse for the same input. Comparison of the obtained PV kernels for the two cases allows rigorous and quantitative assessment of the adequacy of the parametric FD model, since the kernels constitute a canonical (i.e., general and complete) representation of the system dynamics that can be considered \u201cground truth\u201d.A note should be made about the intrinsic variations of the biological processes that subserve synaptic transmission. Several studies have shown that central synapses are heterogeneous in release probability, facilitation and depression (Dobrunz and Stevens ; Hanse and Gustafsson ). In this study, both the parametric FD model and the non-parametric PV model are estimated with EPSCs derived from populations of SC synapses in which cross-synapse variations are largely averaged. More importantly, to eliminate the effect of the remaining variations on the comparison of performance between parametric models and non-parametric models, both types of models are estimated using the same input\u2013output datasets that are recorded from the same cell.It is also interesting to examine the effect of the utilized input on the parameter estimates. To explore this, the parameters of the parametric FD model are estimated with both types of input\u2013output data: FIT and RIT datasets, for each cell under each [Ca2+]o condition. This results in two sets of parameter estimates (denoted as FIT-FD and RIT-FD estimates) that are reported in Tables\u00a0 and . Considerable differences exist in most of the key parameter estimates. It is also evident that the NMRSE of the parametric model prediction for a Poisson RIT input is lower when the RIT-FD parameter estimates are used, since the RIT input contains a broader repertoire of stimulation patterns and thus yields more accurate parameter estimates (Tables\u00a0 and ).In order to illustrate further the effect of the utilized input on the obtained models, we show in Fig.\u00a0 the RDs of the 3rd-order PV models obtained by use of the synthetic input\u2013output data of the parametric FD models with parameters estimated from both the FIT and RIT datasets. These two non-parametric models quantitatively and intuitively represent the functional input\u2013output properties of the two parametric FD models (FIT-FD and RIT-FD). Significant differences are evident for both RDs (r 2 and r 3) under the low-calcium condition, but the differences are subtle for the high-calcium condition.Most importantly, we wish to compare the RDs of the non-parametric models estimated from the FD model (shown in Fig.\u00a0) and from the experimental data (shown in Fig.\u00a0) for the same RIT input under the two calcium conditions. The results indicate that both FIT-FD and RIT-FD RDs have significant differences from the RDs of the experimentally-derived non-parametric model.Specifically, under 2\u00a0mM [Ca2+]o condition, the r 2 values of the experimentally-derived non-parametric model (i.e., the true r 2) shows peak paired-pulse facilitation at about 40ms IPI (Fig.\u00a0, top row), but this peak is not captured by the r 2 values of either the FIT-FD or the RIT-FD model, which are both monotonically decaying in the IPI range up to 400\u00a0ms (Fig.\u00a0). The FIT-FD results also overestimate the paired-pulse facilitation for short IPIs more than the RIT-FD results, relative to the results of the experimentally-derived non-parametric model. The latter peak value is 27 \u00b1 9%, while the peak values for the FIT-FD and RIT-FD models are 39 \u00b1 12% and 30 \u00b1 9% respectively. The more severe overestimation of the FIT-FD model is probably due to the fact that the FIT inputs lack IPIs shorter than 25\u00a0ms, where the synapses exhibit weak paired-pulse facilitation (as shown in Fig.\u00a0). Another difference is evident in the values of the negative (depressive) phase of r 2 for IPIs around 400\u00a0ms, where the parametric models exhibit less depressive characteristics. Significant differences are also observed in the 3rd-order RDs, where the diagonal slice of the true r 3 shows a more complex shape that can be fit in the early part (short IPIs) by an exponential with time constant of 11ms and in the late part (IPIs from 300 to 500\u00a0ms) by an exponential with a time constant of 87\u00a0ms, while the diagonal slice of r 3 of the FD models can be fit by a single exponential function with time constant of 69\u00a0ms for the FIT-FD model and 56\u00a0ms for the RIT-FD model. Notably, the magnitude of 3rd-order depression is much larger in the results of the experimentally-derived non-parametric model. Differences are also evident at the off-diagonal values of r 3. These results suggest that more than one FD processes may be active in the actual SC synapse.Under 1mM [Ca2+]o condition, the r 2 values of the experimentally-derived non-parametric model exhibit a complex shape that contains at least two exponentials (Fig.\u00a0, bottom row), while the r 2 values for the FIT-FD and RIT-FD models exhibit single exponential shapes (with comparable time constants of 107 and 103\u00a0ms, respectively). This result again suggests that the parametric FD model oversimplifies the facilitation process. In terms of the 3rd-order RD, the r 3 values of the experimentally-derived non-parametric model are much stronger and exhibit a more complex shape than its FD-model counterparts. The diagonal slice of r 3 exhibits a triphasic form with strong depression for short IPIs and weak depression for long IPIs, while both FD-model counterparts have a single exponential shape with negligible depression at short IPIs (notably the RIT-FD model exhibits very small values for all IPIs).In order to compare quantitatively the predictive capability of the various models, we compute the NRMSE of the output prediction for Poisson RIT inputs. Results show that, under both [Ca2+]o conditions, the predictions of the experimentally-derived non-parametric models are significantly more accurate than the corresponding FD models (paired t-test, P < 0.01), and the RIT-FD models are more accurate than the corresponding FIT-FD models (paired t-test, P<0.01). Since the RIT-FD models and the experimentally-derived non-parametric models are estimated from the same input\u2013output datasets, their difference in NRMSE is solely due to their different model configurations. The FIT-FD models have the same model configuration with the RIT-FD models but different parameter values that were estimated from the FIT input\u2013output datasets, thus the fact that the RIT-FD model performs better than the FIT-FD model is a result of the greater \u201crichness\u201d of the RIT datasets that contain a vastly broader variety of input (and consequently output) patterns.It is evident from these results that the current parametric FD model cannot account for all the nonlinear dynamics captured by the 3rd-order non-parametric model for RIT inputs.Based on the above experimental and simulation results, it appears necessary to augment the parametric FD model by including a second FD process with different residual calcium time constants (\u03c4  F  and \u03c4  D ). The resulting \u201cdouble-process FD model\u201d of the STP dynamics in SC synapses incorporates two sets of facilitation/depression mechanisms that are driven by two different residual calcium dynamics (fast-decaying calcium and slow-decaying calcium). The FD process with fast calcium dynamics is denoted as \u201cfast FD process\u201d, while the other is named \u201cslow FD process\u201d. Note fast/slow residual calcium dynamics do not necessary result in fast/slow FD dynamics, since the latter are mainly determined by the other key model parameters. These key parameters (e.g., the initial release probabilities and recovery rates) are estimated and shown in Table\u00a0. As shown in Eq. (), the total release probability can be represented as the weighted linear summation of the release probabilities of the two processes.The computed 2nd and 3rd-order RDs of the slow and fast FD processes of the double-process FD model under 2mM [Ca2+]o condition are shown in Fig.\u00a0. It is evident that the negative phase of r 2 is mostly due to the fast process, which has a fairly high release probability (0.51). In this process, the first impulse causes the release of more than half of the available vesicles; the maximum paired-pulse ratio cannot exceed one by definition and r 2 is primarily negative. For very short IPIs, the release is enhanced by residual Ca2+ accumulation and the paired-pulse ratio (0.90) is close to its maximum possible value. The r 2 values for this process are close to zero in this IPI range. For longer IPIs, the release is less facilitated and the recovery of depression is less accelerated due to the decay of residual Ca2+. The r 2 values show strong paired-pulse depression. When the IPI is further increased, all parameters recover to their original values and the values of r 2 return back to zero. Due to these reasons, the form of r 2 for the fast process becomes U-shaped and causes the negative phase in the r 2 of the overall model. The fast FD process also shows strong depression in r 3. This is not surprising since its release probability is so high that there are only very few vesicles available after two consecutive releases, even with the facilitatory effect of the residual Ca2+.The slow process has a relatively lower release probability (0.32), which allows paired-pulse facilitation. This process is the main contributor of facilitation in the r 2 of the overall model (Fig.\u00a0). Also, the form of r 2 for the slow process is bell-shaped for short IPIs and the peak of facilitation is properly lagged by 40\u00a0ms (as in the experimentally-derived non-parametric model) instead of happening at the shortest IPI which is the case for the single-process FD model. This can be explained by its high recovery rate and the fact that the number of releasable vesicles is increasing over time, although the facilitation effect is declining. This tradeoff between facilitatory and depressive mechanisms makes the r 2 bell-shaped instead of monotonically decreasing. The results shown in the third row of Fig.\u00a0 (that include both the fast and the slow process) are nearly identical to the experimentally-derived non-parametric model (Fig.\u00a0, top row).The results under 1 mM [Ca2+]o condition are shown in Fig. . The initial release probabilities of the two processes are relatively low, while the paired-pulse facilitation ratios are high. For both processes, depletion plays a smaller role than under 2\u00a0mM [Ca2+]o condition. The values of r 2 are big and the values of r 3 are relatively small. The difference in the shape of r 2 for the fast FD process between the high and low [Ca2+]o condition is caused by at least two reasons: (1) the initial release probability is decreased from 0.51 to 0.18, leading to smaller depletion of the releasable vesicle pool and weaker depression, and (2) the recovery rate is higher in low rather than in high [Ca2+]o condition. The second reason is seemingly contradictory to the residual calcium-dependent recovery hypothesis, which suggests that the recovery from depletion is positively correlated to the residual calcium concentration\u2013lower [Ca2+]o then should cause slower, instead of faster, recovery rate. However, it must be noted that these two recovery rates are estimated with different initial release probabilities that may have different levels of the \u201cenergy barrier\u201d between the depleted and resting states (i.e., it is likely that the low initial release probability state is easier to recover from the depleted state than the high initial release probabilities state). The slow FD process has a positive slow-decaying r 2 and a negative but relatively small r 3 due to the enhanced recovery rate. The high recovery rate prevents the values of r 3 from having a slow and negative component, which is indeed not observed in the experimental data. The overall r 2 caused by these two FD processes (Fig.\u00a0, third row) shows a shape similar to its non-parametric counterpart (Fig.\u00a0, bottom row), while the overall r 3 values still retain some differences between the non-parametric model and the parametric double-process FD model.In this study, the nonlinear dynamics of STP in hippocampal SC synapses are characterized using a non-parametric Poisson\u2013Volterra (PV) model obtained directly from experimental input\u2013output data collected during Poisson random-interval train (RIT) stimulation. The model provides better prediction of the output data than a parametric facilitation/depression (FD) model. The implication of this finding is that the experimentally-derived non-parametric model captures better the nonlinear dynamic characteristics of synaptic transmission and constitutes a more complete functional description of the biological processes involved. Consequently, we used this non-parametric model as the \u201cground truth\u201d for assessing quantitatively the adequacy of the aforementioned parametric model and to explore ways of improving the parametric model by including additional terms/parameters that represent additional biological processes involved in synaptic transmission. This task was achieved by estimating and evaluating the non-parametric surrogates of the parametric model. Evaluation of the surrogates using the experimentally-derived non-parametric model suggested a double-process mechanism of STP in SC synapse comprising a fast-calcium FD process and a slow-calcium FD process.Both the low (1\u00a0mM) and high (2\u00a0mM) [Ca2+]o condition results suggest such double-process mechanism. As we know, STP is determined by facilitation and depression: facilitation is the synaptic enhancement caused by the action of calcium remaining in the presynaptic terminal after preceding action potential(s); depression is simply caused by the depletion of the pool of release-ready transmitter vesicles. With low [Ca2+]o and the subsequent small initial release probability, only a small portion of the transmitter vesicles are released after a stimulus. The size of the release-ready vesicle pool remains relatively constant and the effect of the depletion-dependent depression was weak. STP under this condition thus primarily reflects the dynamics of facilitation. This is verified by the prominent positive r 2 and small negative r 3 in the PV model under low [Ca2+]o condition. The double-exponential shape r 2 suggests that there are two distinct processes of facilitation.When the [Ca2+]o is elevated, the initial release probability of synapses is increased; depletion mechanism is more involved and the PV model reflects the interplay of both facilitation and depression. Both r 2 and r 3 under high [Ca2+]o condition show patterns distinct from those under low [Ca2+]o condition. Interestingly, the two processes show different levels of sensitivity to the [Ca2+]o elevation: in the short inter-impulse intervals where the fast process is involved, r 2 is almost reversed and shows very weak facilitation in the high [Ca2+]o condition; r 3 become significant and shows strong depression. By contrast, in the longer inter-impulse intervals where the slow process is more involved, r 2 and r 3 are not dramatically changed. These results suggest that the fast FD process is more sensitive to the [Ca2+]o elevation than the slow FD process.Double-process facilitation in STP has been reported in a variety of synapses from different species (Eccles et al. ; Zengel et al. ; Gage and Murphy ; Zengel and Magleby ). To our knowledge, this study is the first to suggest its existence in hippocampal synapses of the rat. There are several reasons why this mechanism may have been overlooked by previous studies with traditional paired-pulse or fixed-interval stimulation protocols, but not by this one. First, instead of directly measuring the paired-pulse facilitation as a function of inter-impulse interval, in this study, the 2nd-order RD, r 2, (which is equivalent to the paired-pulse modification function) is computed from the estimated PV kernels of the non-parametric model that is obtained from broadband RIT data where the intervals between impulses are randomly varying, while the traditional paired-pulse or fixed-interval train protocols only include a small subset of possible input patterns. RIT allows to capture the fine details of the STP pattern. Secondly, the STP dynamics of SC synapses are investigated under two calcium conditions in this study, making the differences between the two processes more visible by comparing the results under these two conditions. Lastly, when the STP dynamics are studied using paired-pulse or fixed-interval train protocols, some transient mechanisms may have been mixed with facilitation and depression due to the non-stationary nature of these protocols. By contrast, in the non-parametric approach the model was estimated with RIT data that do not alter the stationarity of the system.There could be multiple ways of modifying the parametric FD model to better fit the experimentally-identified nonlinear dynamics. We propose the double-process FD model for the following two reasons: First, the double-process FD model introduces minimal extension of the FD model in terms of the modeled mechanisms and dynamic processes (but not minimal number of extra open parameters). Additional model parameters can be readily explained using the existing residual calcium-facilitation/depression framework, which is supported by many other independent studies. Second, there are experimental evidences suggesting the existence of two types of synapses. Using minimal stimulation protocol, Hanse and Gustafsson recorded single CA1 synapses and showed their heterogeneity in initial release probability (Hanse and Gustafsson ). Furthermore, the distribution of initial release probability had two peaks, which is consistent with our double-process FD model. Despite these reasons, however, it should be pointed out that, although the double-process FD model accurately replicates the experimentally-identified STP nonlinear dynamics under two calcium conditions, it still should not be considered as a conclusive representation of the CA1 synapse. For example, this study does not rule out the possibility of having a more complex residual calcium dynamics and/or different types of active zones in a single synapse (as opposed to different types of synapses), since it is carried out at the synaptic population EPSC level. Instead, as a parametric model, our double-process FD model provides new mechanistic predictions/hypotheses, e.g., the relations between initial release probability, paired-pulse facilitation ratio, and recovery rate (Table\u00a0), that can be tested with further studies, e.g., recording and modeling of single synapses.Previous studies also showed the existence of depletion-independent mechanisms of depression in the synaptic STP (Hsu et al. ; Dobrunz et al. ; Kraushaar and Jonas ; Waldeck et al. ; Gover et al. ; Kirischuk et al. ; Pedroarena and Schwarz ; Fuhrmann et al. ). These forms of depression mechanisms are not included in the present parametric modeling, but potentially can be added to better explain the obtained STP dynamics. For example, the double-process FD model still underestimates the paired-pulse depression in short IPIs under high [Ca2+]o condition (Fig.\u00a0, bottom-left), compared to the experimental data (Fig.\u00a0, top-left). This discrepancy is likely to be caused by the lack of such mechanisms (e.g., inactivation of presynaptic N-type Ca2+ channels shown by Dobrunz et al.) in the parametric model. Indeed, the proposed combined parametric/non-parametric modeling strategy should be used iteratively for the discoveries and characterizations of new underlying mechanisms.In summary, the main result of this study is the corroborated proposition that an experimentally-derived non-parametric model constitutes a rigorous quantitative tool for assessing the input\u2013output property of any proposed model and provides the quantitative means to guide the manner in which possible improvements of a parametric model can be achieved by inclusion of additional terms/parameters (representing additional biological processes). This issue was examined here in the context of synaptic transmission in the hippocampus but has broad applicability to all neuronal systems. Therefore, this study may serve as an experimental/modeling paradigm for other applications in the context of systems neuroscience.", "s10827-008-0102-x": "Two key features of sensorimotor prediction are preprogramming and adjusting of performance based on previous experience. Oculomotor tracking of alternating visual targets provides a simple paradigm to study this behavior in the motor system; subjects make predictive eye movements (saccades) at fast target pacing rates (>0.5\u00a0Hz). In addition, the initiation errors (latencies) during predictive tracking are correlated over a small temporal window (correlation window) suggesting that tracking performance within this time range is used in the feedback process of the timing behavior. In this paper, we propose a closed-loop model of this predictive timing. In this model, the timing between movements is based on an internal estimation of stimulus timing (an internal clock), which is represented by a (noisy) signal integrated to a threshold. The threshold of the integrate-to-fire mechanism is determined by the timing between movements made within the correlation window of previous performance and adjusted by feedback of recent and projected initiation error. The correlation window size increases with repeated tracking and was estimated by two independent experiments. We apply the model to several experimental paradigms and show that it produces data specific to predictive tracking: a gradual shift from reaction to prediction on initial tracking, phase transition and hysteresis as pacing frequency changes, scalar property, continuation of predictive tracking despite perturbations, and intertrial correlations of a specific form. These results suggest that the process underlying repetitive predictive motor timing is adjusted by the performance and the corresponding errors accrued over a limited time range and that this range increases with continued confidence in previous performance.Predictive tracking of alternating targets requires the correct movement timing (an intersaccade interval close to the ISI) and the appropriate movement latency (less than the time required for a typical saccadic eye movement, approximately 80\u00a0ms; Zorn et al. ). A sequence of predictive saccades from a typical normal subject in this paradigm is displayed in Fig.\u00a0(b). Target position is represented by the gray line; the black line represents the subject\u2019s eye position. As displayed in the figure, when the subject begins to track the alternating targets the first two saccades are reactive (they occur after the stimulus jump with respective latencies of 161 and 166\u00a0ms). This is due to the subject having no prior knowledge of the stimulus timing. However, by the fourth saccade, the eye movement occurs with a latency of 12\u00a0ms signifying that it is a preplanned predictive movement (visual processing and motor delay require approximately 80\u00a0ms). This predictive tracking behavior utilizes two sources of feedback from previous movements: the timing between movements (intersaccade interval) and the timing error (latency). Initially, to overcome the early timing error (the reactive latencies of 161 and 166\u00a0ms), the subject must decrease the intersaccade interval as marked by the upward arrow in panel (b). Then, following this adjustment, the subject tracks the targets with minimal timing error: The subject\u2019s eye movements are in phase with the stimulus. The intersaccade intervals and latency values of the subsequent saccades are approximately 556 (the same as the timing of the stimulus: the ISI) and near 0\u00a0ms, respectively.This adjustment and subsequent tracking behavior is not a trivial phenomenon. For example, if the subject continued to make saccades with the same timing as the first intersaccade interval (that is, no adjustments to the time between saccades), then subsequent saccades would never become predictive, and the error would continue to increase with each eye movement. This is simulated in Fig.\u00a0(c); note that the eye and target (black and gray traces) become more out of phase with each eye movement. In addition to making adjustments based on the timing between previous movements, the subject must also take the latency or initiation error of previous saccades into account. For example, if the subject adjusts the intersaccade interval as in panel (b) but makes no further adjustments (i.e., maintains this new intersaccade interval), then subsequent eye movements will be predictive (occur before the corresponding stimulus jump), but the timing error will increase substantially with each movement. This is simulated in Fig.\u00a0(d). In this case, the subsequent saccades are predictive, but the movements are out of phase with the stimulus as marked by the downward arrow in panel (d). Thus, predictive tracking of alternating targets involves adjustments based on both previous intersaccade intervals and latency values (Zorn et al. ).There are three behavioral results found recently in our laboratory that suggest that the predictive tracking behavior displayed in Fig.\u00a0(b) is based on the internal representation of target timing (i.e., an internal clock), which is modified by the two sources of feedback discussed above. We now review these three behavioral results, as they are relevant to the proposed model that is the main outcome of this study.Another observation regarding the autocorrelation function of the predictive latency time series is that as predictive tracking continues for many trials (approximately 1,000 trials), the correlation window gradually increases. We made this observation initially in our previous work (Shelhamer ) and verify and quantify it here experimentally (described below). The basic finding is that as subjects track for extended durations, they incorporated performance further in the past into their predictions: They build up a statistical model of confidence in the stimulus. This is revealed by an increase in the width of the autocorrelation function with trial number and gives rise to hysteresis in the transition between reactive and predictive tracking, as we show below.The established framework for describing interval timing and the scalar property is an internal clock model (Treisman ; Gibbon ; Meck and Benson ). The first stage of this model is the estimation of time, which is accomplished by the neural accumulation of pulses emitted by a pacemaker (Woodrow ; Hoagland ; the pulses have recently been hypothesized to be the action potentials emitted by dopaminergic neurons in the basal ganglia, see Matell and Meck ). Once the accumulation of the pulses stops, the accumulated value is compared with a sample value of the expected number of pulses (representing the required time duration) stored in memory. If the accumulated value is within the range of the value in memory, a response is made (for example, an eye or hand movement). The accumulated value, representing the most recent inter-response interval, is then pooled with the distribution of samples stored in memory. In this way, the new value stored in memory can affect subsequent timing behavior through feedback of past performance. Rather than the accumulation of pulses, this process can also be described as the integration, over time, of some noisy signal representing the rate of an internal counter. When the integrated signal reaches a threshold, a timed event occurs (a movement), and the integrator is reset to start a new cycle. The error between the time that the integrated signal reached threshold and the desired time interval as determined by an external stimulus can then be used to adjust the timing of future movements via changes in the integration threshold (feedback). The integrated signal has a random component due to neural noise, and this leads to randomness in the event times. More specifically, the longer the interevent interval, the longer is the integration time of the random signal, and hence the more variability there will be in the event times (Sch\u00f6ner ). In this framework, the scalar property derives naturally from the fact that the variability of estimating the passage of time is proportional to the interval being estimated (Buhusi and Meck ).Despite the success of internal clock models in explaining a large set of behavioral and physiological results (Buhusi and Meck ), established clock models of repetitive movements do not include the feedback described above. For example, one of the first motor-timing models for repetitive tapping to a pacing metronome (Wing and Kristofferson , ) incorporated a clock mechanism in the timing of movements (in these repetitive tapping experiments, subjects are required to tap their index finger in synchrony with a pacing metronome). This model separated total timing error into central clock variance and motor delays and assumed that clock intervals and motor delays were independent from trial to trial. This results in a negative dependence between successive intervals (short intertap intervals are likely to be followed by large intertap intervals and vice versa), with no correlation beyond that. As previously shown (Shelhamer and Joiner ; Shelhamer ), this short-term negative correlation does not hold for sequences of predictive saccades, which exhibit extensive long-term correlations between movements. This and other results (Collins et al. ) confirm that repetitive predictive tracking is not an open-loop process and must depend on some source of feedback.Other nonclock models of repetitive tapping incorporate various forms of feedback in interval production. Michon () proposed a \u201clinear predictor model\u201d in which feedback is restricted to the immediately preceding interval but lacks any correction based on synchronization error (tapping latency). Hary and Moore (, , ) suggested a mixed reset model in which feedback is restricted to the synchronization error of the previous two movements. Similar to the simulation shown above in Fig.\u00a0(c), these authors showed that without such feedback control, the metronome and the tapping responses of the subject will become out of phase due of the variability of the tapping responses (Hary and Moore ). In addition to this feedback, the model incorporates a random switching between two resetting strategies: metronome reset in which the next interval is timed from the previous metronome event and response reset in which the next interval is timed from the previous tapping response (Schulze ). The second resetting strategy will be discussed in more detail in Section\u00a0. Similar to Hary and Moore, Vorberg and Wing () also proposed a model in which synchronization error was the only source of feedback. More recent models (Vos and Helsper ; Mates , ) include the feedback of both synchronization errors and inter-response intervals but only utilize a fixed number of previous responses. As a result, these models do not sufficiently reproduce the behavioral results described above for repetitive predictive saccades (e.g., long-term correlations between synchronization errors).There has been one previous model of predictive saccade tracking of targets that alternate in either a symmetric or asymmetric square wave pattern (Schmid and Ron ; Ron et al. ). The model assumed a predictor in each cerebral hemisphere and was developed using steady-state tracking data (disregarding the first five cycles of the stimulus). By basing the model on this limited data, predictive tracking was not formulated as the result of feedback from the sources previously described for Fig.\u00a0(b)\u2013(d). In addition, the prediction of the targets was not due to an internal timing storage as demonstrated in Fig.\u00a0(a) and (b). Rather, the ability to predict (the simulated response time of the eye movement) was based on excitation and inhibition signals (within each hemisphere) whose accumulation and decay rates were dependent upon the stimulus timing (cycle duration) and the degree of asymmetry of the square wave pattern. This is an oversimplified depiction of the tracking behavior that the authors themselves conceded in their discussion: \u201cThis model can therefore be proposed as the simplest way to interpret the general behavior of a subject tracking with his eyes a symmetrical or asymmetrical square wave pattern of target motion.\u201dOur goal was to develop an accurate model of the behavior exhibited during predictive saccade tracking that utilizes a combination of the concepts previously described: (1) reproducing predictive tracking as the outcome of an internal clock (estimating the time between movements based on an internal timing storage), (2) utilizing feedback of previous intersaccade intervals and timing error/latency, and (3) incorporating a time span of past movements over which this feedback is acquired (a correlation window). The model we describe here reproduces the behavioral results described above: (1) the continuation of predictive saccade tracking despite perturbations to the stimulus, (2) the demonstration of the scalar property by the intersaccade intervals during predictive saccade tracking, (3) the long-term correlations between predictive tracking synchronization errors, and also (4) the behavioral phase transition and hysteresis in tracking behavior (reactive to predictive and vice versa), which is described in Section\u00a0.Analysis of eye-tracking data was done offline. First, eye velocity was calculated using a four-point digital differentiator based upon a least-squares derivative algorithm (Savitzky and Golay ). This is an efficient iterative method of fitting a third-order polynomial to each data point and the preceding and following two values, then finding the derivative of the fitted polynomial. Eye movement latency was determined by comparing the onset of the primary saccade to the onset of the target in each trial. Saccade onset was determined using a velocity threshold (\u226560\u00b0/s). The intersaccade interval was the time between each primary saccade.We wished to confirm our initial findings on the size of the correlation window as a function of number of predictive saccades (Shelhamer ). Five subjects (one example depicted in Fig.\u00a0) made 1,000 consecutive saccades at the predictive pacing rate of 0.9\u00a0Hz (ISI of 556\u00a0ms). The series of latency values was divided into four 250-point nonoverlapping segments. The autocorrelation function of each segment was found, and the size of the correlation window in each case was determined as the half-width (since it is symmetric) of the autocorrelation function at a correlation value of 0.2, as in our previous studies (Shelhamer ; Joiner and Shelhamer ).To simulate predictive tracking of alternating targets, we have constructed a closed-loop model that (1) builds an internal estimation of the stimulus timing based on previous intermovement intervals and (2) adjusts this time estimate through the feedback of the preceding initiation error and the projected initiation error of the time estimate. The model can be used to examine both the short-term (perturbations to the stimulus timing) and long-term (scalar property of intersaccade intervals) behaviors of the timing system.In these equations, ft represents the linear rise of the signal (with slope f, 1\u00a0ms\u22121). Additive noise is in the form of a Wiener process, W(t), which is a continuous-time Gaussian stochastic process with independent increments: N(0,\u03c3  w ) is Gaussian with a mean of 0\u00a0ms and standard deviation, \u03c3  w , of 5\u00a0ms. We chose a noise level (standard deviation) of 5\u00a0ms, since this represents an approximate lower limit to the interval at which two consecutive visual stimuli can be distinguished (Artieda et al. ). Thus, 5\u00a0ms is within the noise level of the neural timing system when timing information is provided by a visual stimulus. Figure (a) displays an example of five integrated signals rising to threshold for the time estimation of 500, 833, and 1,000\u00a0ms (physical time). Due to random fluctuations in the signals, the time when an event occurs (the signal crossing the threshold) is a random variable. The higher the threshold level, the larger is the variance of the times when the signal crosses threshold. In terms of timing behavior, this provides larger variance in the estimation of longer intervals (as demonstrated in Fig.\u00a0(a)).The complete timing model, which is depicted in Fig.\u00a0, was constructed in the following manner (numbers in parentheses correspond to the graphics in the figure). Based on previous results on repetitive saccades (Stark et al. ; Ross and Ross ; Zambarbieri et al. ; Shelhamer and Joiner ; Isotalo et al. ; Joiner and Shelhamer , ) and tapping (M\u00e4tes et al. ; Engstr\u00f6m et al. ; Miyake et al. ), when the pacing of the stimulus is slow, the ISI is large, there is no estimation of the target timing (1), and a reactive response is made after the stimulus jump (the delay of the eye movement, \u0394) (2) (this is also the case for the first two movements to the alternating stimulus even at a smaller ISI; the subject can only estimate the timing of the stimulus and the required timing between movements, after experiencing at least one complete intersaccade interval [two movements]).This scheme is depicted in Fig.\u00a0(b). When the two latency terms are equal, the timing of the stimulus and the response are equivalent. When the timing of the response and the stimulus are not the same (I(i)\u2260ISI(i)), this is because the latency at the start of a given interval, L(i), and the latency at the end of the interval, L(i\u2009+\u20091), are not the same. The model corrects for such errors in timing by estimating the appropriate values of latency and interval and using these errors to adjust the duration of the subsequent intersaccade interval. This adjustment is done by changing the threshold of the integrate-to-threshold mechanism. This is a key feature of the model: latency errors are monitored and used to adjust intersaccade intervals. Latency is a physiologically relevant error which should be reduced, while intervals are the controlled quantity in a predictor/clock model.In this equation, I  N  is the most recent interval, and I 1 is the least recent interval within the window. This storage and averaging (with constant weighting) of previous intervals is plausible given the fact that it is a motor act that is being accumulated: we consider it likely that the nervous system can hold and integrate intervals that is has produced (rather than merely sensed) with high fidelity. Since these intervals are the result of motor acts generated by the brain, it would appear to be a simple matter to maintain an efference copy of these intervals for further processing.Next, this estimate of the required interval must be adjusted by previous errors. For example, if the intersaccade intervals were fixed at 450\u00a0ms with an ISI (target interval) of 500\u00a0ms, the location of the eye movements would be completely out of phase with the stimulus location by the tenth movement. Based on Eq. (), the interval estimate is adjusted by the most recently experienced error or latency, L(i), and the projected future error, L(i\u2009+\u20091) (6). Though multiple past errors could conceivably influence the interval estimate, we have experimental evidence that suggests that only the most recent previous error (latency) affects the next intersaccade interval. The relevant results for a sample subject are presented in Fig.\u00a0(c). When the difference in duration between consecutive intersaccade intervals is plotted as a function of latency, there is a significant negative linear relationship (R 2\u2009=\u2009\u22120.8, P\u2009<\u20090.001; this latency is the error experienced between the two intervals; in terms of Eq. (), Fig.\u00a0(c) plots I(i)\u2009\u2212\u2009I(i\u2009\u2212\u20091) vs L(i)). In other words, when the error is less than the mean latency (represented by the dashed vertical black line in Fig.\u00a0(c), \u221272\u00a0ms), the subject increases the duration of the next interval. When the error is more than the mean latency, the subject decreases the duration of the next interval. This change in duration is near 0\u00a0ms at the mean latency, where no change is needed. It is interesting to note that the significance and magnitude of this relationship quickly decrease when the change in duration is compared to errors occurring earlier within the sequence (data not shown), confirming our contention that these errors have little or no influence over the adjustment of subsequent intervals.The projected future latency must be estimated through a less direct process, since it is manifest at the end of interval I(i), which has not yet occurred when the estimate is needed: L(i\u2009+\u20091) is thus the anticipated error resulting from the duration of the estimated intersaccade interval. This error is posited to be represented in the brain as a Gaussian distribution based on predictive tracking experience. For the model, its parameters were determined from experimental data obtained during steady-state predictive tracking, N(\u03bc prediction,\u03c3 prediction) (the determination of the mean, \u03bc prediction, and standard deviation, \u03c3 prediction, of this distribution is described in the following section).The time that u(t), representing time estimation, reaches this threshold (7), sets the timing of the future movement, I NEW (8). Once this threshold is reached, u(t) is reset to zero, and the time estimation process begins again. Thus, our model utilizes a response reset strategy in which the next intersaccade interval during predictive tracking is timed from the previous movement response (Hary and Moore ; Schulze ).The simulation of this model was done on a PC using MATLAB\u2122.Previous studies of normal human subjects tracking alternating targets (Stark et al. ; Ross and Ross ; Zambarbieri et al. ; Shelhamer and Joiner ; Isotalo et al. ; Joiner and Shelhamer , ) have demonstrated that there are distinct pacing rates, ISIs, that promote high-latency-reactive tracking (ISIs ranging from 1,250 to 2,500\u00a0ms) or low-latency-predictive tracking (ISIs ranging from 500 to 625\u00a0ms). Our earlier experiments (Shelhamer and Joiner ; Joiner and Shelhamer ) reported a behavioral \u201cphase transition\u201d as subjects tracked alternating targets as ISI monotonically decreased. When subjects tracked the targets alternating at a large ISI (2,500 and 1,667\u00a0ms), they made reactive eye movements (latency ~180\u00a0ms). As the ISI monotonically decreased, subjects made an abrupt transition at a critical ISI (near the ISI of 714\u00a0ms) to a predictive response (latency\u2009<\u200980\u00a0ms) and continued this behavior at the small ISIs (556 and 500\u00a0ms). This behavioral transition in tracking behavior is depicted for three subjects in panels (a), (b), and (c) of Fig.\u00a0. The circles in each panel represent the saccade latency for a single trial/target jump. The thick black X is the average saccade latency for a given ISI. The solid and dashed black lines represent an abrupt transition fit and linear regression fit to the data. The transition fit (fitting two lines with zero slopes to the reactive and predictive ranges) is a significantly better representation of the data than is the linear fit (see Shelhamer and Joiner ; Joiner and Shelhamer ). As demonstrated in each panel, saccade latency makes a rapid transition from reactive (approximately 200\u00a0ms) to predictive (<0\u00a0ms) as the ISI decreases. The point of transition is different for each subject; panel (a)833\u00a0ms, panel (b)1,000\u00a0ms, and panel (c)1,667\u00a0ms.The results of this experiment can be utilized to determine parameter values in the model simulation. For example, the critical ISI for the behavioral transition from reactive to predictive tracking marks the point at which target jumps (and saccades) occur sufficiently rapidly for the subject to begin predicting: predictive tracking can begin when at least two previous intersaccade intervals fall within the correlation window (so that a reasonable estimate of ISI can be made, from these two intersaccade intervals). As explained below, this transition point can be used to provide an estimate of the correlation window size over which subjects utilize the feedback of previous movements.The autocorrelation function of the predictive latency time series provides another way to estimate the size of the correlation window over which past performance is monitored. As in previous work (Shelhamer ; Joiner and Shelhamer ), we set a threshold value for \u201csignificant\u201d correlation and define a \u201ccorrelation window\u201d over which the latencies of past saccades are \u201csignificantly\u201d correlated. In particular, we set the threshold for significant correlation at R  LL \u2009=\u20090.2, and determine when the latency autocorrelation function R  LL  crosses this threshold; this is indicated as the set of horizontal bars in panel (d) of Fig.\u00a0, which shows the autocorrelation functions for the same three subjects presented in panels (a), (b), and (c) (red, green, and blue, respectively). The autocorrelation functions for each subject were determined from the latency time series at 0.9\u00a0Hz pacing for 300 trials. The differences in the autocorrelation functions reflect different correlation windows across subjects.In panel (e) of Fig.\u00a0, we compare these two ways of estimating the size of the correlation window. The mean and standard deviation of the intersaccade intervals made at the critical ISI, obtained from the phase-transition experiment, are plotted against the correlation window span, obtained from the autocorrelation function of the predictive latency time series for seven subjects. The data are grouped around the equality line, demonstrating a correspondence between the two estimates of the correlation window size. Thus, we have two related estimates of the window length, which were derived independently based on observations on different sets of data. We should note that the mean and standard deviation of the intersaccade interval data at the point of the behavioral phase transition are here scaled by a factor of two; this implements the fact that, as stated in Section\u00a0, the timing of the stimulus can only be estimated after two movements (Fig.\u00a0(b)), and it is only on the next (third) and subsequent movements that this information can be utilized in programming saccade initiation time (see Joiner and Shelhamer ). In other words, though predictive tracking can begin when two movements fall within the correlation window, the timing between these two movements is not an estimate of the correlation window size. The correct estimate is this timing scaled by a factor of two because it is only on the next movement that this estimation can be utilized. This is further supported by the relationship between latency and the difference between consecutive intersaccade intervals presented in Fig.\u00a0(c). It is this realization that led us to investigate the relationship described in Fig.\u00a0(e), which suggests that subjects make the transition from reactive to predictive tracking when two consecutive movements fall within the correlation window and that the correlation window size is approximately twice the duration of the movement timing demonstrated at the transition.In this equation, \u03bc base and \u03c3 base are the initial size and variability of the correlation window (the mean and standard deviation of the time window are estimated by the scaled intersaccade interval distribution at the reactive\u2013predictive transition in tracking behavior during the phase-transition experiment), \u03b2 is the growth rate of the window size (determined from the extended tracking experiment described in Section\u00a0), and N p is the number of previously experienced predictive saccades.The second experimental observation involves the steady-state reactive and predictive tracking behavior observed during the phase-transition experiment. The latency data in the reactive range of the phase-transition experiment (immediately before the first and after the second behavioral transition) are used to estimate the distribution of the latency (or delay, \u0394) of reactive saccades that occur when the timing of the stimulus is outside the correlation window. As mentioned previously, the latency data when predictive saccades are made (immediately after the first and before the second behavioral transition) are used to determine the parameters (\u03bc prediction and \u03c3 prediction) of the Gaussian distribution of the projected future error, L(i\u2009+\u20091).Autocorrelation functions for predictive tracking latencies (approximately 1,000 trials at 0.9\u00a0Hz pacing) for one subject are presented in Fig.\u00a0(a). The latency data were divided into four segments of approximately 250 trials each, and the four autocorrelation functions (colored traces) are based on these four segments; the black trace is the autocorrelation function for the entire data set. As demonstrated in the figure, correlation window lengths (estimated by the width of the autocorrelation functions at a level of 0.2 as represented the horizontal lines, see Section\u00a0) increased as the respective segment occurred later in the data set. The correlation window lengths for the five autocorrelation functions presented in Fig.\u00a0(a) are plotted as a function of consecutive trial number in Fig.\u00a0(b). A logistic function fit (gray dashed line) represents the increase in time window length across all segments (R 2\u2009=\u20090.96, P\u2009=\u20090.02). However, over a smaller range (500 trials), a linear fit over the first three segments (black dashed line) is a sufficient representation (R 2\u2009=\u20090.96, P\u2009=\u20090.19). Utilizing this linear relationship, we determined the growth rate of the window size, \u03b2, as a function of number of predictive trials, N p. For example, setting \u03b2 to 0.0025\u00a0s per trial, 40 predictive trials equates to an increase of 100\u00a0ms in the correlation window length. We make use of this simplified linear fit in the development of our model, since it encompasses the data set sizes on which other aspects of the model are based.The simulation results for the same stimulus pacing rate (an ISI of 556\u00a0ms) are displayed in panels (b), (d), and (e) of Fig.\u00a0. In panel (b), the simulated results are displayed in the same format as the eye and target position traces presented in panel (a). The repetitive noisy internal-counter signals and respective thresholds for the simulated data in panel (b) are displayed in panel (d) (the noisy signals in panel (d) are the same as those described in Fig.\u00a0(a)). There are three important aspects of the model that are displayed in panel (d) that should be noted. First, the internal estimation begins only after the first two saccades; at least one intermovement interval is required for time estimation. Second, the threshold for the internal estimation changes based on feedback (see Section\u00a0); the vertical positions of the red bars change throughout the simulation. Third, the response reset strategy (the next intersaccade interval is timed from the last movement response) described in previous reports (Hary and Moore ; Schulze ) is demonstrated; the noisy signal is reset to zero after the threshold is reached. The changes in latency and intersaccade interval throughout the simulation are presented in panel (e). It is important to note that these aspects of the simulation resemble those of the sample subject (panel (c)): to overcome the initial initiation-timing error (the first two reactive movements), the intersaccade interval is decreased and then increased once the latency has been reduced.Evidence that subjects utilize the same internal-timing mechanism during predictive tracking at different pacing rates was provided by the scalar property: timing variance (intersaccade interval standard deviation) increased proportionally with interval length (Fig.\u00a0(a)). As a result, normalized intersaccade interval histograms for different ISIs overlap (Fig.\u00a0(b)). To determine if the simulation also demonstrated this property, the model was presented with ISIs of 500 and 833\u00a0ms for 300 trials each. The simulated intersaccade intervals produced by the model are distributed around the ISI, and the variability (width of the distribution) increased with interval length (Fig.\u00a0(b)). These distributions also demonstrate the scalar property when they are divided by the mean interval (Fig.\u00a0(d)).As stated in Section\u00a0, there are several previous models of motor synchronization to a pacing stimulus. We believe that the model presented in the current manuscript is novel for several reasons. First, whereas other models restricted feedback to a predefined number of previous movements (Michon ; Hary and Moore , , ; Vos and Hlsper, ; Mates , ), the feedback used in our model is gathered over a temporal window (correlation window) of previous performance. Next, the model combines aspects of the internal clock framework (Treisman ; Gibbon ; Meck and Benson ) with the monitoring of previous movements within a temporal window. In our model, predictive saccade tracking is based on an internal clock (internal estimation of the stimulus timing represented by the integrate-to-fire mechanism), the duration (threshold) of which is modified by feedback from previous intersaccade intervals and recent and projected timing errors (latencies). Third, the model reproduced a number of different behavioral results (change from reaction to prediction on initial tracking, phase transition, hysteresis, scalar property, continuation of predictive tracking despite perturbations, and intertrial correlations) with very few basic principles, while other timing studies and models have addressed these issues more or less independently (see Sch\u00f6ner ; Matell and Meck ; Mauk and Buonomano ; Buhusi and Meck ). For example, previous models have taken a more theoretical approach in quantitatively describing behavioral phase transitions (Haken et al. ; Sch\u00f6ner et al. ; Jirsa et al. ; in these studies, the models describe the transition between two coordinative patterns of the hands rather than timing and describe the transition in terms of oscillators). In the present model, the behavioral phase transition in timing behavior is due to the width of the correlation window; reactive or predictive tracking is dependent upon whether there is a sufficient amount of information (the number of movements) within the temporal window to estimate the stimulus timing. Though theoretical, this parameter of the model was determined experimentally. As described in Section\u00a0, the duration of the window time span was estimated from two independent experiments, and the correspondence between the two estimates further suggests that the transition occurs when the timing information is sufficient. In addition, the behavioral hysteresis is a natural consequence of the widening of the window with extended predictive saccade tracking, another experimental finding.There is recent evidence that temporal processing may not be centralized but rather distributed among different neural structures (Rao et al. ; Mauk and Buonomano ; Buhusi and Meck ). In our proposed model, the estimation of time is represented by a single integrate-to-threshold process. It is not our intent to suggest that the estimation of time is single centralized mechanism. Rather, we propose that the variability of time estimation, whether centralized or distributed, in the temporal range where subjects demonstrate predictive tracking behavior (approximately 500 to 1,500\u00a0ms) can be represented as a noisy integration process. The Gaussian distribution of the noise can be considered as the averaged effect of the many independent sources of variability that could arise from different neural areas (Gardiner ). Though we could have modeled time estimation as a process with a constant rise that varied from trial-to-trial, we chose the noisy integration process (the combination of a random walk process with a process that increased at a constant rate with time) for two reasons: (1) The neurobiological sources of variability throughout the time estimation process (Meck and Benson ; Buhusi and Meck ) suggests that the rate is subject to noise, which is captured with the random walk process (Sch\u00f6ner ), and (2) the finding that the scalar property holds for repetitive predictive saccade sequences (Fig.\u00a0) suggests that the longer this variability is integrated into the estimate, the more variability there is in the timing distribution (Fig.\u00a0(a)).This type of noisy integration process has successfully been utilized to describe some aspects of cognitive behavior. For example, Sigman and Dehaene () utilized this mechanism to model the decision-making process and the resulting reaction time distributions from a dual task: Subjects performed a number classification task followed by a tone discrimination task. Though simple, the model made predictions that held over a range of task manipulations (i.e., presenting the numbers as digits or in spelled words), which enabled the authors to parse the task into serial and parallel components. In addition, similar theoretical models have been used to describe the reaction time of single reactive saccadic eye movements (LATER model: Carpenter and Williams ; Reddi and Carpenter ). Rather than the integration of a noisy signal, the process that initiates the eye movement in these studies is a linear accumulation to threshold with a growth rate that varies randomly from movement to movement. This rise-to-threshold behavior has been identified in single-neuron recordings of the frontal eye fields and superior colliculus (Hanes and Schall ; Hanes et al. ; Par\u00e9 and Hanes ) and supports the theory of a trigger threshold in movement initiation.There are several behavioral similarities between repetitive tapping and saccades that suggest that the proposed model could also be applied to synchronized tapping. For example, repetitive tapping movements to a pacing metronome also demonstrate a behavioral phase transition from reactive to anticipatory as the ISI monotonically decreases (Engstr\u00f6m et al. ). In addition, it has been previously proposed that synchronized tapping (anticipatory behavior) is based on a temporal integration of previous movements occurring within a time window of approximately 3\u00a0s (Mates et al. ). Thus, it is likely that the framework of the model described in the current manuscript (correlation window, feedback of timing error and intermovement interval, time estimation modeled as a noisy integration process) could also be applied to repetitive tapping movements. However, despite these behavioral similarities, there is an important difference between the two movements that may require adjustments to the model to adequately depict repetitive tapping. The timing error during repetitive tapping movements is sensed as tactile information as opposed to the visual error during repetitive saccade tracking. This sensory difference may require a change in the feedback of the timing error. For example, an adjustment in the weighting of the previous and projected error may account for this difference between the two movements. In addition, differences have been found in the cortical representations of different effectors in the neural pathway for the transformation from sensory to motor signals (e.g., Calton et al. ; Lawrence and Snyder ), which suggests that different predictors/timers might exist for different effectors.Our latency data, both behavioral and simulated, show a statistical structure that resembles long-term correlations (often called \u201clong-range dependence\u201d). This is manifest as a gradual (power law) decay of the autocorrelation function and a 1/f  \u03b1  form of power spectrum. The implications of these long-range correlations have been discussed in many other contexts (for example Hausdorff et al. ; Linkenkaer-Hansen et al. ; Peng et al. ). While the most common explanation of these correlations is the interaction of many different neural processes acting on many different time scales, more recent work suggests that a more straightforward mechanism might be at work: Simple summation of a small number of random processes with different short-term time scales can produce time series that exhibit (or appear to) long-range correlations (Wagenmakers et al. ; Wing et al. ). One such formulation is white noise added to moving-average-filtered versions of white noise. This loosely resembles our implementation of the modulation of the correlation window with tracking experience (Eq. ): intersaccade interval errors are monitored over the time specified by this window, and this window increases in size with extended predictive tracking.It is important to note that though we use a distribution of predictive latencies as a parameter of the model, L(i\u2009+\u20091), the use of this data is not what produces the correlations between simulated trials (Fig.\u00a0(d)). L(i\u2009+\u20091) is the projected error of the current interval estimate, I NEW or threshold, and the values are drawn from this distribution (uncorrelated samples). Additionally, this latency value is only used in setting the threshold of the integrate-to-fire mechanism. In fact, it is the termination of the integrate-to-fire mechanism with respect to the stimulus jump that actually determines the error of the synchronization (for example, the latencies plotted in Fig.\u00a0(c)). The intertrial correlations of the simulated data suggest that it is the influence of multiple previous intermovement intervals (not latency errors) within the correlation window that results in the correlations seen experimentally (Fig.\u00a0(a)). One question these results raise is how this source of timing information is encoded and stored. Recent findings suggest that the estimation of time passage may be accomplished by the accumulation of the action potentials emitted by dopaminergic neurons in the basal ganglia (Matell and Meck ). In addition, there is evidence that the storage of these time estimates may be mediated by the frontal and hippocampal systems (Meck et al. ).Physiologically, we suggest that the increase in correlations with extended tracking, shown in Fig.\u00a0, reflects the brain\u2019s increasing confidence in stimulus history: As pacing continues at the same rate, it is safe to use performance from further in the past in guiding future tracking. However, this might instead be an epiphenomenon of tracking performance becoming less variable with experience. While this is an interesting possibility, in fact, tracking performance does not typically become less variable with experience (data not shown). This raises the question of why the tracking system should bother to increase the correlation window if it does not lead to improved performance. We propose that building up a longer correlation window is an effective way to perseverate storage of previous tracking, which is useful if predictive pacing is briefly interrupted or if experience must be pieced together from several short segments of consistent tracking. Another interesting question is how the size of the correlation window is determined and modulated with experience. We suspect that the window increases in size for each trial in which tracking error is below a tolerable limiting value. Evaluation of this idea is an aim of our future work.While our results do not make a definitive statement as to the provenance or neurobiological role of these correlations, they do show that some aspects of our model\u2014in particular the feedback of recent timing error and the real-time adjustment of the window over which this feedback takes place\u2014can generate correlations that are similar to those that we find in behavioral data.While continuous tracking of periodic targets is not very biologically meaningful, we note that the model is based on these data but can reproduce the initiation phase of tracking\u2014the first few saccades, a more biologically relevant situation. Furthermore, other types of rhythmic behaviors such as music and dance are biologically relevant for communication and socialization\u2014any increase in our knowledge of timing behaviors in general could have a bearing on this understanding. For example, our model may contribute to the understanding of internally timed movements by its application to data acquired from patient populations that exhibit deficits in the ability to produce anticipatory movements. Two likely neural structures involved in the internal estimation of time are the basal ganglia and the cerebellum (Harrington and Haaland ; Buonomano and Karmarkar ; Mauk and Buonomano ; Buhusi and Meck ). For example, patients with disorders of the basal ganglia exhibit deficits in predictive tracking of targets alternating at ISIs of approximately 1,000\u00a0ms (Bronstein and Kennard , ; Crawford et al. ; Tian et al. ; Ventre et al. ). These deficits include an increase in timing variability and a decrease in the proportion of anticipatory responses compared to control subjects. In addition, patients with Parkinson\u2019s disease demonstrate an increase in timing variability during repetitive tapping that is attributed to an increase in internal clock variability rather than motor delay (Wing et al. ; Harrington et al. ). Based on the hypothesized involvement of the basal ganglia in the internal clock mechanism (see Matell and Meck ), this decrease in the percentage of predictive movements and increase in total timing variability may be replicated in simulation by increasing the noise added to the linear-rising signal.The cerebellum has also been shown to play a critical role in rhythmic timing. Similar to Huntington\u2019s and Parkinson\u2019s disease patients, cerebellar patients display an increase in timing variability during rhythmic tapping to an auditory metronome pacing at an ISI of 550\u00a0ms (Ivry and Keele ; it should be noted that the ISI of 550\u00a0ms promotes anticipatory tapping behavior; Engstr\u00f6m et al. ). These results are supported by the impairment of time perception in the millisecond range (400\u2013600\u00a0ms) by induced disruption (via repetitive transcranial magnetic stimulation) of specific cerebellar regions (Koch et al. ; Lee et al. ). The cerebellum has also been shown to play a role in response timing adjustments. Dreher and Grafman () demonstrated that the cerebellum is more active when subjects perform a task with timing irregularity rather than with constant timing. Similarly, a recent functional magnetic resonance imaging study (Schlerf et al. ) reported that specific regions of the cerebellum are involved in timing error correction. During a paced button press task, distinct regions of the cerebellum showed greater activity when the timing of the stimulus was variable rather than isochronous. These results suggest that the cerebellum may be involved in both the time estimation process and the feedback of inter-response intervals and timing errors. The increase in the intermovement variance by cerebellar patients during rhythmic finger tapping may be the result of damage to the timing error feedback mechanism. For example, adding random noise to the feedback of timing errors during the simulation (random fluctuations in the threshold of the internal counter) will also result in an increase in the intermovement variance. Similar to the example described for Parkinson\u2019s patients, in this manner, it may be possible to attribute such functions as error feedback or time estimation during predictive repetitive movements to specific neural structures."}